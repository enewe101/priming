'''
This module has one class, the purpose of which is to represent the raw data, 
which was generated by AMT as a csv file.  

It does heplful things like handle the random
partitioning of treatments into non-overlapping testing and training sets
of a given size, and allowing coarser-grained access to the data (e.g.
how many times does the word "wine" appear in image 2 for treatment 2, as 
opposed to how many times does it appear in pos 2 of image 2 for treatment 2.)

In hindsight, some of the concerns that it handles should maybe belong in the
naive_bayes module, but that's not totally clear.

It also produces printouts of the words used by each treatment for each image
in a simple text format, which makes it easier to look at the data and use
it in other software.  These word-count printouts were produced and are stored
in `<proj-root>/data/word_counts/*`.  Those lists were put out on a per-image
basis, and are so-named.  Files that break out the per-image word counts on
a per-image, per-treatment basis are also there.

These word-count printouts were used to build the ontologies by hand.
'''

import numpy as np
import re
import os
import util
import random
import json
import csv
from collections import defaultdict, Counter
from random import shuffle
from nltk.corpus import wordnet as wn
import nltk


def readDataset(is_exp_2_dataset=False):
	'''
	Factory method that builds a CleanDataset from the original Amazon
	Mechanical Turk CSV files
	'''

	if not is_exp_2_dataset:
		# Create a new priming-image-label-experiment dataset
		dataSet = CleanDataset()

		# Read from the raw amt csv files.  
		# Note: order matters!  The older files have duplicates workers that
		# get ignored.  Make sure to read the newer file files earlier
		dataSet.read_csv('amt_csv/exp_1_2014.04/amt1_cut.csv', True)
		dataSet.read_csv('amt_csv/exp_1_2014.04/amt2_cut.csv', True)
		dataSet.read_csv('amt_csv/exp_1_2014.04/amt3_cut.csv', True)

		# The dataset needs to do some internal calts to refresh its state 
		dataSet.aggregateCounts()
		dataSet.calc_ktop(5)

		dataSet.uniform_truncate(125)
		return dataSet

	else:
		# Create a new priming-image-label-experiment dataset
		dataSet = CleanDataset(is_exp_2_dataset)

		# Read from the raw amt csv files.  
		# Note: order matters!  The older files have duplicates workers that
		# get ignored.  Make sure to read the newer file files earlier
		dataSet.read_csv('amt_csv/exp_2_2014.09/amt_cut_2014.09.csv')

		dataSet.uniform_truncate()
		return dataSet


def get_correct_treatments(image_num, pos):
	'''
		helper function to handle the permutation of images that was used
		during experiment 2.  This takes a particular image number, and 
		the desired position in which you wish to find it, then returns the
		treatment indices that were presented said image number in said
		position.  
		
		Example: To find the treatments that had image 2 in position 0:
			get_correct_treatments(2,0) => (2,7)

		Because treatments 2 and 7 were shown image 2 as the first test image.
	'''

	rank = (image_num - pos) % 5
	return (rank, rank+5)


class CleanDatasetException(Exception):
	pass


class CleanDatasetRotationException(Exception):
	pass


def permute(input_list, class_idx, test_only=True):
	'''
	helper to deal with the fact that images in the second experiment were
	shown in permuted order.  
	Returns an array whose values show the position of the image (and the
	array indices are the image ids)
	So, test_permutation[3] would give the position of image named '3'
	'''

	# note each element with its starting index (implicitly it's image idx)
	input_list = enumerate(input_list)

	# the test images are in range(5,10), but usually since we don't handle
	# the priming images, we want to address them
	# by integers 0 through 4 (i.e. without an offset)
	if test_only:
		offset = False
	else:
		offset = True

	# permute the input list
	output_list = sorted(
		input_list, None, lambda x: get_pos(x[0], class_idx, offset))

	# discard the original indices
	return [x[1] for x in output_list]


def get_pos(img_idx, class_idx, offset=False):

	if not offset:
		return (img_idx - class_idx) % 5

	if img_idx < 5:
		return img_idx

	return (img_idx - class_idx) % 5 + 5


SPLIT_RE = re.compile(r'[^a-zA-Z0-9]')
def clean_dataset_adaptor(
		clean_dataset, 
		treatments=['treatment0', 'treatment1'], 
		images=['test0'],
		show_token_pos=True,
		show_plain_token=True,
		do_split=True,
		remove_stops=True,
		lemmatize=True,
#		spellcheck=True,
#		experiment=2,
		weight='tfidf', # Not implemented does nothing
	):
	'''
	extracts a dataset from the clean_dataset object in the format expected
	by the naive bayes cross validation tester.  
	'''

	# we'll accumulate the new dataset in here
	naive_bayes_dataset = defaultdict(lambda: [])

	# we might need a dictionary of spelling corrections
	#if spellcheck:
	#	dictionary = json.loads(open('data/new_data/dictionary.json').read())

	# we might need a lemmatizer
	if lemmatize:
		lmtzr = nltk.stem.wordnet.WordNetLemmatizer()

	# we might need the stopwords list
	if remove_stops:
		stops = set(nltk.corpus.stopwords.words('english'))

	# walk over all the treatments
	for treatment in treatments:

		# walk over all the entries.  Entries in the new dataset are called
		# "examples"
		for entry in clean_dataset.entries[treatment]:

			# the example always has the treatment name as the first component
			example = [treatment]

			# It isn't strictly necessary, but we are preserving the position
			# of the label (i.e. label put in first or second text-box?)
			all_items = sorted(entry.items())

			# filter out just the labels attributed to the image(s) of 
			# interest
			for image in images:
				labels_for_image = filter(lambda x: x[0][0]==image, all_items)

				features_for_image = []

				for label in labels_for_image:

					# take apart the label components
					(img_idx, pos), word = label

					# maybe split up multiple words entered in same input
					if do_split:
						words = SPLIT_RE.split(word)
					else:
						words = [word]

					# maybe remove stopwords
					if remove_stops:
						words = [w for w in words if w not in stops]

#					# maybe correct spelling
#					if spellcheck:
#						dict_key = '%d_%d' % (experiment, int(image[-1])+5)
#						if dict_key in dictionary:
#							this_dict = dictionary[dict_key]
#
#							yope = False
#							if any([w in this_dict for w in words]):
#								yope = True
#								print words
#
#							words = [
#								this_dict[w] if w in this_dict else w 
#								for w in words
#							]
#							if yope:
#								print words

					# maybe lemmatize
					if lemmatize:
						words = [lmtzr.lemmatize(w) for w in words]

					# keep each word as a separate feature.  Tag with img_idx
					if show_plain_token:
						features_for_image.extend([
							(img_idx, word) for word in words])

					# we can also keep track of the position of the labels
					if show_token_pos:
						features_for_image.extend([
							(img_idx, pos, word) for word in words])

				# tack on the features from this image onto the example
				example.extend(features_for_image)

			# add the example to the new dataset
			naive_bayes_dataset[treatment].append(tuple(example))

	return dict(naive_bayes_dataset)


def simple_dataset_2_naive_bayes(simple_dataset):
	'''
		pass in a SimpleDataset, and get out a dictionary in the format
		expected by the NaiveBayesCrossValidationTester.
	'''
	nb_dataset = defaultdict(lambda: [])
	data = simple_dataset.data

	for class_name in data:
		entries = data[class_name]
		nb_dataset[class_name] = [
			tuple([e['class_idx']] + e['features']) for e in entries
		]

	return nb_dataset


class SimpleDataset(object):
	'''
	this was designed to be well-adapted to training an svm classifier
	'''

	# This class's assumptions
	NUM_PRIMING_IMAGES = 5
	NUM_TEST_IMAGES = 5
	NUM_IMAGES = NUM_PRIMING_IMAGES + NUM_TEST_IMAGES
	NUM_WORDS_PER_IMAGE = 5
	DATA_DIR = 'data/amt_csv'
	EXP1_PATH = 'exp_1_2014.04'
	EXP2_PATH = 'exp_2_2014.09'
	EXP1_FNAMES = ['amt1_cut.csv', 'amt2_cut.csv', 'amt3_cut.csv']
	EXP2_FNAMES = ['amt_cut_2014.09.csv']
	CACHE_PATH = 'cache'
	CORRECTIONS_PATH = 'data/new_data/dictionaries/with_allrecipes/'
	DICTIONARY_FNAMES = ['dictionary_1.json', 'dictionary_2.json']
	WORD_MAP_FNAME = 'mapped_words.json'
	WORD_BREAK = re.compile(r'[^a-zA-Z]+')

	STRIP = re.compile(r"[^a-zA-Z'_-]+", re.I)
	UNDERSCORE = re.compile('_')

	def __init__(
		self, 
		which_experiment=1,
		show_token_pos=True,
		show_plain_token=True,
		show_token_img=True,
		do_split=True,
		class_idxs=[0,1],
		img_idxs=range(5,10),
		spellcheck=True,
		lemmatize=True,
		remove_stops=True,
		balance_classes=True,
	):

		# validate options
		# There were two experiments, each with its own dataset
		assert(which_experiment in [1, 2])

		if not show_token_pos and not show_plain_token:
			print ('Warning: neither plain tokens nor position-prepended '
				'tokens are being included -- the vocabulary will be empty.')

		# register options 
		self.which_experiment = which_experiment
		self.show_token_pos = show_token_pos
		self.show_plain_token = show_plain_token
		self.do_split = do_split
		self.class_idxs = class_idxs
		self.img_idxs = img_idxs
		self.spellcheck = spellcheck
		self.num_examples = None # this only gets set in balance_classes()
		self.show_token_img = show_token_img
		self.lemmatize = lemmatize
		self.remove_stops = remove_stops

		# determine the paths to the desired data
		self.raw_paths = self.resolve_raw_data_paths(which_experiment)

		# state variables
		self.data = defaultdict(lambda: [])
		self.worker_ids = set()
		self.vocab_list = []
		self.vocab_counts = Counter()
		self.num_docs = 0

		# read some associated helper data
		self.read_dictionary()
		self.lmtzr = nltk.stem.wordnet.WordNetLemmatizer()
		self.stops = set(nltk.corpus.stopwords.words('english'))

		# read in the raw data
		self.read_raw_data()
		if balance_classes:
			truncate_to = None
			if isinstance(balance_classes, int):
				truncate_to = balance_classes

			self.balance_classes(truncate_to)


	def balance_classes(self, truncate_to=None):

		# what is the min number of examples in each class?
		if truncate_to is None:
			truncate_to = min([len(self.data[idx]) for idx in self.data])

		# truncate all data (randomly) to even out number of examples
		for idx in self.data:
			self.data[idx] = random.sample(self.data[idx], truncate_to)

		self.num_examples = truncate_to
		print 'truncated to %d examples.' % truncate_to


	def read_dictionary(self):
		'''
			the dictionaries contain pre-computed mappings from misspelled
			words to correct spellings.  This method loads the dictionaries.
		'''

		# read the dictionaries
		dictionaries = []
		for fname in self.DICTIONARY_FNAMES:
			fh = open(os.path.join(self.CORRECTIONS_PATH, fname))
			dictionaries.extend(json.loads(fh.read()))
			fh.close()

		# re-organize the dictionaries to be indexed by experiment and image
		self.dictionaries = {}
		for d in dictionaries:
			params = d['params']
			exp, img = params['which_experiment'], params['img_idxs'][0]
			results = d['results']
			self.dictionaries[(exp, img)] = results

		# read the word_map -- this contains aliases, which resolve cases
		# where there is more than one way to spell the same word
		fname = os.path.join(self.CORRECTIONS_PATH, self.WORD_MAP_FNAME)
		word_map = json.loads(open(fname).read())

		# we're interested in the entries that have an "alias", which means
		# they are alternate spellings of a cononical word form
		self.word_map = dict([
			(w['word'], w['alias']) 
			for w in word_map if 'alias' in w
		])


	def resolve_raw_data_paths(self, which_experiment):

		if which_experiment == 1:
			raw_fnames = [
				os.path.join(self.DATA_DIR, self.EXP1_PATH, f) 
				for f in self.EXP1_FNAMES
			]

		else:
			raw_fnames = [
				os.path.join(self.DATA_DIR, self.EXP2_PATH, f)
				for f in self.EXP2_FNAMES
			]

		return raw_fnames


	def seen_worker(self, worker_id):
		'''
		check if the worker has been seen before, and record the id if not
		'''

		if worker_id in self.worker_ids:
			return True

		self.worker_ids.add(worker_id)
		return False


	def img_idx_2_pos(self, img_idx, class_idx):
		if img_idx < self.NUM_PRIMING_IMAGES:
			return img_idx

		if self.which_experiment == 1:
			return img_idx

		return (img_idx - class_idx)%5 + 5


	def img_pos_2_idx(self, image_num, treatment_num):
		'''
		Get the image id for the image located at position image_num.
		This is necessary because the positions of images were permuted in
		the second experiment
		'''
		# the priming images are not permuted
		if image_num < self.NUM_PRIMING_IMAGES:
			return image_num

		# the original dataset is not permuted
		if self.which_experiment == 1:
			return image_num

		return (image_num + treatment_num)%5 + 5


	def read_raw_data(self):

		for fname in self.raw_paths:
			self.read_raw_file(fname)


	def cache(self, cache_address, data):
		pass


	def as_vect(self, randomize=True, weights='tfidf'):

		vectors = []
		for class_idx in self.class_idxs:
			for example in self.data[class_idx]:
				if weights == 'tfidf':
					feature_vector = [
						1*np.log2(self.num_docs/float(self.vocab_counts[t]))
						if t in example['features'] else 0
						for t in self.vocab_list
					]
				else:
					feature_vector = [
						1 if t in example['features'] else 0
						for t in self.vocab_list
					]

				vectors.append((feature_vector, example['class_idx']))

		if randomize:
			shuffle(vectors)

		# zip(*list_of_tuples) ==> `unzip`... think about it
		features, outputs = zip(*vectors)
		return features, outputs



	def read_raw_file(self, fname):

		fh = open(fname)
		reader = csv.DictReader(fh)

		for record in reader:

			# Skip duplicate workers.
			if self.seen_worker(record['WorkerId']):
				continue

			self.num_docs += 1

			# The experimental treatment for this worker is its class
			try:
				class_idx = int(record['Answer.treatment_id'])
			except ValueError:
				print record['Answer.treatment_id']

			# we only load data for the desired classes
			if class_idx not in self.class_idxs and self.class_idxs != 'all':
				continue

			# we look at the position where the desired images are found
			# note that the images are permuted based on treatment (class)
			if self.which_experiment==2 and class_idx < 10:
				img_positions = [self.img_idx_2_pos(p, class_idx) 
					for p in self.img_idxs]
			else:
				img_positions = self.img_idxs

			# Iterate over all the images in the image-set
			entry = {'class_idx': class_idx, 'features':[]}
			self.data[class_idx].append(entry)
			for img_pos in img_positions:

				# get the (unpermuted) id for the image
				if self.which_experiment == 2 and class_idx < 10:
					img_idx = self.img_pos_2_idx(img_pos, class_idx)
				else:
					img_idx = img_pos

				word_key_prefix = 'Answer.img_%d_word_' % img_pos
				dictionary = self.dictionaries[(
					self.which_experiment,img_idx)]

				for word_pos in range(self.NUM_WORDS_PER_IMAGE):
					word_key = word_key_prefix + str(word_pos)
					word = record[word_key]
					words = self.normalize_word(
						word, word_pos, dictionary, img_idx)
					entry['features'].extend(words)

			# add the tokens
			self.vocab_counts.update(entry['features'])

		fh.close()
		self.vocab_list = self.vocab_counts.keys()
		self.vocab_list.sort()


	def normalize_word(self, w, p, d, i):

		w = w.lower()

		# replace consecutive wonky characters with a single space
		word  = self.STRIP.sub(' ', w)

		# Maybe do spell correction
		if self.spellcheck:

			# do corrections on a word by word basis
			old_word = word

			# temporarily split for the purpose of spell checking
			words = self.WORD_BREAK.split(word)

			# replace words with correct spellings (we have precomputed
			# which words are misspelled and what their probable replacements)
			words = [d[w] if w in d else w for w in words]

			# map back to aliases, to account for words that
			# have multiple spellings
			words = [
				self.word_map[w] if w in self.word_map
				else w for w in words
			]
			
			# bust up wordnet's compound words
			words = [self.UNDERSCORE.sub(' ',w) for w in words]

			# put words back together
			word = ' '.join(words)

		# Maybe lemmatize
		if self.lemmatize:

			# temporarily split words for lemmatization
			words = self.WORD_BREAK.split(word)
			words = [self.lmtzr.lemmatize(w) for w in words]
			word = ' '.join(words)

		# Maybe remove stops
		if self.remove_stops:

			# temporarily split words for lemmatization
			old_word = word
			words = self.WORD_BREAK.split(word)
			words = [w for w in words if w not in self.stops]
			word = ' '.join(words)
			if old_word != word:
				print old_word, '->', word

		# Maybe split words
		if self.do_split:
			words = self.WORD_BREAK.split(word)
		else:
			words = [word]

		return_words = []

		# prepend tokens to indicate what position they were in
		if self.show_token_pos:
			return_words.extend(['%d_%s' % (p, w) for w in words])

		# include un-prepended tokens
		if self.show_plain_token:
			return_words.extend(words)

		# maybe add the img_idx to the word
		if self.show_token_img:
			return_words = ['%d_%s' % (i, w) for w in return_words]

		return return_words


class CleanDataset(object):

	# CONSTANTS # 

	K = 5
	NUM_IMAGES = 5
	NUM_WORDS_PER_IMAGE = 5

	# Treatment Types
	IMAGE_PRIMING = 0
	FRAMING = 1

	def __init__(self, is_exp_2_dataset=False):

		# State flags
		self.hasKtop = False		# whether ktop is up-to-date
		self.isAggregated = False	# whether aggregated values are up-to-date

		# Data
		self.dictionary = set()		# a list of all words that occur 
		self.entries = {}			# an entry holds the data for one worker
		self.testEntries = {}		# holds the entries for testing when 
									# sub-sampled
		self.unusedForTest = {}	# keeps track of which entries have 
										# been used for testing so far

		self.unusedEntries = {}		# used to hold entries that are removed
									# during truncation
		self.trainEntries = {}
		self.workerIds = set()		# to prevent duplicates
		self.counts = {}			# stores words and frequency of occurrence
		self.ktops = {}
		self.areTreatmentsEqual = False	# boolean, indicates whether all 
										# treatments have the same number of 
										# entries

		self.is_exp_2_dataset = is_exp_2_dataset


	def get_correct_treatments(self, image_num, pos):
		rank = (image_num - pos) % 5
		return ('treatment%d' % rank, 'treatment%d' % (rank+5))


	# Use this to make all of the treatments have the same size 
	def uniform_truncate(self, truncateSize=None):

		# Figure out size of smallest treatment
		min_treatment_size = min(
			[len(treatment) for treatment in self.entries.values()])

		# if treatment size isn't specified, use size of smallest treatment
		if truncateSize is None:
			truncateSize = min_treatment_size

		# else check that treatment size is not bigger than smallest treatment
		else:
			if truncateSize > min_treatment_size:
				raise CleanDatasetException('CleanDataset.uniform_truncate: '\
					'truncateSize must not be larger than smallest '\
					'treatment size in data set.')

		# tell the user that their data is being curtailed
		#for treatment_id, treatment in self.entries.items():
		#	print '%s has %d entries.'% (treatment_id, len(treatment))
		#print 'truncating all treatments to %d entries' % truncateSize

		# Make all treatments the same size
		for treatment in self.entries.keys():
			
			remaining = len(self.entries[treatment]) - truncateSize
			subsampled_treatment, unused_from_treatment = util.randomPartition(
				self.entries[treatment], truncateSize, remaining)

			self.entries[treatment] = subsampled_treatment
			self.unusedEntries[treatment] = unused_from_treatment

		self.areTreatmentsEqual = True


	def subsample(self, testSetSize):
		'''
		Partitions the dataset into a training set, and test set.  
		The treatments must be of uniform size.  If they are not, then run
		uniform_truncate() with no arguments to make them all the size of 
		the smallest treatment.
		'''

		# State Validation
		if not self.areTreatmentsEqual:
			raise CleanDatasetException('The treatments are not all of the '\
				'same size')

		# Input Validation
		treatment_size = len(self.entries.values()[0])
		if testSetSize > treatment_size:
			raise CleanDatasetException('The test set size cannot be '\
				'larger than the size of the treatments')

		self.trainEntries = {}
		self.testEntries = {}
		self.unusedForTest = {}

		# the testSetSize is everything outside the training set 
		trainingSetSize = treatment_size - testSetSize

		self.testSetSize = testSetSize
		self.trainingSetSize = trainingSetSize

		# when we begin, no treatments have yet been used for testing
		# we'll track which entries have been used based on their workerId
		for treatment, entries in self.entries.items():
			self.unusedForTest[treatment] = set(
				[e['workerId'] for e in entries])


		# set up the first partition of the dataset into training and
		# test set.  Further calls to rotateSubsample will give partitions
		# whose test set is non-overlapping with prior test sets since calling
		# subsample()
		self.rotateSubsample()


	def rotateSubsample(self):

		# clear the existing data partition: we are now re-partitioning the
		# data
		self.trainEntries = {}
		self.testEntries = {}

		# Make sure that we have enough entries that haven't been used for a
		# previous test set, in order to make the next test set
		if len(self.unusedForTest.values()[0]) < self.testSetSize:
			raise CleanDatasetRotationException('cannot rotateSubsample: not '\
				'enough entries available that have not been used before in '\
				'a test set.')

		# Split the entries of each treatment into testing and training sets
		for treatment, entries in self.unusedForTest.items():

			# create a place to put the training and test entries for this
			# treatment
			self.trainEntries[treatment] = []
			self.testEntries[treatment] = []

			# pick out which entries to include in the test partition for
			# this treatment
			test = random.sample(entries, self.testSetSize)

			# partition the entries based on the choice of test partition
			for e in self.entries[treatment]:
				if e['workerId'] in test:
					self.testEntries[treatment].append(e)
				else:
					self.trainEntries[treatment].append(e)

			# The entries used for test can't be used in the future
			self.unusedForTest[treatment] -= set(test)

		# Now that partitioning is done, recalculate aggregates
		self.aggregateCounts()
		self.calc_ktop()


	def getTestInstances(self):
		return self.testEntries


	def aggregateCounts(self):

		# First we need to clear out stale counts
		self.counts = {}

		# Next, we package up note all unique intstances of features
		for treatment, entries in self.trainEntries.items():
			for entry in entries:
				features = filter(lambda (k,v): isinstance(k, tuple),
					entry.items())

				for ((image, position), word) in features:
					self._aggregateCount((treatment, image, position), word, 1)

		# Finally, we roll up the counts to aggregate counts
		# For example, if the same word occurs in different positions of the
		# same image, we would now be able to access the aggregate count for
		# that image, regardless of position
		for treatment, image, position in self.counts.keys():
			count_dict = self.counts[(treatment, image, position)]

			# This should never happen, it was a quick check. Replace with a 
			# test
			if position is None:
				assert(False)

			for word, frequency in count_dict.items():

				# Depending on which of the entries in the key-tuple is left
				# as None, we aggregate counts to various degrees
				self._aggregateCount((treatment, None, None), word, frequency)
				self._aggregateCount((treatment, image, None), word, frequency)
				self._aggregateCount((None, image, None), word, frequency)

				# This entry in counts is a data-set wide count. Maybe that
				# means I should drop the self.dictionary
				self._aggregateCount((None, None, None), word, frequency)

		self.isAggregated = True


	def _aggregateCount(self, key, word, count):

		# If this is a new key, make it
		if key not in self.counts:
			self.counts[key] = {}

		# If this is a new word, make a new word-count entry
		if word not in self.counts[key]:
			self.counts[key][word] = count

		# Otherwise, just increment the word-count entry
		else:
			self.counts[key][word] += count


	def read_csv(
		self,
		fname,
		hold=False):

		self.hasKtop = False
		self.isAggregated = False
		self.areTreatmentsEqual = False

		fh = open(fname, 'r')
		reader = csv.DictReader(fh)

		for record in reader:

			# Skip duplicate workers.
			workerId = record['WorkerId']
			if workerId in self.workerIds:
				continue

			newEntry = {}

			# Note the new worker id
			self.workerIds.add(workerId)
			newEntry['workerId'] = workerId

			# Note the experimental treatment for this worker
			treatment_index = int(record['Answer.treatment_id'])
			tmt_id = 'treatment%d' % treatment_index
			newEntry['treatment'] = tmt_id

		
			# For the original dataset it wasn't necessary to distinguish
			# between "treatment types", i.e. framing or inter-task priming
			if not self.is_exp_2_dataset:
				treatment_type=None

			# But we need to make that distinction for experiment 2 data
			if self.is_exp_2_dataset:
				if treatment_index < (2 * self.NUM_IMAGES):
					treatment_type = self.IMAGE_PRIMING
				else:
					treatment_type = self.FRAMING


			if tmt_id not in self.entries:
				self.entries[tmt_id] = []

			self.entries[tmt_id].append(newEntry)

			# Record the image files used to prime this worker
			newEntry['primingImageFiles'] = []

			# TODO:
			# handle the fact that images are permuted, so their ID is related
			# to img_num in a complicated way.
			# handle the fact that the framed images have no priming 
			# images
			# Do the following for both the priming and testing image-sets
			for sub_treatment in ['prime', 'test']:

				# The framing treatments in experiment2 dataset don't have
				# and priming images
				if (
					treatment_type == self.FRAMING and 
					self.is_exp_2_dataset and 
					sub_treatment == 'prime'
				):
					continue

				# Iterate over all the images in the image-set
				for img_num in range(self.NUM_IMAGES):

					# The test images are numbered sequentially, following the
					# priming images, so we need to apply an offset
					offset = 0
					if sub_treatment == 'test':
						offset = self.NUM_IMAGES

					offset = 0 if sub_treatment=='prime' else self.NUM_IMAGES
					amt_img_num = img_num + offset

					# in the second experiment, the test images are permuted
					# for the IMG_FOOD and IMG_OBJ treatments.  We need to
					# ensure that the labels get attributed to the right images
					if self.is_exp_2_dataset:
						if treatment_type == self.IMAGE_PRIMING:
							permutation_offset = treatment_index
						else:
							permutation_offset = 0
					else:
						permutation_offset = 0

					# This is how we name images in the dataset
					img_id = sub_treatment + str(
							(img_num + permutation_offset) % self.NUM_IMAGES
						)

					# Record the name of the file for this image
					newEntry['primingImageFiles'].append(
						record['Answer.img_%d_id' % amt_img_num])

					# Now, within the data recorded for each image, 
					# iterate over each position.  These correspond to the
					# text-inputs in the HIT
					for word_pos in range(self.NUM_WORDS_PER_IMAGE):

						# Get the word from the csv file, normalize to 
						# lowercase, store it in the record, and add it to
						# the data-set-wide dictionary
						word = record['Answer.img_%d_word_%d' 
							% (amt_img_num, word_pos)].lower()
						newEntry[(img_id, word_pos)] = word
						self.dictionary.add(word)

		# Make the training entry set be the full entry set, and make the
		# test set empty.  Subsampling changes this partitioning
		for treatment, entries in self.entries.items():
			self.trainEntries[treatment] = list(entries)
			self.testEntries[treatment] = []

		# Check if the treatments all have the same size
		last_treatment_size = None
		areTreatmentsEqual = True
		for treatment, entries in self.entries.items():

			# for the first treatment, just record its size
			if last_treatment_size is None:
				last_treatment_size = len(entries)

			# for subsequent treatments, check if they have the same size
			else:
				if len(entries) != last_treatment_size:
					areTreatmentsEqual = False

		self.areTreatmentsEqual = areTreatmentsEqual
				
		# Update the aggregated counts and the k-top words (except if held)
		if not hold:
			self.aggregateCounts()
			self.calc_ktop(self.K)


	def write_counts(self, directory):

		# Make sure that the directory ends with a slash
		# TODO check that it exists
		if not directory.endswith('/'):
			directory += '/'

		for image_id in self.counts_by_image.keys():

			# Open a file for image counts for this image
			# Write the counts, then close.
			fh_img_counts = open(directory + image_id + '.txt', 'w')
			fh_img_counts.write(self.list_counts_for_img(image_id))
			fh_img_counts.close()

			for treatment_id in self.counts_by_tmt_image.keys():


				# Open a file for image counts for this image and treatment
				# combination.  Write the counts, then close.
				fh_tmt_img_counts = open(
					directory + image_id + '_' + treatment_id + '.txt', 'w')
				fh_tmt_img_counts.write(
					self.list_counts_for_tmt_img(treatment_id, image_id))
				fh_tmt_img_counts.close()


	def getWordFrequency(
		self, pWord, pTreatment=None, pImage=None, pPosition=None):
		'''
		Returns the number of occurrences of a word, when looking within a
		specific treatment, at the tags attributed to a specific image, and
		in a specific text input (e.g first, second,..., fifth)

		We can regard the treatment_id, image_id, and position as coordinates
		that successively zero in on a more specificly designed feature of 
		the dataset.  But it makes sense to be able to omit some or all of 
		these.  So, for example, by specifying only a word, but not a 
		treatment_id, image_id, or position, we should receive the frequency
		of occurrence of that word over the entire dataset.  On the other hand
		if we specify the treatment_id and image, but not position, then we 
		should get the number of times a word was attributed to the indicated
		image under the indicated treatment, but without regard for which 
		position it was in.

		For the moment, I will support only specifying a parameter provided
		that the ones earlier in the list are specified.  So, e.g. it is an 
		error to specify which position, but not which image.
		'''
		numOccurrences = 0

		if not self.isAggregated:
			self.aggregateCounts()

		if pWord not in self.counts[(pTreatment, pImage, pPosition)]:
			return 0

		else:
			return self.counts[(pTreatment, pImage, pPosition)][pWord]

				
	def list_counts_for_img(self, img_id):
		'''
		Return a string that lists one word per line followed by its frequency
		sorted in descending order of frequency
		'''
		counts = sorted(
			self.counts[(None, img_id, None)].items(), 
			None, lambda x: x[1], True)

		string = ''
		for word, frequency in counts:
			string += word + ' ' + str(frequency) + '\n'

		return string


	def get_counts_for_treatment_image(self, treatment, image):
		'''
		return word counts, in the form of a Counter, associated to a given
		treatment and image.
		'''
		return Counter(self.counts[(treatment, image, None)])


	def list_counts_for_tmt_img(self, treatment_id, image_id):
		'''
		Return a string that lists one word per line followed by its frequency
		sorted in descending order of frequency
		'''
		counts = sorted(
			self.counts[(treatment_id, image_id, None)].items(), 
			None, lambda x: x[1], True)

		string = ''
		for word, frequency in counts:
			string += word + ' ' + str(frequency) + '\n'

		return string


	def calc_ktop(self, k=K):
		'''
		Determine the most frequent k words in each word position of each
		image for each treatment.  This is stored in self.ktops which has a
		structure that is analogous to the self.counts.
		'''

		# Iterate over all of the counts for specific treatments, images, and
		# word positions
		for treatment, image, position in self.counts.keys():

			# We only want to consider the minimally aggregated data, that is
			# we want counts that are specific to a word-position
			if position is None:
				continue

			word_count_dict = self.counts[(treatment, image, position)]
			ranked_word_counts = sorted(word_count_dict.items(),
					None, lambda x: x[1], True)

			top_k_words = ranked_word_counts[0:k]
			self.ktops[(treatment, image, position)] = top_k_words

		hasKtop = True






