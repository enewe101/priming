'''
This module has one class, the purpose of which is to represent the raw data, 
which was generated by AMT as a csv file.  

It takes care of converting the data from csv into a manageable form in memory.
In doing so, it performs lemmatization, stop-word removal, spelling correction,
and other word normalization, which is expected to improve classification.
'''


import numpy as np
import re
import os
import util
import random
import json
import csv
from collections import defaultdict, Counter
from random import shuffle
from nltk.corpus import wordnet as wn
import nltk


def get_correct_treatments(image_num, pos):
	'''
		helper function to handle the permutation of images that was used
		during experiment 2.  This takes a particular image number, and 
		the desired position in which you wish to find it, then returns the
		treatment indices that were presented said image number in said
		position.  
		
		Example: To find the treatments that had image 2 in position 0:
			get_correct_treatments(2,0) => (2,7)

		Because treatments 2 and 7 were shown image 2 as the first test image.
	'''

	rank = (image_num - pos) % 5
	return (rank, rank+5)


def permute(input_list, class_idx, test_only=True):
	'''
	helper to deal with the fact that images in the second experiment were
	shown in permuted order.  
	Returns an array whose values show the position of the image (and the
	array indices are the image ids)
	So, test_permutation[3] would give the position of image named '3'
	'''

	# note each element with its starting index (implicitly it's image idx)
	input_list = enumerate(input_list)

	# the test images are in range(5,10), but usually since we don't handle
	# the priming images, we want to address them
	# by integers 0 through 4 (i.e. without an offset)
	if test_only:
		offset = False
	else:
		offset = True

	# permute the input list
	output_list = sorted(
		input_list, None, lambda x: get_pos(x[0], class_idx, offset))

	# discard the original indices
	return [x[1] for x in output_list]


def get_pos(img_idx, class_idx, offset=False):

	if not offset:
		return (img_idx - class_idx) % 5

	if img_idx < 5:
		return img_idx

	return (img_idx - class_idx) % 5 + 5


SPLIT_RE = re.compile(r'[^a-zA-Z0-9]')


def simple_dataset_2_naive_bayes(simple_dataset):
	'''
		pass in a SimpleDataset, and get out a dictionary in the format
		expected by the NaiveBayesCrossValidationTester.
	'''
	nb_dataset = defaultdict(lambda: [])
	data = simple_dataset.data

	for class_name in data:
		entries = data[class_name]
		nb_dataset[class_name] = [
			tuple([e['class_idx']] + e['features']) for e in entries
		]

	return nb_dataset


class SimpleDataset(object):
	'''
	this was designed to be well-adapted to training an svm classifier
	'''

	# This class's assumptions
	NUM_PRIMING_IMAGES = 5
	NUM_TEST_IMAGES = 5
	NUM_IMAGES = NUM_PRIMING_IMAGES + NUM_TEST_IMAGES
	NUM_WORDS_PER_IMAGE = 5
	DATA_DIR = 'data/amt_csv'
	EXP1_PATH = 'exp_1_2014.04'
	EXP2_PATH = 'exp_2_2014.09'
	EXP1_FNAMES = ['amt1_cut.csv', 'amt2_cut.csv', 'amt3_cut.csv']
	EXP2_FNAMES = ['amt_cut_2014.09.csv']
	CACHE_PATH = 'cache'
	CORRECTIONS_PATH = 'data/new_data/dictionaries/with_allrecipes/'
	DICTIONARY_FNAMES = ['dictionary_1.json', 'dictionary_2.json']
	WORD_MAP_FNAME = 'mapped_words.json'
	WORD_BREAK = re.compile(r'[^a-zA-Z]+')

	STRIP = re.compile(r"[^a-zA-Z'_-]+", re.I)
	UNDERSCORE = re.compile('_')

	def __init__(
		self, 
		which_experiment=1,
		show_token_pos=True,
		show_plain_token=True,
		show_token_img=True,
		do_split=True,
		class_idxs=[0,1],
		img_idxs=range(5,10),
		spellcheck=True,
		lemmatize=True,
		remove_stops=True,
		balance_classes=True,
	):

		# validate options
		# There were two experiments, each with its own dataset
		assert(which_experiment in [1, 2])

		if not show_token_pos and not show_plain_token:
			print ('Warning: neither plain tokens nor position-prepended '
				'tokens are being included -- the vocabulary will be empty.')

		# register options 
		self.which_experiment = which_experiment
		self.show_token_pos = show_token_pos
		self.show_plain_token = show_plain_token
		self.do_split = do_split
		self.class_idxs = class_idxs
		self.img_idxs = img_idxs
		self.spellcheck = spellcheck
		self.num_examples = None # this only gets set in balance_classes()
		self.show_token_img = show_token_img
		self.lemmatize = lemmatize
		self.remove_stops = remove_stops

		# determine the paths to the desired data
		self.raw_paths = self.resolve_raw_data_paths(which_experiment)

		# state variables
		self.data = defaultdict(lambda: [])
		self.worker_ids = set()
		self.vocab_list = []
		self.vocab_counts = Counter()
		self.num_docs = 0

		# read some associated helper data
		self.read_dictionary()
		self.lmtzr = nltk.stem.wordnet.WordNetLemmatizer()
		self.stops = set(nltk.corpus.stopwords.words('english'))

		# read in the raw data
		self.read_raw_data()
		if balance_classes:
			truncate_to = None

			# if positive int (not a bool) is passed, truncate to that.
			if not isinstance(balance_classes, bool):
				truncate_to = balance_classes

			self.balance_classes(truncate_to)


	def balance_classes(self, truncate_to=None):

		# what is the min number of examples in each class?
		if truncate_to is None:
			truncate_to = min([len(self.data[idx]) for idx in self.data])

		# truncate all data (randomly) to even out number of examples
		for idx in self.data:
			self.data[idx] = random.sample(self.data[idx], truncate_to)

		self.num_examples = truncate_to


	def read_dictionary(self):
		'''
			the dictionaries contain pre-computed mappings from misspelled
			words to correct spellings.  This method loads the dictionaries.
		'''

		# read the dictionaries
		dictionaries = []
		for fname in self.DICTIONARY_FNAMES:
			fh = open(os.path.join(self.CORRECTIONS_PATH, fname))
			dictionaries.extend(json.loads(fh.read()))
			fh.close()

		# re-organize the dictionaries to be indexed by experiment and image
		self.dictionaries = {}
		for d in dictionaries:
			params = d['params']
			exp, img = params['which_experiment'], params['img_idxs'][0]
			results = d['results']
			self.dictionaries[(exp, img)] = results

		# read the word_map -- this contains aliases, which resolve cases
		# where there is more than one way to spell the same word
		fname = os.path.join(self.CORRECTIONS_PATH, self.WORD_MAP_FNAME)
		word_map = json.loads(open(fname).read())

		# we're interested in the entries that have an "alias", which means
		# they are alternate spellings of a cononical word form
		self.word_map = dict([
			(w['word'], w['alias']) 
			for w in word_map if 'alias' in w
		])


	def resolve_raw_data_paths(self, which_experiment):

		if which_experiment == 1:
			raw_fnames = [
				os.path.join(self.DATA_DIR, self.EXP1_PATH, f) 
				for f in self.EXP1_FNAMES
			]

		else:
			raw_fnames = [
				os.path.join(self.DATA_DIR, self.EXP2_PATH, f)
				for f in self.EXP2_FNAMES
			]

		return raw_fnames


	def seen_worker(self, worker_id):
		'''
		check if the worker has been seen before, and record the id if not
		'''

		if worker_id in self.worker_ids:
			return True

		self.worker_ids.add(worker_id)
		return False


	def img_idx_2_pos(self, img_idx, class_idx):
		if img_idx < self.NUM_PRIMING_IMAGES:
			return img_idx

		if self.which_experiment == 1:
			return img_idx

		return (img_idx - class_idx)%5 + 5


	def img_pos_2_idx(self, image_num, treatment_num):
		'''
		Get the image id for the image located at position image_num.
		This is necessary because the positions of images were permuted in
		the second experiment
		'''
		# the priming images are not permuted
		if image_num < self.NUM_PRIMING_IMAGES:
			return image_num

		# the original dataset is not permuted
		if self.which_experiment == 1:
			return image_num

		return (image_num + treatment_num)%5 + 5


	def read_raw_data(self):

		for fname in self.raw_paths:
			self.read_raw_file(fname)


	def cache(self, cache_address, data):
		pass


	def as_vect(self, randomize=True, weights='tfidf'):

		vectors = []
		for class_idx in self.class_idxs:
			for example in self.data[class_idx]:
				if weights == 'tfidf':
					feature_vector = [
						1*np.log2(self.num_docs/float(self.vocab_counts[t]))
						if t in example['features'] else 0
						for t in self.vocab_list
					]
				else:
					feature_vector = [
						1 if t in example['features'] else 0
						for t in self.vocab_list
					]

				vectors.append((feature_vector, example['class_idx']))

		if randomize:
			shuffle(vectors)

		# zip(*list_of_tuples) ==> `unzip`... think about it
		features, outputs = zip(*vectors)
		return features, outputs



	def read_raw_file(self, fname):

		fh = open(fname)
		reader = csv.DictReader(fh)

		for record in reader:

			# Skip duplicate workers.
			if self.seen_worker(record['WorkerId']):
				continue

			self.num_docs += 1

			# The experimental treatment for this worker is its class
			try:
				class_idx = int(record['Answer.treatment_id'])
			except ValueError:
				print record['Answer.treatment_id']

			# we only load data for the desired classes
			if class_idx not in self.class_idxs and self.class_idxs != 'all':
				continue

			# we look at the position where the desired images are found
			# note that the images are permuted based on treatment (class)
			if self.which_experiment==2 and class_idx < 10:
				img_positions = [self.img_idx_2_pos(p, class_idx) 
					for p in self.img_idxs]
			else:
				img_positions = self.img_idxs

			# Iterate over all the images in the image-set
			entry = {'class_idx': class_idx, 'features':[]}
			self.data[class_idx].append(entry)
			for img_pos in img_positions:

				# get the (unpermuted) id for the image
				if self.which_experiment == 2 and class_idx < 10:
					img_idx = self.img_pos_2_idx(img_pos, class_idx)
				else:
					img_idx = img_pos

				word_key_prefix = 'Answer.img_%d_word_' % img_pos
				dictionary = self.dictionaries[(
					self.which_experiment,img_idx)]

				for word_pos in range(self.NUM_WORDS_PER_IMAGE):
					word_key = word_key_prefix + str(word_pos)
					word = record[word_key]
					words = self.normalize_word(
						word, word_pos, dictionary, img_idx)
					entry['features'].extend(words)

			# add the tokens
			self.vocab_counts.update(entry['features'])

		fh.close()
		self.vocab_list = self.vocab_counts.keys()
		self.vocab_list.sort()


	def normalize_word(self, w, p, d, i):

		w = w.lower()

		# replace consecutive wonky characters with a single space
		word  = self.STRIP.sub(' ', w)

		# Maybe do spell correction
		if self.spellcheck:

			# do corrections on a word by word basis
			old_word = word

			# temporarily split for the purpose of spell checking
			words = self.WORD_BREAK.split(word)

			# replace words with correct spellings (we have precomputed
			# which words are misspelled and what their probable replacements)
			words = [d[w] if w in d else w for w in words]

			# map back to aliases, to account for words that
			# have multiple spellings
			words = [
				self.word_map[w] if w in self.word_map
				else w for w in words
			]

			# bust up wordnet's compound words
			words = [self.UNDERSCORE.sub(' ',w) for w in words]

			# put words back together
			word = ' '.join(words)

		# Maybe lemmatize
		if self.lemmatize:

			# temporarily split words for lemmatization
			words = self.WORD_BREAK.split(word)
			words = [self.lmtzr.lemmatize(w) for w in words]
			word = ' '.join(words)

		# Maybe remove stops
		if self.remove_stops:

			# temporarily split words for lemmatization
			words = self.WORD_BREAK.split(word)
			words = [w for w in words if w not in self.stops]
			word = ' '.join(words)

		# Maybe split words
		if self.do_split:
			words = self.WORD_BREAK.split(word)
		else:
			words = [word]

		return_words = []

		# prepend tokens to indicate what position they were in
		if self.show_token_pos:
			return_words.extend(['%d_%s' % (p, w) for w in words])

		# include un-prepended tokens
		if self.show_plain_token:
			return_words.extend(words)

		# maybe add the img_idx to the word
		if self.show_token_img:
			return_words = ['%d_%s' % (i, w) for w in return_words]

		return return_words

