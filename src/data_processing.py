'''
This module has one class, the purpose of which is to represent the raw data, 
which was generated by AMT as a csv file.  

It does heplful things like handle the random
partitioning of treatments into non-overlapping testing and training sets
of a given size, and allowing coarser-grained access to the data (e.g.
how many times does the word "wine" appear in image 2 for treatment 2, as 
opposed to how many times does it appear in pos 2 of image 2 for treatment 2.)

In hindsight, some of the concerns that it handles should maybe belong in the
naive_bayes module, but that's not totally clear.

It also produces printouts of the words used by each treatment for each image
in a simple text format, which makes it easier to look at the data and use
it in other software.  These word-count printouts were produced and are stored
in `<proj-root>/data/word_counts/*`.  Those lists were put out on a per-image
basis, and are so-named.  Files that break out the per-image word counts on
a per-image, per-treatment basis are also there.

These word-count printouts were used to build the ontologies by hand.
'''


import util
import random
import json
import csv




def readDataset():
	'''
	Factory method that builds a CleanDataset from the original Amazon
	Mechanical Turk CSV files
	'''

	# Create a new priming-image-label-experiment dataset
	dataSet = CleanDataset()

	# Read from the raw amt csv files.  
	# Note: order matters!  The older files have duplicates workers that
	# get ignored.  Make sure to read the newer file files earlier
	dataSet.read_csv('amt_csv/amt1_cut.csv', True)
	dataSet.read_csv('amt_csv/amt2_cut.csv', True)
	dataSet.read_csv('amt_csv/amt3_cut.csv', True)

	# The dataset needs to do some internal calts to refresh its state 
	dataSet.aggregateCounts()
	dataSet.calc_ktop(5)

	dataSet.uniform_truncate(125)
	return dataSet


class CleanDatasetException(Exception):
	pass


class CleanDatasetRotationException(Exception):
	pass

class CleanDataset(object):

	# Constants
	K = 5
	NUM_IMAGES = 5
	NUM_WORDS_PER_IMAGE = 5

	def __init__(self):

		# State flags
		self.hasKtop = False		# whether ktop is up-to-date
		self.isAggregated = False	# whether aggregated values are up-to-date

		# Data
		self.dictionary = set()		# a list of all words that occur 
		self.entries = {}			# an entry holds the data for one worker
		self.testEntries = {}		# holds the entries for testing when 
									# sub-sampled
		self.unusedForTest = {}	# keeps track of which entries have 
										# been used for testing so far

		self.unusedEntries = {}		# used to hold entries that are removed
									# during truncation
		self.trainEntries = {}
		self.workerIds = set()		# to prevent duplicates
		self.counts = {}			# stores words and frequency of occurrence
		self.ktops = {}
		self.areTreatmentsEqual = False	# boolean, indicates whether all 
										# treatments have the same number of 
										# entries


	# Use this to make all of the treatments have the same size 
	def uniform_truncate(self, truncateSize=None):

		# Figure out size of smallest treatment
		min_treatment_size = min(
			[len(treatment) for treatment in self.entries.values()])

		# if treatment size isn't specified, use size of smallest treatment
		if truncateSize is None:
			truncateSize = min_treatment_size

		# else check that treatment size is not bigger than smallest treatment
		else:
			if truncateSize > min_treatment_size:
				raise CleanDatasetException('CleanDataset.uniform_truncate: '\
					'truncateSize must not be larger than smallest '\
					'treatment size in data set.')

		# Make all treatments the same size
		for treatment in self.entries.keys():
			
			remaining = len(self.entries[treatment]) - truncateSize
			subsampled_treatment, unused_from_treatment = util.randomPartition(
				self.entries[treatment], truncateSize, remaining)

			self.entries[treatment] = subsampled_treatment
			self.unusedEntries[treatment] = unused_from_treatment

		self.areTreatmentsEqual = True


	def subsample(self, testSetSize):
		'''
		Partitions the dataset into a training set, and test set.  
		The treatments must be of uniform size.  If they are not, then run
		uniform_truncate() with no arguments to make them all the size of 
		the smallest treatment.
		'''

		# State Validation
		if not self.areTreatmentsEqual:
			raise CleanDatasetException('The treatments are not all of the '\
				'same size')

		# Input Validation
		treatment_size = len(self.entries.values()[0])
		if testSetSize > treatment_size:
			raise CleanDatasetException('The test set size cannot be '\
				'larger than the size of the treatments')

		self.trainEntries = {}
		self.testEntries = {}
		self.unusedForTest = {}

		# the testSetSize is everything outside the training set 
		trainingSetSize = treatment_size - testSetSize

		self.testSetSize = testSetSize
		self.trainingSetSize = trainingSetSize

		# when we begin, no treatments have yet been used for testing
		# we'll track which entries have been used based on their workerId
		for treatment, entries in self.entries.items():
			self.unusedForTest[treatment] = set(
				[e['workerId'] for e in entries])


		# set up the first partition of the dataset into training and
		# test set.  Further calls to rotateSubsample will give partitions
		# whose test set is non-overlapping with prior test sets since calling
		# subsample()
		self.rotateSubsample()


	def rotateSubsample(self):

		# clear the existing data partition: we are now re-partitioning the
		# data
		self.trainEntries = {}
		self.testEntries = {}

		# Make sure that we have enough entries that haven't been used for a
		# previous test set, in order to make the next test set
		if len(self.unusedForTest.values()[0]) < self.testSetSize:
			raise CleanDatasetRotationException('cannot rotateSubsample: not '\
				'enough entries available that have not been used before in '\
				'a test set.')

		# Split the entries of each treatment into testing and training sets
		for treatment, entries in self.unusedForTest.items():

			# create a place to put the training and test entries for this
			# treatment
			self.trainEntries[treatment] = []
			self.testEntries[treatment] = []

			# pick out which entries to include in the test partition for
			# this treatment
			test = random.sample(entries, self.testSetSize)

			# partition the entries based on the choice of test partition
			for e in self.entries[treatment]:
				if e['workerId'] in test:
					self.testEntries[treatment].append(e)
				else:
					self.trainEntries[treatment].append(e)

			# The entries used for test can't be used in the future
			self.unusedForTest[treatment] -= set(test)

		# Now that partitioning is done, recalculate aggregates
		self.aggregateCounts()
		self.calc_ktop()


	def getTestInstances(self):
		return self.testEntries


	def aggregateCounts(self):

		# First we need to clear out stale counts
		self.counts = {}

		# Next, we package up note all unique intstances of features
		for treatment, entries in self.trainEntries.items():
			for entry in entries:
				features = filter(lambda (k,v): isinstance(k, tuple),
					entry.items())

				for ((image, position), word) in features:
					self._aggregateCount((treatment, image, position), word, 1)

		# Finally, we roll up the counts to aggregate counts
		# For example, if the same word occurs in different positions of the
		# same image, we would now be able to access the aggregate count for
		# that image, regardless of position
		for treatment, image, position in self.counts.keys():
			count_dict = self.counts[(treatment, image, position)]

			# This should never happen, it was a quick check. Replace with a 
			# test
			if position is None:
				assert(False)

			for word, frequency in count_dict.items():

				# Depending on which of the entries in the key-tuple is left
				# as None, we aggregate counts to various degrees
				self._aggregateCount((treatment, None, None), word, frequency)
				self._aggregateCount((treatment, image, None), word, frequency)
				self._aggregateCount((None, image, None), word, frequency)

				# This entry in counts is a data-set wide count. Maybe that
				# means I should drop the self.dictionary
				self._aggregateCount((None, None, None), word, frequency)

		self.isAggregated = True


	def _aggregateCount(self, key, word, count):

		# If this is a new key, make it
		if key not in self.counts:
			self.counts[key] = {}

		# If this is a new word, make a new word-count entry
		if word not in self.counts[key]:
			self.counts[key][word] = count

		# Otherwise, just increment the word-count entry
		else:
			self.counts[key][word] += count


	def read_csv(
		self,
		fname,
		hold=False):

		self.hasKtop = False
		self.isAggregated = False
		self.areTreatmentsEqual = False

		fh = open(fname, 'r')
		reader = csv.DictReader(fh)

		for record in reader:

			# Skip duplicate workers.
			workerId = record['WorkerId']
			if workerId in self.workerIds:
				continue

			newEntry = {}

			# Note the new worker id
			self.workerIds.add(workerId)
			newEntry['workerId'] = workerId

			# Note the experimental treatment for this worker
			tmt_id = 'treatment' + record['Answer.treatment_id']
			newEntry['treatment'] = tmt_id

			if tmt_id not in self.entries:
				self.entries[tmt_id] = []

			self.entries[tmt_id].append(newEntry)

			# Record the image files used to prime this worker
			newEntry['primingImageFiles'] = []

			# Do the following for both the priming and testing image-sets
			for sub_treatment in ['prime', 'test']:

				# Iterate over all the images in the image-set
				for img_num in range(self.NUM_IMAGES):

					# The test images are numbered sequentially, following the
					# priming images, so we need to apply an offset
					offset = 0 if sub_treatment=='prime' else self.NUM_IMAGES
					amt_img_num = img_num + offset

					# This is how we name images in the dataset
					img_id = sub_treatment + str(img_num)

					# Record the name of the file for this image
					newEntry['primingImageFiles'].append(
						record['Answer.img_%d_id' % amt_img_num])

					# Now, within the data recorded for each image, 
					# iterate over each position.  These correspond to the
					# text-inputs in the HIT
					for word_pos in range(self.NUM_WORDS_PER_IMAGE):

						# Get the word from the csv file, normalize to 
						# lowercase, store it in the record, and add it to
						# the data-set-wide dictionary
						word = record['Answer.img_%d_word_%d' 
							% (amt_img_num, word_pos)].lower()
						newEntry[(img_id, word_pos)] = word
						self.dictionary.add(word)

		# Make the training entry set be the full entry set, and make the
		# test set empty.  Subsampling changes this partitioning
		for treatment, entries in self.entries.items():
			self.trainEntries[treatment] = list(entries)
			self.testEntries[treatment] = []

		# Check if the treatments all have the same size
		last_treatment_size = None
		areTreatmentsEqual = True
		for treatment, entries in self.entries.items():

			# for the first treatment, just record its size
			if last_treatment_size is None:
				last_treatment_size = len(entries)

			# for subsequent treatments, check if they have the same size
			else:
				if len(entries) != last_treatment_size:
					areTreatmentsEqual = False

		self.areTreatmentsEqual = areTreatmentsEqual
				
		# Update the aggregated counts and the k-top words (except if held)
		if not hold:
			self.aggregateCounts()
			self.calc_ktop(self.K)


	def write_counts(self, directory):

		# Make sure that the directory ends with a slash
		# TODO check that it exists
		if not directory.endswith('/'):
			directory += '/'

		for image_id in self.counts_by_image.keys():

			# Open a file for image counts for this image
			# Write the counts, then close.
			fh_img_counts = open(directory + image_id + '.txt', 'w')
			fh_img_counts.write(self.list_counts_for_img(image_id))
			fh_img_counts.close()

			for treatment_id in self.counts_by_tmt_image.keys():


				# Open a file for image counts for this image and treatment
				# combination.  Write the counts, then close.
				fh_tmt_img_counts = open(
					directory + image_id + '_' + treatment_id + '.txt', 'w')
				fh_tmt_img_counts.write(
					self.list_counts_for_tmt_img(treatment_id, image_id))
				fh_tmt_img_counts.close()


	def getWordFrequency(
		self, pWord, pTreatment=None, pImage=None, pPosition=None):
		'''
		Returns the number of occurrences of a word, when looking within a
		specific treatment, at the tags attributed to a specific image, and
		in a specific text input (e.g first, second,..., fifth)

		We can regard the treatment_id, image_id, and position as coordinates
		that successively zero in on a more specificly designed feature of 
		the dataset.  But it makes sense to be able to omit some or all of 
		these.  So, for example, by specifying only a word, but not a 
		treatment_id, image_id, or position, we should receive the frequency
		of occurrence of that word over the entire dataset.  On the other hand
		if we specify the treatment_id and image, but not position, then we 
		should get the number of times a word was attributed to the indicated
		image under the indicated treatment, but without regard for which 
		position it was in.

		For the moment, I will support only specifying a parameter provided
		that the ones earlier in the list are specified.  So, e.g. it is an 
		error to specify which position, but not which image.
		'''
		numOccurrences = 0

		if not self.isAggregated:
			self.aggregateCounts()

		if pWord not in self.counts[(pTreatment, pImage, pPosition)]:
			return 0

		else:
			return self.counts[(pTreatment, pImage, pPosition)][pWord]

				
	def list_counts_for_img(self, img_id):
		'''
		Return a string that lists one word per line followed by its frequency
		sorted in descending order of frequency
		'''
		counts = sorted(
			self.counts[(None, img_id, None)].items(), 
			None, lambda x: x[1], True)

		string = ''
		for word, frequency in counts:
			string += word + ' ' + str(frequency) + '\n'

		return string


	def list_counts_for_tmt_img(self, treatment_id, image_id):
		'''
		Return a string that lists one word per line followed by its frequency
		sorted in descending order of frequency
		'''
		counts = sorted(
			self.counts[(treatment_id, image_id, None)].items(), 
			None, lambda x: x[1], True)

		string = ''
		for word, frequency in counts:
			string += word + ' ' + str(frequency) + '\n'

		return string


	def calc_ktop(self, k=K):
		'''
		Determine the most frequent k words in each word position of each
		image for each treatment.  This is stored in self.ktops which has a
		structure that is analogous to the self.counts.
		'''

		# Iterate over all of the counts for specific treatments, images, and
		# word positions
		for treatment, image, position in self.counts.keys():

			# We only want to consider the minimally aggregated data, that is
			# we want counts that are specific to a word-position
			if position is None:
				continue

			word_count_dict = self.counts[(treatment, image, position)]
			ranked_word_counts = sorted(word_count_dict.items(),
					None, lambda x: x[1], True)

			top_k_words = ranked_word_counts[0:k]
			self.ktops[(treatment, image, position)] = top_k_words

		hasKtop = True






