# make data processer load images in the correct way for new data
	x test that it attributes images in the right way

# make the NB classifier calculate likelihood properly
	x sum of log probabilities
	x add one smoothing
	x make a tests with synthetic classification data

# make a cross-validator
	x test for proper data partitioning
	x test for proper performance on synthetic data

# Functional parts that need to be built:
 	x dataset adaptor: 
		x repackages clean dataset into class-sorted examples in the format
			expected by the naive bayes classifier cross validation test
		x allows one to select labels are included (i.e. labels attributed to
			which image). 

	x analysis function:
		x reads in old dataset
		x uses a dataset adaptor to take features associated to test0
		x performs cross validation on IMG-FOOD and IMG-CULT
		x repeats, using the new dataset

	x create a specificity tester for wordnet
		x map words from treatments A and B into respective synset counts
		x given a synset from A, how many synsets in B are ancester do it?
			descendant from it?
		x count ancester minus descendant for each synset in one example set
		[] create a symmetric null model, and use this to estimate variance

	x create a foodishness tester
		- synsets to include:
			food.n.02
			food.n.01
			helping.n.01
			taste.n.01
			taste.n.05
			taste.n.06
			taste.n.07

	x create a similarity tester based on wordnet

	x create code to identify misspelled words
		x firstly, just count them to find out how many there are
		x next, create a misspelling-dataset that preserves the misspelled
			word's connection to a specific user, position, and image
			I probably want to connect the misspelled words to the 
			corresponding image as a guide for correction
		
	
# analysis 
	x the more similar the priming images are to the test images, the more
		specific are the labels
		- IMG:AMBG is most similar to TEST, followed by IMG:INGR, and 
			IMG:CULT is most different from TEST 
			This result holds in the old dataset where similarity is 
			intuitive.  But it also holds in the new dataset, where it is less
			obvious whether the food or the object initial set is more 
			similar to the food set.  Based on our similarity metric, the 
			food initial set is more similar to the test images in the second
			dataset, and we also see that it has greater specificity

		- What about the specificity of framing treatments?
			The results here are ambiguous, and not strong.  Sometimes the
			corresponding frame (e.g. IMG:FOOD <-> FRM:FOOD) is more specific
			sometimes less, but not strongly so

	x which of the treatments can be told appart by the Naive Bayes classifier
		and how well?




			

# What are the interesting analyses to do?
	old data:
		[] difference in priming between IMG-FOOD and IMG-CULT
		[] specificity testing, compare IMG-FOOD, IMG-CULT, IMG-INGR
			in the degree of specificity of their test-labels, and 
			measure the similarities of each of these to the testing images

	new data:
		[] difference in priming between IMG-FOOD and IMG-OBJ
		[] difference in priming between FRM-FOOD and FRM-OBJ
		[] longitudinal difference in priming between IMG-FOOD and IMG-OBJ
		[] fraction of food-related words in IMG-FOOD and IMG-OBJ
			fraction of non-food-object-related words?



