\documentclass[letterpaper, 11pt, twocolumn]{article}
\usepackage[margin=1in]{geometry}
\setlength{\columnsep}{0.75cm}

\usepackage{fixltx2e}
\usepackage{tikz}
\usetikzlibrary{calc}
\usepackage{algorithm2e}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{cite}
\usepackage{framed, color}
\usepackage{color, soul}
\definecolor{shadecolor}{rgb}{.93,.93,.93}
\definecolor{blu}{rgb}{0,0,1}
\newcommand{\td}[1]{{\color{blu}\hl{TODO: #1}}}
\newcommand{\vwm}{$V_{w,n,k}$}
\newcommand{\wm}{$W_{w,n,k}$}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{gensymb}

\title{Climate control for performant HPUs}
\author{}
 
\parindent0pt \parskip8pt
\begin{document}
\maketitle

\section*{Abstract}
\textbf{}
\section*{Introduction}
There are still many tasks for which humans outperform computers
These tend to be tasks that rely on a large repertoire of prior knowledge,
the excercise of common sense judgment, certain visual tasks, and 
tasks that require spontaneous hypothesis generation.

With the emergence of microtask platforms like Amazon Mechanical Turk (AMT),
comes the hope of building compute in which human intelligence can be accessed
on-demand.  This leads to an analogy of the human to the CPU, wherein we think
of human processing units.

A major challenge in building systems that deliver on this vision is the 
natual variability obtained in HPU output.  In some tasks, this variability
is desireable.  For example, in an image labelling task, the natural 
variability in
HPU output leads to greater coverage of the semantic space occupied by an 
image.  In other cases this variability can be problematic, such as in a 
transcription task, where there is only one corect output, and any variability
is only noise.

Whether desired or not, to build up efficient systems from HPUs, one must 
understand the natural variance quantitatively, and understand the factors that
influence it.

Here we model the variance between HPUs as comming from two sources.  The first
is a persistent, or intrinsic variability.  We imagine that this to encompass  
temperamental predispositions, life history, and developmental stage of the 
individual.  While
it may change over the course of a person's lifetime, in the view of the 
computational architect, it is essentially constant.

The seccond is a short-term variability, which arises from the fact that 
peoples immediate state is a function of recent events.  This would include
such things as an HPU becomming more efficient at a task once it has completed
a few tasks of the same type.  It would also include such things as the 
potentially detrimental effects of a noisy environment.

In this view, one can imagine HPUs being different in their specifications,
and also exhibiting hysteresis, whereby their output at some time $t$ is a
function of their inputs at time $t$, $t-1$, and so on.

In psychology, the act of producing these short-lived states that effect a
person's performance during a task is called \textit{priming}.  We will adopt
this term throughout the present work.


\section*{Prior Work}
Priming itself can be broken into many types, and many studies exist exploring
the effects of overt priming on the performance of HPUs. Priming may arise
due to how the task is framed.  This includes such things as the stated
purpose of the task, and the identity of the requester the task.

In \cite{chandler2013breaking}, the researchers investigate the effects of 
framing task either in a meaningful or meaningless way.  Compared to a 
zero-context control treatment, workers increase their output (for less pay),
when they are told that they are helping identify cancer, but there is no
change in quality.  When workers are told that their submissions will be 
discarded, there is no change in the amount of work done, but the quality 
declines.

Another source of priming can be due to what might be called 
\textit{sidestream information}: information that is presented to the 
individual but which is not actually salient to the task.

Other researchers have studied how having peer's responses available to 
the task could influence the worker.

In the present work, we focus on a more subtle source of priming, which arises
simply from the worker's performance in earlier stages of the task. 

We compare this to priming arising from framing the task by disclosing a 
(fictitious) funding agency that is paying for the study in which they are 
participating.

\section*{Methods}

\textbf{Task set-up.}
We paid 900 AMT workers to perform an image-labelling task.  A task consisted 
of labelling 10 images, with 5 labels each.  The first 5 images were varied 
depending on the priming treatment, while the last 5 images were the same 
accross all treatments.  Ordering of the images was kept constant.

Workers were randomly assigned to one of 6 treatments.  The treatments differed
from one another along two dimensions. The first dimension consisted of 
varying the first 5 images shown to the worker.  This was used to test the
effects of \textit{in-task} priming.

The second dimension concerned disclosure of a (fictitious) organization,
purportedly funding work as part of a research study.  Depending on the 
treatment, one of two funding agencies was presented, or no indication was 
made.

Tasks were presented to workers as a series of panels or flash cards.  The
first panel provided brief instructions, and was identical for all treatments.
Workers could see this panel when previewing the task, but could not advance.
Depending on the treatment, the worker was either shown a second pannel 
stating the name of 
one of two fictitious organizations funding the work, or this panel was 
skipped.  The next five pannels each consisting of a priming sub-tasks, 
wherein the worker was asked to submit five descriptive labels.  The images
used during the priming sub-task depended on the treatment.  The last 5 pannels
consisted of testing subtasks, wherein, as for the priming sub-tasks, workers
were asked to submit 5 descriptive labels.

\textbf{Choice of images.}
The 5 test images, were chosen with two ideals in mind.  
First, we chose images that we judged would generate a diverse vocabulary of 
labels, such that the effects of priming could be detected.  In other words,
sparse images with a single object in the foreground were not considered good 
candidates, since they would be less likely to elicit labels that varied from 
one worker and one priming treatment to the next.

Second, we chose images which would produce labels belonging to two broad
concepts, which would serve as the targets of our priming: food and culture.  
This created the opportunity to attempt to prime workers in a way that would
bias them toward emitting food-related or culture-related labels.

Under these considerations, we chose the images shown in 
Fig.~\ref{fig:testImages}.  Each of these images has food as its main focus,
but also has a strong and specific cultural reference due to the unique, 
iconic character of the food and the artifacts depicted.

To investigate in-task priming, we chose a set of images that highly
recognizeable cultural settings and no food, and another set that contained
separated food ingredients, without any overt cultural content.  The third
set of images was chosen to be very much like the test images, showing prepared
meals, and though prepared food is inseperable from culture, these images
were chosen based on being culturally more muted or ambiguous. 

\textbf{Label ontology.}
In order to provide a deeper analysis, we built an ontology
of the corpus of all labels applied to the first test image.  The ontology
was built as a directed acyclic graph starting 

\section*{Results and discussion}

\subsection*{Priming affects HPU output.}
Before looking for differences in the content of labels provided by workers
from different treatments, we first demonstrate that each treatment has a
distinguishable impact on worker output relative to the \textsc{ambg}.

Using a naive bayes classifier, we are able to distinguish with high precision
and specificity between workers from the \textsc{ambg} treatment and any of
the other 5 treatments.  Fig~\ref{fig:classifier} shows F1-score for a
naive bayes classifier when distinguishing between the \textsc{ambg} and 
target treatments. The classifier was trained using random subsamples of 
output 80 HPUs drawn from each of the \textsc{ambg} and target treatments, and
tested on the ouput of 20 HPUs from each treatment.

We find it remarkable that, using only the labels that workers provide, it is
possible to infer with good accuracy the treatment to which the worker was 
subjected.

\begin{figure*}
	\includegraphics[scale=0.65]{../figs/f1scores.pdf}
	\caption{caption here}
	\label{fig:classifier}
\end{figure*}


A natural question is to test the ability for the classifier to perform 
distinguish between other pairings of treatments.  For example, can the 
classifier distinguish $\textsc{cult}_{img}$ workers from 
$\textsc{cult}_{fund}$ workers?

% Graphics commented out because they make compiling take long!  
% Uncomment when desired!
\begin{figure*}
%	\includegraphics[scale=1.00]{../figs/testImages.png}
	\caption{caption here}
	\label{fig:testImages}
\end{figure*}

\begin{figure*}
%	\includegraphics[scale=1.00]{../figs/ambiguous.png}
	\caption{caption here}
	\label{fig:ambiguous}
\end{figure*}

\begin{figure*}
%	\includegraphics[scale=1.00]{../figs/cultural.png}
	\caption{caption here}
	\label{fig:cultural}
\end{figure*}

\begin{figure*}
%	\includegraphics[scale=1.00]{../figs/ingredients.png}
	\caption{caption here}
	\label{fig:ingredients}
\end{figure*}

\section*{Results}
\begin{figure*}
	\includegraphics[scale=0.65]{../figs/specificity2.pdf}
\end{figure*}

\bibliographystyle{plain}
\bibliography{bab.bib}
\end{document}
