%        File: pnas1.tex
%     Created: Wed Apr 30 04:00 pm 2014 E
% Last Change: Wed Apr 30 04:00 pm 2014 E
%
\documentclass[a4paper]{report}
\usepackage[hmargin=1in,vmargin=1in]{geometry}

\usepackage{graphicx}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{gensymb}
\usepackage{algorithm2e}
\usepackage{amsthm}

\newtheorem*{mydef}{Definition}

\usepackage{cite}

\usepackage{framed, color}
\usepackage{soul}
\usepackage[colorlinks=true, urlcolor=blue]{hyperref}

\newcommand{\td}[1]{{\color{blu}\hl{TODO: #1}}}

\definecolor{shadecolor}{rgb}{.93,.93,.93}
\definecolor{blu}{rgb}{0,0,1}
\setlength{\parindent}{0cm}
\setlength{\parskip}{4mm plus1mm minus1mm}


\title{The effects of priming in human computation}
\author{Edward Newel, Derek Ruths}
 
\parindent0pt \parskip8pt

\begin{document}
\maketitle
\section*{Abstract}
\section*{Significance Statement}
\section*{Introduction}

Microtask crowdsourcing platforms like Amazon Mechanical Turk (AMT) make it 
possible to submit batches of small tasks to a large pool of human workers, 
who do the tasks for fun and remuneration.  A typical task might be the 
flagging of innapropriate images, something that people are still better at 
than computers.

Such platforms render human cognition as fluid a resource as compute cycles: 
it is available on demand in small units, with low transaction cost, from 
anywhere 
via the Internet \footnote{\href{http://mturk.com}{http://mturk.com}}.  The 
result is that one can treat AMT like a compute cluster, and seemlessly 
integrate human and machine intelligence in large-scale computations.

This development invites a new perspective on the role of human cognition 
in technical systems.  Researchers have put forward the term HPU 
(Human co-Processing Unit)\footnote{We acknowledge the irony of this term:
the original meaning of `computer' was a person who performs computations,
its reference to machines being a personification brought into usage in the 
late 1800s\cite{Dictionary:hl}.}, viewing the introduction of 
microtask platforms as a fundamentally new computing architecture
\cite{5543192}.  Thinking of networked 
people as compute systems is not new.  Working in operations research, Simon 
and Newell viewed the primary activity of human organizations to be 
computation\cite{Simon:1958fp}. 

In adopting this view, we must contend with a stark difference between 
people and CPUs: even when given the same instructions, people do not produce
the same output.  In other words, one pays for the ``high level instructions 
set'' of HPUs by accepting variability in outputs.

Variability raises a quality control issue. There have been many studies 
into how user interface design, payments, social reputation schemes, and the 
disclosure of the wider purpose of tasks affect worker outputs. We will 
review the prior work in the section following this one.
{\em Here, we report our investigation of worker {\em hysteresis}, that is, 
the tendency of a worker's output during one task to be influenced by other 
recently completed tasks}. 

We test hysteresis by presenting AMT workers with
five images that depict meals with strong cultural overtones, and ask them to
label each image with descriptive words.  Before presenting test images,
we expose the workers to different priming treatments.  We observe how priming
treatments effect the workers' subsequent choice of labels, by looking at 
how specific the labels are, and how often workers make 
cultural references.

We employ two priming modalities in this study.  
In the first modality, which we call \textit{in-task priming}, workers are 
shown one of three sets of priming images.  The \textit{cultural}
image set, \textbf{Fig. 2}, shows images chosen for their clear depiction of an
iconic cultural scene.  The \textit{ingredients} image set, \textbf{Fig. 3}, 
shows separated food ingredients, as opposed to the prepared meals shown in 
the test set, and avoids strong cultural signal.  The \textit{ambiguous} 
image set contains images of prepared meals, and is most similar to the test 
set, but there are no prominent traces of cultural reference.  These images 
are shown right
before the test images, and they must be labeled in the same way.  From the 
workers' view-point there is no distinction between the priming and test 
images. 

The second priming modality, which we call \textit{framing}, consists of
presenting the workers with the name of a (fictitious) research funder before
presenting any images to be labeled.  Workers are shown either the funder 
name  
``The National Foundation for Nutritional Awarenes'' or ``The Global
Foundation for Cultural Recognition'', or else they are shown no name.  These 
names were chosen to signal interest, on the part of the requester, in the
cultural or food-related aspects of the images.

Our primary interest lays in the hysteretic effects of in-task priming.  We
include framing mainly as a point of comparison.  A defensible expectation is
that the framing would produce a much stronger effect, because it clearly 
signals the requester's interest in a specific aspect of image content. 
Remarkably, with respect to the specificity of labels, and their content 
(either food- or culture-orineted), in-task priming produces much stronger 
effects.

However, to measure the effects of priming, one generally has to make an 
assumption about what those effects will be.  At risk of circularity, the 
definition of priming itself is traditionally operationalized in terms 
of these effects \cite{}.  We seek to alleviate this 
situation by providing an application-independant definition of priming 
suitable for studying HPUs hysteresis.  We 
present this definition following the section on prior work.

\subsection*{Prior Work}
- human computation pioneered by von Ahn and L Dabbish when they demonstrated
	that images could be labeled by people via a game platform
	\cite{von2004labeling}

- Human computation is generally thought of in the context of crowdsourcing,
	which implies soliciting work in the form of an open call to an 
	unstructured pool of workers, general without any longterm engagement and 
	limited ability to screen workers screening of workers. 
	\cite{howe2008crowdsourcing}

- As such, quality assurance has received a lot of attention.
	- varying levels of attention and success \cite{kazai2013analysis}
	- Widely varying reports on accuracy from very negative 
	\cite{marsden2009crowdsourcing}, to positive 
	\cite{alonso2009can, grady2010crowdsourcing}
	- quality control \cite{lease2011quality}
		o filtering 
			- honeypots	\cite{snow2008cheap}
		o averaging
		- \cite{alonso2009can}, \cite{paolacci2010running}

	- training \cite{le2010ensuring}

	- prescreening, even for cultural effects
		\cite{paolacci2010running}
			- worker rep
			
	- game theoretic treatments
	- worker qualification 					  |	
	- price									  | \cite{kazai2013analysis}
	- task complexity						  |
	- spam, variability, adversarial behavior |


- effects of motivation
	for fun does badly, for money does well
	claimed expertize does badly
	interest does better than boring, and boring performs faster, 
	difficult does worse
	pay 'ok' does best

- studied effects of autonomy, skill use, and meaningfullness, in 
- other studies study the effects of reputation and social standing
- studied effect of giving workflow context\cite{Kinnaird2012281}
- studied effects of task design (user interface): \cite{Finnerty2013}

NOTES ON PRIMING
- when shown an image, a person is better at recognizing the image in a 
	tachistoscope up to 45 min later.\cite{BJOP:BJOP1796}
	* the mechanisms are complex: showing an image of something that had the
		same name also helps recognition, but being presented with a word and
		reading it aloud does not


- priming can be auditory(\cite{BJOP:BJOP1826}) or visual(\cite{BJOP:BJOP1796})

- priming has been detected for words sharing morphemes 
	(reflects vs reflecting) but not for semantically related words that such 
	as held and holding.\cite{BJOP:BJOP1826}
	- suggests stimulus encoding \cite{beller1971priming}\cite{BJOP:BJOP1826}
	\cite{BJOP:BJOP1796} and memory access \cite{beller1971priming} priming 
	effects exist.  Other studies provide definitive evidence that priming
	leads to better application of object knowledge and top-down processes
	\cite{Ghuman17062008}

- existing work suggests that there are separble verbal- and image-based
	semantic systems, as shown through studies of event-related brain 
	potentials in response to high and low imageable words \cite{Swaab200299} 
- priming is an effect whereby previous exposure to a stimulus leads to 
	faster, more accurate responses to similar stimuli \cite{Ghuman17062008}, 
	or lower response thresholds \cite{BJOP:BJOP1826}
	

\section*{Text}
\subsection*{An algorithmic definition of priming}
One difficulty with the notion of priming is that there appears to be no 
defensible notion of an ``unprimed state''.  Consider the ambiguous image set 
in our experiment.  Would we be better to include a treatment with no
priming images at all?  We argue that, in that case, one would mainly observe  
effects of \textit{primacy}.  
Further, a participant always enters the experiment in \textit{some} state of 
mind, depending on her exposures prior to the experiment, which cannot be 
known by the experimenter.  By including a priming exposure that is in some 
sense `neutral' for the purposes of an experiment, one at least makes the 
priming of participants more uniform.  

Because there
can be no unprimed state, we define priming relatively, and speak of one
treatment being \textit{differently primed} from another.

Another factor that we must consider is that the effects of priming will
depend on the task presented to the participant.  Two priming treatments that
produce markedly different performance for one task might produce 
indistinguishable performance for a diffirent task.  Thus, we define priming
in relation to the task.

A final factor that we must consider is the fact that, even when priming 
treatments are taken into account, the performance of participants in a task 
will differ due to the natural variance between people.  There would appear to
be no hope of accounting for the myriad factors that drive this variance.  Thus
to be practical, priming must be defined in relation to populations, not
individuals.

With these considerations, we propose the following algorithmic definition of
priming:

\begin{mydef}
	Two populations of people $J$ and $K$ that produce respectively two 
	populations of outputs $\mathcal{J}$ and $\mathcal{K}$ in the completion
	of a task $\mathcal{T}$, are said to \emph{differ in priming by 
	a degree at least $\theta$}, if there exists an algorithm $\mathcal{A}$ 
	that can distinguish (classify)
	a person drawn uniformly from $J$ or $K$ based on the outputs $x$ of the 
	person during $\mathcal{T}$ with accuracy $\frac{1+\theta}{2}$.
	$\mathcal{A}$ receives as input $\mathcal{J}$, $\mathcal{K}$, and $x$,
	and must have a running time that is polynomial in the size of its input.
\end{mydef}

In the language of computer science, $\mathcal{A}$ is a classifier.  Thus,
intuitively, if there exists a classifier that can distinguish two populations,
then they are differently primed.

Other than providing the usual running time constraint on $\mathcal{A}$, we
leave it completely undefined.  This is intentional since we want the 
definition to depend on the inherent distinguishability of populations, and
restricting the basis of distinction would arbitrarily mask certain kinds of
distinctness.

One consequence of this choice is that empirical measures of $\theta$ always
represent lower bounds on the theoretical value appearing in the definition.
So, empirically demonstrating non-negligible $\theta$ establishes a priming 
difference of \textit{at least} $\theta$, but on measuring a negligible 
$\theta$, one might argue that $\theta$ is not really negligible, and rather 
that a bad choice of classifier was made.

In practice we do not expect any serious difficulty on choosing a classifier
and establishing a meaningful value of $\theta$.  One of the simplest 
classifier algoritms, the \textit{Naive Bayes classifier}, performs very 
nearly as well as the most sophisticated classifiers in nearly all 
applications.  Therefore, we submit $\theta_\text{NB}$, as determined by a 
Naive Bayes classifier, to be a general-purpose measure of priming difference.

\subsection*{Results}
\paragraph{All treatments were differently primed.}
We test for priming effects using our definition proposed in the introduction,
implemented with a Naive Bayes classifier.  \textbf{Figure 1} shows
the classifier performance in distinguishing between different pairs of 
treatments, when provided workers' labels for the test images.

The classifier distinguishes the various treatments from one another (pairwise)
with high accuracy, confirming that both priming modalities 
(in-task and framing) were effective.  It is perhaps not surprising that
a Naive Bayes classifier can distinguish treatments tending toward a different 
``orientation'', e.g. $\textsc{cult}_{img}$ and $\textsc{ingr}_{img}$
(\textbf{Fig. 1A}).
But it is remarkable that, based only on the labels provided for the five test 
images, the classifier can distinguish workers from treatments of the 
\textit{same} orientation, such as $\textsc{cult}_{img}$ and 
$\textsc{cult}_{fund, img}$, with high accuracy (\textbf{Fig. 1B}).

Next we look at the degree to which labels from individual test images reveal 
priming.  It would stand to reason that, as workers proceed through 
the test images, the effects of priming might be ``washed out''.  
\textbf{Figure 2} shows the
performance of the Naive Bayes classifier when using the labels workers provide
for particular images.  This does show that, as we look to later test images, 
the treatments' difference in priming, $\theta_\text{NB}$, declines.  

\paragraph{Priming orients focus.} Having demonstrated differential 
priming, we investigate the \textit{nature} of the priming.  To this
end, we construct an ontology of all the labels produced by the workers
for the five test images.  The ontology is a directed acyclic graph of terms,
with directed edges drawn from abstractions toward instances.  For example,
our ontology contains the path \texttt{food} $\to$ \texttt{ingredients} $\to$ 
\texttt{vegetables} 
$\to$ \texttt{tomato}.  In general, each of these terms is a label that
occured in the corpus of labels provided, but in some cases we added 
an abstraction when it was missing, to connect the ontology.

At the root of the ontology are the abstract classes \texttt{activity}, 
\texttt{adjective}, \texttt{culture}, \texttt{food}, and \texttt{thing}.
By simply counting the number of labels descended from \texttt{culture}
and \texttt{food}, we can guage effects on the orientation of workers's focus
with respect to these concepts.

\textbf{Figure 3} plots the composition of labels (fraction of culture- and 
food-oriented labels) for different treatments and images.  In 
panel \textbf{A}, we can see that, before considering the effects of 
treatments, there is significant variance in the composition of the labels 
for different images due to inherent differences in their content.  For our 
purpose, we are interested in the differential of composition between 
different treatments.

In \textbf{Fig. 3B} we exhibit the composition of labels for the various 
treatments (aggregating together the labels for all images for a given 
treatment).  We can see that the treatments which expose workers to the
cultural image sets do elicit significantly more culture-oriented and fewer
food-oriented labels compared to the other treatments ($\alpha=0.05$).  It 
also apperas that all the $\textsc{ingr}_x$ treatments produce more 
food-oriented labels, but we cannot assert this at significance 
$\alpha = 0.05$. 

The effect size, however, is somewhat small.  Since we observed that priming 
difference ``wears off'', it may be the case 
that aggregating label compositions for all images actually masks the effects. 
In \textbf{Fig. 3C}, we plot label compositions for only the first
image.  This reveals that, in labeling the first test image, workers in the
$\textsc{cult}_{img,x}$ were strongly influenced to favor culture- over 
food-oriented labels.  The effect was strong enough to essentially reverse 
the proportions of these labels compared to the other treatments.

We next inspect this effect longitudinally, that is, as a function of 
the test image.  To do so, we compute the ``excess cultural orientation'' 
($\Delta_{cult}$) of
the $\textsc{cult}_{img}$ relative to $\textsc{ambg}$ as follows: 
\begin{align}
	\Delta_{cult}(i) = \frac{1}{N}\left[ \sum_{w\in\textsc{cult}_{img}} \left(N_{w,cult}^{(i)} - N_{w,food}^{(i)}\right)
	- \sum_{w\in\textsc{ambg}} \left(N_{w,cult}^{(i)} - N_{w,food}^{(i)}\right)\right],
\end{align}
where $N_{w,cult}^{(i)}$ stands for the number of culture-oriented labels 
attributed by worker $w$ to image $i$, while $N_{w,food}^{(i)}$ similarly 
counts food-oriented labels, and $N$ is the total number of labels in a 
treatment (2500).  
Intuitively, this is intended to capture how much 
more culture-oriented $\textsc{cult}_{img}$ is than $\textsc{ambg}$.  
\textbf{Figure 4} plots $\Delta_{cult}$ as a function of test image, $i$. It
clearly shows the strong influence on the labels attributed to the first image,
but this effect immediately drops below statistically measureable levels, while
remaining always positive.

\paragraph{Priming affects attention to detail.} Using the ontology described
in the previous section, we compare treatments in their degree of specificity.
When comparing two labels $\ell_1$ and $\ell_2$ produced by different workers,
we say that $\ell_2$ is more specific than $\ell_1$ if there is a 
\textit{directed path} from $\ell_1$ to $\ell_2$.  If there is no directed path
between $\ell_1$ and $\ell_2$, then they are uncomparable.  As an example,
\texttt{tomato} is more specific than \texttt{food}, while \texttt{statue}
and \texttt{food} are uncomparable.

We can compare the bulk specificity of two treatments by taking a subsample
of workers from each, and the labels of the workers in one
subsample to each of those in the other.  In drawing this comparison, we only 
compare labels that were attributed to the same image.  Thus the relative 
specificity of two treatments can then be computed as:
\begin{align}
	S(\textsc{x},\textsc{y}) = 
		\sum_{i=1}^5 \; 
		\sum_{\ell\in \mathcal{L}(\textsc{x},i)} \;
		\sum_{m\in \mathcal{L}
		(\textsc{y},i)} \left(\mathbf{1}[\ell > m] 
		- \mathbf{1}[m>\ell]\right),
\end{align}
Where \textsc{x} and \textsc{y} are treatment subsamples,
$\mathcal{L}(\textsc{x},i)$ is the set of all labels produced by workers in 
$\textsc{x}$ for test image $i$, and $\mathbf{1}[\ell > m]$ evaluates to 1 if 
$\ell$ is more specific than $m$, and 0 otherwise.  \textbf{Figure 5} 
exhibits the results.

In panel \textbf{A}, we see that when we compare each of the treatments to
$\textsc{ambg}$, all treatments in which the non-ambiguous image set was 
displayed have \textit{lower} specificity (though not all at $\alpha=0.05$).  
In panels \textbf{D} and \textbf{G} we compare the same pairs of treatments, 
but restrict to words of culture- and food-orientation respectively (words
of both orientations are excluded).  These plots show that the relative lack of specificity is mainly attributable to food-oriented words.  

Likewise, in panels \textbf{B}, and \textbf{C}, 
wherever we see significant differences in specificity, it turns out that
these differences appear to mainly be due to differences in food specificity,
as can be seen by comparing to panels \textbf{E}, \textbf{F}, \textbf{H}, and 
\textbf{I}.  In fact, looking to panels \textbf{D}, \textbf{E}, and \textbf{F},
there are no significant differences in cultural specificity to speak of. 

\subsection*{Discussion}

\paragraph{In-task priming is strong.}  Perhaps the most unexpected result of 
this study is the finding that the in-task priming was more influential than
framing.  We expected that the funder names would provide an overt signal of 
intent behind the task, and so would be stronger than in-task priming which 
we expected would not signal intent.  Perhaps in applications where priming 
is a concern, a certain portion of the task should be devoted to  
``burn-in''.  During burn-in, the participant would acclimatize to the task, 
while the effects of stimuli preceeding the task are washed out.

\paragraph{The rate of priming wash-out depends on how it is measured}
Seeing that $\theta_\text{NB}$ declines as one proceeds through the test 
images (\textbf{Fig. 2}), one should ask whether this actually represents 
priming wash-out.  One might also attribute this to 
differences in the \textit{content} of the images.  Nevertheless, we can
certainly assert that the effects of priming detected by the classifier 
persisted through the labelling of all five test images.  

It is interesting to compare this to the result in \textbf{Fig. 4}, which
shows that the effect of priming on \textit{orientation} (culture vs food) 
decays much more quickly than the overall effects detected by
the classifier.  We take this to be the result of the complex network of 
mechanisms likely to be at play in priming, wherein the orientation of focus
is one mechanism that is particularly fast-responding.


\paragraph{Priming effects can be subtle even when strong.}
Just as the classifier continues to detect priming difference between 
$\textsc{cult}_{img}$ and \textsc{ambg} after the first test image,
it is able to differentiate both $\textsc{cult}_{fund}$ and 
$\textsc{ingr}_{fund}$ from \textsc{ambg}, even though we find no significant
differences in label orientation or specificity.  

Clearly, the effects of
priming, even when definitive, can be subtle. Adopting the classifier-based
definition of priming allows one to detect effecs even if they are subtle or
not of an expected nature.

\paragraph{Priming did not affect cultural specificity.}
It was quite surpising that the effects on workers' specificity was essentially
confined to food-oriented labels, and that we observed no significant movements
in cultural specificity.   This might simply reflect the different roles
played by food and culture in the human lexicon.  This seems plausible when we 
consider the vital role of food, and the known result that people are less
discerning of characteristics of ``outsiders''.

Even considering the above explanation, is surprising that 
$\textsc{cult}_{img,x}$ could be strongly influenced to produce more 
cultur-oriented labels, and yet not show increased 
specificity in their use of culture-oriented labels.  Since effects on 
label composition were only appreciable for the first test image 
(see \textbf{Fig. 4}), we also
performed the specificity analysis restricting to labels from the first
test image (see \textbf{Fig. S1}).  But even focusing on the first test image
shows no significant increase in cultural specificity for 
$\textsc{cult}_{img,x}$.  Thus, we observe that orientation of focus and 
label specificity do not necessarily.


\paragraph{Similarity drives nuance.}
We observed already that treatments primed with the 
culture or ingredients image sets have less specificity compared to 
\textsc{ambg}.  Actually, nearly all the difference in specificity
can be explained by how different a treatment's priming images are from the 
test images.  

Recall that the ambigouous set featured prepared meals, and 
was very similar to the test set.  The ingredients set at least featured 
food, although as separate ingredients instead of prepared meals, while the
culture image set was very different from the test images, containing no food
at all.
Indeed, not only is it true for comparisons to \textsc{ambg}, but in all the 
comparisons between treatments, specificity is higher for the 
treatments shown the ambiguous set than those shown ingredients set, 
and higher for those shown the ingredients set than those shown the culture 
set.

An appealing explanation for 
this is the notion of \textit{negative priming}, wherin the participant
becomes desensitized to stimuli that do not seem salient.  According to this 
argument, workers who have been shown successive images of prepared meals 
would be much less inclined to provide generic terms such as \texttt{food} or 
\texttt{meal} since these terms would no longer seem significant in the 
context of their priming treatment.  

This suggests that, in the serial image labelling task, the worker
does not seek labels that are appropriate descriptors for the image so much 
as labels that distinguish the images from others she has recently seen.
It would be interesting to investigate whether this manifests other 
categorization and labeling tasks.

\paragraph{Priming for better HPU performance.}
Thus, if one seeks very nuanced sorting or labeling from human workers, 
we recommend assembling the HPUs in a hierarchical fashion, with the 
HPUs at the top of the hierarchy performing coarse sorting (or labeling), 
splitting objects into streams to be handled by HPUs performing more nuanced 
work.  To implement this for an
image-labeling task, one could first serve the images for coarse labelling,
then cluster the images based on the labels and a language model, and then 
reserve the images, ensuring that each HPU operates on images from a single 
cluster.  The ``denser'' the image set in terms of coverage of certain
regions of ``content space'', the more one might gain through repeated 
clustering and labelling.

This exhibits an interesting difference between HPUs and CPUs.  In general,
whenever an aspect of a problem can be parallelized when employing CPUs, one
gains efficiency.  But in the case of HPUs, which are hysteretic, we see an 
example where one might lose efficiency through parallelization.

\subsection*{Conclusion}
We have demonstrated that in-task priming has a strong effect.  It is stronger
in its direct effects on orientation of focus and specificity than is 
disclosing a funder name that signals the intent of the study.  
Rather than being a nuisance, in-task priming might be exploited to improve
task output, for example, to drive more (or less) nuanced responses.

Although the nature of the effects of framing were difficult to trace
in this experiment, we nevertheless demonstrated that those effects exist
and are strong, using our algorithmic definition of priming.  This exhibits
one of the main advantages of this definition: the analyst need 
not understand the details of \textit{how} priming influences a task to 
quantify its effect.

We advocate the view of priming as a form of hysteresis.  The notion of 
hysteresis is already used in varied disciplines like physics, engineering,
and economics.  This view invites the use of insights already gained from 
other fields, and application-independant models, to help understand human 
cognition.

\section*{Acknowledgements}
\section*{References}
\begingroup
\renewcommand{\chapter}[2]{}
\bibliography{newbib.bib}
\endgroup
\bibliographystyle{plain} 

\section*{Figure Legends}
\paragraph{Figure 1.}
$F_1$ score and $\theta_\text{NB}$ for the various 
priming treatments, as measured using a Naive Bayes classifier. Each panel 
presents classifier performance results that indicate the difference in
priming between a basis 
treatment, indicated in the inset, and the other treatments, indicated on the
abscissa.

\paragraph{Figure 2.}
$F_1$ score and $\theta_\text{NB}$ for the classification of 
$\textsc{cult}_{img}$ and $\textsc{ingr}_{img}$ using a Naive Bayes 
classifier.  The classifier is provided only the labels attributed to
particular images as indicated on the abscissa.

\paragraph{Figure 3.}
Percent label composition (culture- vs food-oriented labels) for 
various images and treatments.  Panel A shows the composition of 
labels, aggregated over all treatments, as a function of test image.
Panel B shows the composition of labels, aggregated over all images, as
a function of treatment.  Panel C shows the composition of labels 
attributed to the first test image by various treatments.

\paragraph{Figure 4.}
Excess culture-oriented label composition in the labels provided
by $\textsc{cult}_{img}$ as compared to $\textsc{ambg}$, for various
test images (see \textbf{Eq. 1} for calculation).

\paragraph{Figure 5.}
Relative specificity of various treatments.
Each panel shows the comparison between a basis treatment (inset) and 
target treatments (abscissa).
Bar heights indicate relative specificity of the target 
treatment compared to the basis (a positive quantity means the target 
is more specific), and are normalized to units of standard 
deviations of the null comparison.  Dotted horizontal lines indicate
the 95\% confidence interval for the null comparison, which necessarily
has zero relative specificity (see \textbf{Methods} 
for an explanation of the null comparison).  Panels D, E, and F 
represent relative specificity calculated using only culture-oriented
labels, while panels G, H, and I using only food-oriented labels.
Labels that are both culture- and food-oriented are excluded from the
calculations for panels D through I.

\paragraph{Figure S1}
Relative specificity of various treatments, for labels attributed
to the first test image.
Each panel shows the comparison between a basis treatment (inset) and 
target treatments (abscissa).
Bar heights indicate relative specificity of the target 
treatment compared to the basis (a positive quantity means the target 
is more specific), and are normalized to units of standard 
deviations of the null comparison.  Dotted horizontal lines indicate
the 95\% confidence interval for the null comparison, which necessarily
has zero relative specificity (see \textbf{Methods} 
for an explanation of the null comparison).  Panels D, E, and F 
represent relative specificity calculated using only culture-oriented
labels, while panels G, H, and I using only food-oriented labels.
Labels that are both culture- and food-oriented are excluded from the
calculations for panels D through I.

\end{document}


