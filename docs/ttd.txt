Manuscript Notes
================
- We investigate the effects taht the content of earlier tasks have on workers
	--> On microtask platforms, workers usually perform batches of tasks. We
		investigate the effects that the tasks a worker has previously
		preformed have on subsequent ones.

- Define the words 'requester' and 'worker'
- Semantically loaded name
	--> First called it "name for the task-requester", then "funder name"
		

- Next, workers were shown ten images -- insert para break before
	--> one at a time (.)  Workers were required to provide 5 descriptive
		labels before moving on to the next image.

- For the final image set --> (,)

- We chose three different initial image sets, which we call "ambiguous", 
	"cultural", and "ingredients".  The <em>ambiguous</em>...

- Keep the algorithmic priming test, but talk about testing significance of
	difference in distributions and how classifiers are an easier way to do
	this, and that the result is predictive too, and that besides, we're 
	interested in the effect on output of an algorithm, hence using classifier.

- We can reject the null hypothesis if thetaNB is *at least* theta star
- Results re cross-comparison of classifier competance:
	- against the ambiguous, we can only distinguish the treatments using
		cultural imaging.  When the cultural funder is added, it doesn't seem
		to help.

	# in-task priming is strong
	# funder framing is not sufficient to enable distinction, but changing
		funder framing does affect the ability to distinguish treatments that
		differ in in-task priming.
	- we can look at these comparisons to guage how effective image-based and
		funder based effects are.
		- the cultural image treatments can always be distinguished from other
			treatments, except, of course, when cultural img is compared to 
			cult fund image.
		- none of the funding-only can be distinguished from ambg
			- nor can the funding-img be distinguished from the img-only
			- nor can the cultural-funding be distinguished from the 
				ingredients-funding
			- cult-fund can't be distinguished from ingr-fund
			- when both the image set and the funder treatment differ,
				the funder aspect does have an effect, sometimes decreasing,
				and sometimes increasing the distinguishability


	- the cultural images are easily distinguished from the ambiguous images, 
		are distinguished (though less easily) from ingredients; while 
		ingredients images are not distinguished from ambiguous.

Do Now
======

- Data Stuff
	x implement k-fold cross-validation
	x plot the new \theta_NB data and check for singificance
		x include all comparisons (e.g. comparing one funding treatment to the 
			other
	x make new longitudinal plot of \theta_NB and check for significance
		x compare cult_img and ambg
	x Check where the 95% confidence level is for binary classification with
		test set of 25

	x It would be best to plot averages for the cross-val data, because
		different runs produce different levels: some look much better
		than others, but it's not right to cherry pick!  An average would
		be most representational

	x assess significance tests for the orientation composition measures
		o I would feel better about using the stdev of 'food_nx' (food non 
			exclusive)... maybe...
	x re-plot the longitudinal excess cutural orientation
	[] crossComparison got rid of numReplicates -- propogate
	x review the calculation of significance for the specificity comparisons
		x I think I have the variance calculated correctly now
		x have a look at the plots of specificity comparisons
			[] one thing that has me concerned is that the variance didn't seem
				to be lower in the plots, although the variance printouts
				at runtime are lower...
		[] perform all comparisons
		x the mask approach is not working.  Desired behavior: e.g. for 
			cultural specificity comparison, use any tokens that have a 
			cultural ancestor and no food ancestor
			x implemented a 'mask' and a 'drop' feature
	[] re-plot the specificity comparisons
		[] add all comparisons

	[] make a all-pairs classifier-theta plot for when treatments are taken
		all together.  This will have lower distinguishability throughout.
		Include it as supplementary material.
	[] push together the longitudinal plots into one figure (for binary 
		classification performance and excess cultural orientation).  Add a
		separate subplot for the excess cultural orientation, calculated for
		alltreatments.
		

[] Ontology
	- the ontology needs to be cleaned up
	- I noticed that there are some orphans

- check back for my math stack exchange answer
	=> added a bounty
	=> posted on cross-validated

- statistical significance for theta_NB
	- re-calculate for testing set of 40.  Wasn't enough
	- re-calculate for testing set of 50.

- hypothesis testing for orientation and excess culture

- hypothesis testing for specificity

- methods

- plot 8 should compare cult_img and ingr_img instead of cult_img and ambg

- read *the future of crowd work*

Other stuff
===========
- Recalculate specificity
	- use 134 as the U sample size, and 67 as the V sample size
	- make sure to print running times, this will be long

- Change the specificity plots
	- plot on a meaningful scale # words more specific - # words less
	- plot the standard error (sqare root of eq8), not the 95% condfidence 
		interval
	- indicate significant measurements with stars -- calculate Stdev using
		eq 8!
	- discuss my hypothesis test with Derek

- am I using a consistent training and test size?

- the excess cultural orientation should be calculated with a fixed subsample
	from each treatment, because the treatments aren't the exact same size.


Might include
============
* never compared total vocab...
* compare total vocabulary between different treatments, and between a 
	population comprised of 50 users from cultural and 50 users from 
	ingredients
