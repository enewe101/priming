\documentclass{sigchi}

% Use this command to override the default ACM copyright statement
% (e.g. for preprints).  Consult the conference website for the
% camera-ready copyright statement.


%% EXAMPLE BEGIN -- HOW TO OVERRIDE THE DEFAULT COPYRIGHT STRIP -- (July 22, 2013 - Paul Baumann)
% \toappear{Permission to make digital or hard copies of all or part of this work for personal or classroom use is      granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \\
% {\emph{CHI'14}}, April 26--May 1, 2014, Toronto, Canada. \\
% Copyright \copyright~2014 ACM ISBN/14/04...\$15.00. \\
% DOI string from ACM form confirmation}
%% EXAMPLE END -- HOW TO OVERRIDE THE DEFAULT COPYRIGHT STRIP -- (July 22, 2013 - Paul Baumann)


% Arabic page numbers for submission.  Remove this line to eliminate
% page numbers for the camera ready copy 

%\pagenumbering{arabic}

% Load basic packages
\usepackage{balance}  % to better equalize the last page
\usepackage{graphics} % for EPS, load graphicx instead 
%\usepackage[T1]{fontenc}
\usepackage{txfonts}
\usepackage{times}    % comment if you want LaTeX's default font
\usepackage[pdftex]{hyperref}
% \usepackage{url}      % llt: nicely formatted URLs
\usepackage{color}
\usepackage{textcomp}
\usepackage{booktabs}
\usepackage{ccicons}
\usepackage{todonotes}

% My packages
\usepackage{dcolumn}
\usepackage{multirow}
\newcolumntype{d}{D{.}{.}{4.0}}
\newcolumntype{s}{D{.}{.}{1.4}}
\usepackage{arydshln}
\usepackage{enumitem}

% llt: Define a global style for URLs, rather that the default one
\makeatletter
\def\url@leostyle{%
  \@ifundefined{selectfont}{\def\UrlFont{\sf}}{\def\UrlFont{\small\bf\ttfamily}}}
\makeatother
\urlstyle{leo}

% To make various LaTeX processors do the right thing with page size.
\def\pprw{8.5in}
\def\pprh{11in}
\special{papersize=\pprw,\pprh}
\setlength{\paperwidth}{\pprw}
\setlength{\paperheight}{\pprh}
\setlength{\pdfpagewidth}{\pprw}
\setlength{\pdfpageheight}{\pprh}

% Make sure hyperref comes last of your loaded packages, to give it a
% fighting chance of not being over-written, since its job is to
% redefine many LaTeX commands.
\definecolor{linkColor}{RGB}{6,125,233}
\hypersetup{%
  pdftitle={SIGCHI Conference Proceedings Format},
  pdfauthor={LaTeX},
  pdfkeywords={SIGCHI, proceedings, archival format},
  bookmarksnumbered,
  pdfstartview={FitH},
  colorlinks,
  citecolor=black,
  filecolor=black,
  linkcolor=black,
  urlcolor=linkColor,
  breaklinks=true,
}

% create a shortcut to typeset table headings
% \newcommand\tabhead[1]{\small\textbf{#1}}

% End of preamble. Here it comes the document.
\begin{document}

\title{How One Microtask Affects Another}

\numberofauthors{3}
\author{%
  \alignauthor{Edward Newell\\
    \affaddr{Network Dynamics Group\\
	School of Computer Science\\ McGill University}\\
    \affaddr{Montreal, Qu\'ebec}\\
    \email{e-mail address}}\\
  \alignauthor{2nd Author Name\\
    \affaddr{Affiliation}\\
    \affaddr{City, Country}\\
    \email{e-mail address}}\\
  \alignauthor{3rd Author Name\\
    \affaddr{Affiliation}\\
    \affaddr{City, Country}\\
    \email{e-mail address}}\\
}

\maketitle

\begin{abstract}
Microtask platforms are becoming commonplace tools for performing human
research, producing gold-standard data, and annotating large datasets.
These platforms connect \textit{requesters}
(researchers or companies) with large populations (crowds) of workers, who 
perform small tasks, typically taking less than five minutes each.
A topic of ongoing research concerns the design of tasks that elicit high
quality annotation.
Here we identify a feature of nearly all crowdsourcing 
workflows that profoundly impacts workers' responses.
Microtask assignments typically consist of a sequence 
of tasks sharing a common format (e.g., circle galaxies in an image). 
Using image-labeling, a canonical microtask format, we 
discover that earlier tasks have a priming effect on the worker, shifting 
the distribution of future responses by 30-50\% 
(total variational distance). 
Specifically, prior tasks influence the content that workers focus on, 
as well as the richness and specialization of responses. 
We call this phenomenon \textit{intertask effects}.
We compare intertask effects to the overt framing of the assignment, 
effected by stating the requester's research interest, 
and find that intertask effects are on par or stronger.
While intertask effects can be a source of systematic bias, 
our results suggest that, with appropriate task design, 
they might be leveraged to hone worker focus and acuity, 
helping to elicit reproducible, expert-level judgments.
Intertask effects are a crucial aspect of human computation that should be
considered in the design of any crowdsourced study.
\end{abstract}

\keywords{Authors' choice; of terms; separated; by semi\-colons;
  commas, within terms only; this section is required.}

\category{H.5.m.}{Information Interfaces and Presentation
  (e.g. HCI)}{Miscellaneous} \category{See
  \url{http://acm.org/about/class/1998/} for the full list of ACM
  classifiers. This section is required.}{}{}

\section{Introduction}
There are many tasks that are trivial for people, but difficult to solve
programatically.  
Typical tasks include tagging and categorizing images,
%\cite{6116320,Zhai2012357}, 
coding and transcribing media,
%\cite{chandler2013breaking,Berinsky2012351,Finnerty2013},%paolacci2010running},
%judging the relevancy or quality of content,
and performing surveys for 
academic or market reserach purposes (see Table~\ref{table:task_composition} for a listing of task types seen on the Amazon's Mechanical Turk 
microtask platform).
%\cite{le2010ensuring,grady2010crowdsourcing,alonso2009can,kazai2013analysis}.
Many tasks are ill-defined, in the sense that they do not have
a clear ``correct'' response, and require high-level, qualitative judgment.
Microtask platforms are marketplaces that help fill the gap in current 
computational capabilities, by matching requesters, who need to have such 
tasks completed, with human workers.  Amazon's Mechanical Turk, and 
CrowdFlower are two popular microtask platforms.  

This form of crowdsourcing embeds workers in a controlled but flexible
task infrastructure.  
To the requester, workers seem almost like input-output devices.  
This provides much of the flexibility and 
cost-savings of fully automating the work in a computer program: 
the workforce is available on demand over the Internet using automated 
scripts, without the need for interviews or contracts 
\cite{wolfson2011look,5543192}.
Work can be performed at a fraction of the cost of traditional methods for 
recruiting temporary workers or experimental subjects
\cite{Berinsky2012351}. %,ranade2012crowdsource}%,paolacci2010running}.
Many researchers consider microtask platforms 
as a new kind of \textit{human computing} architecture
\cite{5543192}. %,little2010turkit,minder2012crowdlang,kittur2011crowdforge}.

The flexibility and cost-effectiveness of microtask labour
has led to a surge in demand from industry and 
academia \cite{wolfson2011look,Berinsky2012351}.  More recently microtask 
platforms have
been assessed as a way to supplement expert human resources to increase
capacity in critical applications, such as in medical diagnostic functions,
with promising results \cite{Warby2014385}.

Naturally, researchers have investigated the factors affecting the 
reliability of microtask work, including the design of the task interface
\cite{Finnerty2013},
the design of workflows (how work is divided into tasks)
\cite{kittur2011crowdforge,Huang201077,laseckieffects},
and the framing of tasks
\cite{Kinnaird2012281,chandler2013breaking,thibodeau2013natural}.
Here we draw attention to a ubiquitous yet overlooked feature of microtask 
work: the tendency for workers to perform many similar tasks in quick 
succession.  

This tendency arises from a combination of worker preferences and the 
logistics of serving small tasks.  
Workers have a preference for performing sequences of similar microtasks
\cite{Chilton20101}, probably because it reduces 
cognitive load arising from task-switching \cite{Adamczyk2004271}.
Morever, as workers complete task assignments, they must continually 
switch between working on an assignment and choosing their next 
assignment (weighing such factors as the wage paid, and effort required).  
Since microtasks are very short,
it makes sense to bundle tasks together into
larger assignments, to reduce the overhead of switching between  
working on assignments and choosing them.
This may explain why the pre-defined assignment templates available on 
microtask platforms generally bundle many tasks together by 
default\footnote{e.g. on \url{mturk.com} and \url{crowdflower.com}}.

Psychological experiments show that the exposure to stimuli immediately 
before performing a task influences performance, an effect known as 
\textit{priming} \cite{BJOP1796}.
The effect of priming appears to be stronger when the modality of the
prime is the same as the task.
Thus, the typical microtask setup creates the conditions in which
earlier tasks could induce bias in later ones, through priming.

We seek to determine if any significant priming effect occurs between
tasks, and if so, to measure and characterize it.
Certain tasks admit a well-defined notion of ``correct'' and 
``incorrect'' responses.  But many tasks involving qualitative judgment
do not.  To provide a generalized measure of bias, we seek to quantify the
extent to which intertask priming can shift the distribution of responses 
to a task.  
Measuring the \textit{extent} of the shift in response distribution,
i.e. the extent of bias, is substantially harder than
simply determining \textit{whether} a biasing effect has occured.  
Our first contribution is a statistically grounded method for doing so.

To investigate the phenomenon in a
highly generalizable setting, we sought a task that would involve 
qualitative judgment and enable relatively unconstrained responses. 
We also sought a task that would be a typical exemplar of the kinds of
tasks actually seen on microtask platforms (see 
Table~\ref{table:task_composition} for examples).  For this purpose, we 
adopted
image-labeling as a canonical micratask, in which workers provide 
descriptive labels to images using free-text inputs.  Image labeling is
a qualitative task without clearly ``correct'' or ``incorrect'' responses,
and is among the most common kinds of tasks on the major crowdsourcing
platform Amazon's Mechanical Turk.  If task-induced priming arises in our 
setup, it should be expected in virtually every microtask and 
crowdsourced workflow where workers are called upon to provide qualitative
judgment.

In our experiments, workers label a series of images, one at a time.
Depending on the treatment to which a worker is assigned, we vary the
images in the first five tasks, while keeping those in the last five tasks 
the same.  For example, in one experiment, the first five images shown to
one group of workers contained food, while those shown to the other 
contained (non-food) objects.  The last five images for both
groups contained both food and objects.

Our results show that a worker's responses are strongly influenced by 
the content of tasks performed beforehand, leading to as much as 50\%
bias (measured as total variational distance, 
see Figure~\ref{fig:l1_example} for illustrative definition).  
Using the wordnet 
knowledge base, we analyze worker's word choices to characterize the 
the nature of these effects in detail.  We find that, when workers 
label a series of images that are more similar, their responses become 
more specialized more diverse.  Prior tasks can shift the topical focus of 
worker's labels, inducing them to focus on different aspects of the images.

As a point of comparison, we perform similar experiments, in which we
alter the framing of microtask assignments, in terms of the stated purpose
of the task.  Remarkably, the effects prior microtasks themselves,
which are virtually ubiquitous, are on par with, or stronger than, 
overtly framing the purpose of an assignment.

We call the effects that earlier tasks exert on later ones 
\textit{intertask effects}.  If, as has been suggested \cite{5543192}, 
microtask platforms are to be considered as a new form of computing 
architecture, it will be necessary to reconcile the fact that the human 
computing elements exhibit \textit{hysteresis}, 
meaning that microtask workers' outputs 
depend on the \textit{history} of their inputs.
We propose a dual priming mechanism which can exaplain our
observations.  Irrespective of the underlying mechanism, this 
very strong effect might be exploited in task design to tune worker 
focus and acuity.

As our main contributions, we
\begin{itemize}[noitemsep,nolistsep]
  \item{derive a general method for measuring bias in responses;}
  \item{measure bias in microtasks due to prior tasks;}
  \item{show this effect is stronger than, or on par with framing;}
  \item{
	show that when completing a series of similar microtasks,
	worker's responses become more specialized and diverse;
  }
  \item{provide design considerations in light of our observations.}
\end{itemize}

The rest of the paper is organized as follows.  In the next section, we 
review the relevant prior work.  We then present a method for 
measuring bias due to intertask effects.  
Next we present our experimental results, followed by a discussion 
of interpretations and ramefications for microtask-based and crowdsourced 
work.  We conclude with suggestions for task design and future work.

\section{Prior work}
\subsection{Effects of microtask design on response quality}
% Accuracy, reproducibility, and a controlled task environment are 
% important
Microtask platforms are increasingly used clean and codify datasets,
administer experiments with human participants, and distribute 
surveys.  Accuracy and replicability are crucial in all of these 
applications.

\begin{table}
\centering
\begin{tabular}{l c c}
\toprule
Task Type & Count & Fraction (\%) \\
\toprule
Image transcription & 57 & 28.5 \\
Information gathering & 46 & 23.0 \\
Image tagging and classification & 36 & 18.0 \\
Copy writing and editing & 18 & 9.0 \\
Text tagging and classification & 15 & 7.5 \\
Audio/Video transcription & 12 & 6.0 \\
Survey & 9 & 4.5 \\
Unknown & 7 & 3.5 \\
\bottomrule
\end{tabular}
\caption{
	Frequency of various broad task types seen among the 200 most 
	recently posted tasks on MTurk, accessed on 12 February, 2015.
}
\label{table:task_composition}
\end{table}

% These things are definitely not assured, and people have tried to 
% figure out what to do about it
Accuracy and replicability are central to any crowdsourcing study.
There is ongoing research investigating how the desing of microtasks,
and their context, affect the quality of responses.  
One of the most basic issues with crowd work
is that workers vary in the amount of attention they pay to tasks, 
and in their understanding of instructions.
Workers who disregard or misunderstand instructions can be effectively 
screened out by including quiz-questions among the tasks, while
instruction comprehension can be improved using a training phase before
actual tasks begin
\cite{le2010ensuring,kazai2013analysis}. %paolacci2010running},
In addition, providing real-time feedback to workers encourages  
higher-quality responses, while prompting workers to review their work
according to a set of criteria increases the worker's response quality 
over time \cite{Dow20121013}.

% Designing tasks themselves
The design of the task interface also has important effects on response
quality.  A simpler user interface can improve accuracy 
\cite{Finnerty2013}, and certain input controls
are inherently less effortful and error-prone than others
\cite{cheng2015measuring}.

% Task pricing
Since microtask work is generally paid, wage is a basic parameter in
the requesters control that might influence response quality.
One might expect a higher wage to \textit{buy} better performance,
but results are ambiguous.  One study found that increasing wage
has no effect on response quality, but does increase the amount of 
responses \cite{Mason200977}.
Another study \cite{kazai2013analysis} showed that increasing
wages does increse quality, but that this effect saturates, and 
eventually increases in wage attract a larger porportion of workers who 
disregard the task instructions.

% Task framing
Workers have other motivations for completeing tasks aside from 
remuneration \cite{kazai2013analysis}.  
Researchers have attempted to appeal to
worker's motivation to do work that has a meaningful purpose:
when a task was framed as assisting with medical research,
workers are more likely to participate and completed more tasks, than
when they were not provided any context \cite{chandler2013breaking}.
Providing a meaninful context did not, however, increase the 
\textit{quality} of work.

Other studies have investigating different means of framing.  
When asked to rate various policy interventions, 
workers emphasized respectively more or less punitive approaches
depending on whether the phrase ``crime is a beast'' or 
``crime is a virus'' appeared in the problem description
\cite{thibodeau2013natural}.
When workers perform a task within the context of larger workflow, 
explaining how the task fits into the workflow, 
which amounts to a kind a framing, increased both the quantity and 
quality of responses \cite{Kinnaird2012281}.

% Optimization based on the breakdown of tasks
Breaking down of complex jobs into simpler tasks, can increasing worker
efficiency by enabling greater parallelism.
While this could disrupt the natural context provided by performing the
whole job as a single task, it turns out that, 
for complex tasks such as writing an article, 
dividing the job into subtasks, including outlining, 
information-gathering, and paragraph-synthesis, actually improves the
quality of the end-product \cite{kittur2011crowdforge}.

% effects of task interruption
Perhaps the most similar study to the work we describe here 
investigated the effects of interruptions on task performance
\cite{laseckieffects}.  There, workers 
completed a series of tasks that required them to answer questions about
an illustrated map.  When interruptions were introduced
in the form of either a time delay, or a different task involving a
different map, workers took longer to complete the task which 
followed the interruption.  However this study did not test for
effects on the quality of responses.
%\cite{kreifeldt1981interruption}

Taken together, the prior work suggests that, beyond the design
of tasks themselves, the design of the context of tasks has a major
effect on responses.
However, the potential of earlier tasks to act as primes for later ones 
has not been adequately investigated.

%\subsection{Performance during repetitive tasks}
%Studies of worker performance during repetitive work have been studied 
%in the context of long-running monitoring tasks, such as those required
%of some facility operators.
%Studies show that performance tends to decline, in terms of
%lengthening reaction time and increased rates of error, as a function of 
%time-on-task \cite{pattyn2008psychophysiological}.  In such 
%contexts, interruptions may actually serve to restore mental attention.
%Studies have shown that improved recall during tasks that were 
%interrupted.  
%
%These studies illustrate the impact of task-duration and
%repetition on performance.  However, the
%relatively low-input monitoring tasks involved in such studies are very
%different from typical microtasks, which require continuous cognitive 
%effort and input from the worker.
%
%In the microtask setting, one study investigated the effect of delays
%and interruptions introduced into a microtask setting.  This study
%found that interruptions and delays
%leading to a significant increase in the time needed to complete the
%interrupted task once resumed.
%However, the researchers did not investigate
%whether interruptions could affect the content of responses.
%
%Here we focus on how the content of earlier tasks can influence responses
%to tasks later ones.  Because qualitative judgment tasks are difficult
%to automate, but often easy for people to perform, many microtasks are of 
%this kind.  We hypothesize that such tasks are highly susceptible to 
%priming effects, and that in the repetitive pattern of microtask work
%(and most crowdsourced work), earlier tasks may act as primes to later 
%ones, and thereby introduce singificant amounts of bias.

\subsection{Priming}
Priming is a psychological phenomenon whereby previous exposure to a 
stimulus leads to faster or more accurate responses to similar stimuli 
\cite{Ghuman17062008},
or to lower response thresholds \cite{BJOP1826}.  
Priming occurs by means of pre-activation, which can influence
stimulus encodings (during perception) \cite{BJOP1826}, 
the top-down application of object-knowledge \cite{Ghuman17062008},
and memory access \cite{beller1971priming}.
Priming is more 
effective when the prime and the target task involve the kind of 
activity.  For example, exposing participants to an image helps them 
subsequently recognize that image when shown very breifly in a 
tachistoscope; however, being presented with a related \textit{word}, 
and reading it aloud, does not \cite{BJOP1796}.

As we have mentioned, workers prefer to perform series of similar tasks
series of similar tasks in quick succession.  Moreover, the many 
microtasks involve tasks based on visual and auditory perception 
similar to the kinds used in priming studies.
Thus, in a typical microtask assignment, there are many opportunities for 
priming, the effects of which should be explored.

\subsection{Measuring the extent of priming}
If a prior task acts as a prime for a later one, it will 
result in a shift in the responses workers provide for the later task.
A variety of tests exist to determine \textit{whether} two samples, 
i.e. sets of responses, come from different distributions, 
most notably the $\chi^2$ test.  
This will suffice to determine whether there are any grounds for conern 
about priming effects in the first place.  However, such tests
do not tell us \textit{how much} two distributions differ, and hence
how severe the priming effects are.

Determining the severity of priming effects amounts to measuring a 
distance, or \textit{divergence}, between response distributions, by
sampling responses from workers exposed to different priming conditions.
We will present a method for bounding a kind of divergence known as 
\textit{total variational distance}\footnote{
  Other formulation such as the Kulback-Leibler and 
  Jenson-Shannon divergence can be calculated from the total variational
  distance
}, which we will henceforth denote by $\theta$.
The value of $\theta$ between two distributions $P$ and $Q$ 
is the fraction under the graph of $P$ that does not overlap with that
under $Q$ (or vice-versa; see Figure~\ref{fig:l1_example}).  
Formally:
\begin{align}
	\theta = \frac{1}{2}\sum_{x \in X} \left| p(x) - q(x) \right|,
	\label{eq:theta}
\end{align}
Where $p(x)$ and $q(x)$ are the respective probabilities 
that a worker submits response $x$, from a set of possible responses 
$\mathcal{X}$, under distributions $P$ and $Q$.
The divergence is constrained to 
$0 \leq \theta \leq 1$ (or from 0 to 100\%).
When $\theta = 100\%$, the distributions do not overlap at all.  
When $\theta = 0\%$, the distributions are identical.
For reference, the distributions plotted in 
Figure~\ref{fig:l1_example}A and B have $\theta=30\%$ and $\theta=50\%$ 
respectively.

\begin{figure}
	\centering
	\includegraphics[scale=1.0]{figs/normal_example.pdf}
	\caption{
	  	Normal distributions exhibiting a total variational distance
		of A) 50\% and B) 30\%.  The total variational distance is 
		the non-overlapping portion of the distributions, e.g. in
		A) it is given by 
		$\theta = a_1 / (a_1 + a_2) = a_2 / (a_2 + a_3)$.
	}
	\label{fig:l1_example}
\end{figure}

Empirically measuring the divergence between distributions
is a matter of ongoing
research \cite{val-thesis,batu2013testing,chan2014optimal}.
In the ``na\"ive'' approach one first uses the samples to reconstruct 
empirical
distributions, by calculating their maximum likelihood parameters.
Then, one substitutes the empirical values of $p(x)$ and $q(x)$ into 
Equation~\ref{eq:theta} \cite{batu2013testing}.  
For example, one might directly estimate $\hat{p}(x) = N_x/N$, 
where $N_x$ is the number of times response $x$ is observed among $N$ 
total responses.

However this approach can \textit{drastically} overestimate $\theta$
\cite{val-thesis}.  As an illustrative example,
suppose that we have sampled two sets of 1000 words from \textit{possibly} 
different distributions, and that we wish to estimate the divergence 
between these distributions.  
It turns out that, if both sets of words were actually drawn from 
an identical Zipf distribution\footnote{The 
  Zipf distribution is a common model for word frequencies 
  \cite{powers1998applications,zipf1949human}:
  \begin{align}
	p(x) = \frac{x^{-1}}{\sum_{n=1}^{\infty}x^{-1}},
	\label{eq:zipf}
  \end{align}
  where $p(x)$ is the probability of the $x$th most common word.
}, the na\"ive approach would typically lead one to report 
$\hat{\theta} \approx 65\%$, event though, in reality $\theta = 0\%$
(this can be shown by simulated sampling).

However, we can establish a lower bound for $\theta$ by 
exploiting a fact about the theoretical limits on the accuracy of a 
classifier algorithm.  The intuition is as follows: 
suppose workers are shown one of two alternative designs\footnote{We will
use ``design'' in a general sense, to include both the design of the task
and of its context.  In particular, the ``design'' might include the
particular tasks that were shown beforehand, or the use of framing.}
for an otherwise
similar task, and that we build a classifier which, 
based on a worker's response, infers which of the designs had been 
used.
If worker's responses are not affected by the design alternative,
then the classification problem will be hard, and the classifier's 
accuracy poor.
Conversely, if the classifier accuracy is good, then design alternative 
must have a strong effect on the response distribution.

Stated formally, any classifier algorithm, $\mathcal{A}$, that
takes the response, $x$, of a worker, and guesses the 
design that elicited the response (from two possibilities:
$\mathrm{design}_1$ or $\mathrm{design}_2$), will do so with accuracy 
$\eta_\mathcal{A}$, that is bounded according to:
\begin{equation}
	\theta \geq 2\eta_\mathcal{A} - 1,
	\label{eq:sup:l1}
\end{equation}
where $\theta$ is the divergence between the distributions of responses
to $\mathrm{design}_1$ and $\mathrm{design}_2$.

We will now establish Inequality~\ref{eq:sup:l1} by considering an
optimal classifier having accuracy $\eta_*$.  
Let us assume that workers are shown 
$\mathrm{design}_1$ or $\mathrm{design}_2$ with equal probability.
If the worker gives the response $x$, it is optimal to guess
that the worker was shown the design most likely to elicit $x$.
In other words, if $p_i(x)$ is the probability that a worker shown 
$\mathrm{design}_i$ responds with $x$, then it is optimal to 
guess that the worker saw $\mathrm{design}_j$, where 
$j = \arg\max_j{p_j(x)}$.

Of course, neither $p_1(x)$ nor $p_2(x)$ are known.  But, on seeing $x$,
the probability that such a classifier would be correct is:
\begin{align}
  \mathrm{Pr}\{\mathrm{correct}|x\} = \frac{1}{2} 
	+ \frac{|p_0(x) - p_1(x)|}{2(p_0(x) + p_1(x))}
\end{align}
Summing over all possible responses that a worker could provide, 
$x \in \mathcal{X}$, weighted by the probability of observing $x$, 
we obtain the accuracy of the optimal classifier:
\begin{align}
\eta_* 
  &= \sum_{x\in\mathcal{X}} 
	\mathrm{Pr}\{\mathrm{correct}|x\}\mathrm{Pr}\{x\} \\
  &= \sum_{x\in\mathcal{X}} 
	\left(
	\frac{1}{2} + \frac{|p_0(x) - p_1(x)|}{2(p_0(x) + p_1(x))}
  \right) \left( 
	\frac{p_0(x) + p_1(x)}{2} 
  \right) \\
  &= \frac{1 + \theta}{2}
\end{align}
Since no classifier can be more accurate than an optimal classifier,
it follows that, for any \textit{practical classifier} 
with accuracy $\eta_\mathcal{A}$, the bound 
$\theta \geq 2\eta_\mathcal{A} -1$ holds.

Thus, we can establish a \textit{lower bound} on $\theta$ by first 
building a classifier that infers the design shown to workers from their 
responses, and then measuring its accuracy.

\section{Results}
\begin{figure}
	\includegraphics[scale=0.36]{figs/task-schematic-2.pdf}
	\caption{
	  Template for the experiments described in the main text.
	  After selecting our assignment on Mechanical 
	  Turk, workers were separated into two treatments.
	  Workers from each treatment were subjected to framing and/or
	  five initial tasks, which differed between treatments, and then 
	  completed a set of five test tasks that were identical for both
	  treatments.  Most experiments involved either framing or initial
	  tasks (see table \ref{table:experiments}), 
	  but \textit{frame-food-culture} involved both.
	  After the experiment, workers return to the Mechanical Turk 
	  task-selection interface.
	}
	\label{fig:task-schematic}
\end{figure}


\begin{figure*}
	\centering
	\includegraphics[scale=1.0]{figs/images.jpg}
	\caption{
		Examples of images used in
		initial tasks for the (\textbf{A}) \textit{food} and (\textbf{B}) 
		\textit{objects} treatments of \textit{intertask-food-objects};
		(\textbf{C}) test tasks for \textit{intertask-food-objects} and 
		\textit{frame-food-objects};
		initial tasks for the (\textbf{D}) \textit{food} and (\textbf{E}) 
		\textit{culture} treatments of \textit{intertask-food-culture};
		and (\textbf{F}) test tasks for \textit{intertask-food-culture} and 
		\textit{frame-food-culture}.
		The full set of experimental materials is shown in the 
		supplementary text.
	}

	\label{fig:task}
\end{figure*}


\begin{figure}
	\includegraphics[scale=0.75]{figs/theta.pdf}
	\caption{
		Empirical bias, $\theta_\mathrm{NB}$, measured using a na\"ive Bayes 
		classifier, induced in image-labeling tasks, due to workers' 
		exposure to framing and prior tasks.  
		(\textbf{A}) Exposing workers to initial tasks induced more severe
		bias than framing, except when echoed framing was used. 
		(\textbf{B}) As workers proceeded through test tasks, 
		the bias due to initial tasks waned, 
		but remained significant even after five tasks.  
		Standard error bars are shown.
	}
	\label{fig:theta}
\end{figure}


\begin{table}
\small
\centering
\setlength{\tabcolsep}{1pt}
\begin{tabular}{c c c c c}
\toprule
Experiment & \parbox[c]{3.0cm}{\centering{Priming modality}} & Initial Tasks & Frame \\
\midrule
\multirow{2}{*}{\textit{intertask-food-objects}} 
& \multirow{2}{*}{initial tasks} & \textit{food} & none \\
  & & \textit{objects} & none  \\

\noalign{\smallskip}
\hdashline
\noalign{\smallskip}

\multirow{2}{*}{\textit{frame-food-objects}} 
& \multirow{2}{*}{framing} & none 
	& ``food''\textsuperscript{a} \\
& & none 
	& ``objects''\textsuperscript{b} \\

\noalign{\smallskip}
\hdashline
\noalign{\smallskip}

\multirow{2}{*}{\textit{echo-food-objects}} 
& \multirow{2}{*}{echoed framing} & none
	& ``food''\textsuperscript{c} \\
& & none & ``objects''\textsuperscript{d} \\

\noalign{\smallskip}
\hdashline
\noalign{\smallskip}

\multirow{2}{*}{\textit{intertask-food-culture}} 
& \multirow{2}{*}{initial tasks} & \textit{food} 
	& none \\
& & \textit{culture} 
	& none \\

\noalign{\smallskip}
\hdashline
\noalign{\smallskip}

\multirow{2}{*}{\textit{frame-food-culture}} 
& \multirow{2}{*}{framing} & \textit{food} 
	& ``food''\textsuperscript{e} \\
  & & \textit{food}
	& ``culture''\textsuperscript{f} \\

\bottomrule
\end{tabular}
\caption{
	Description of experiments performed.  Each experiment had two treatments
	which differed either in the initial tasks shown or the framing used 
	(the priming modality).  
	During echoed framing, the worker had to respond to the framing language
	using a combo box input.  The framing language used was as follows:
	\newline\textsuperscript{a} ``Funded by the laboratory for the visual 
		perception of Food and Ingredients''
	\newline\textsuperscript{b} ``Funded by the laboratory for the visual 
		perception of Objects and Tools''
	\newline\textsuperscript{c} ``The purpose of this study is to understand the visual perception of Food and Ingredients''
	\newline\textsuperscript{d} ``The purpose of this study is to understand the visual perception of Objects and Tools''
	\newline\textsuperscript{e} ``This research is proudly funded by The National 
		Foundation for Nutritional Awareness''
	\newline\textsuperscript{f} ``This research is proudly funded by The Global 
		Foundation for the Recognition of Cultures''
}
\label{table:experiments}
\end{table}



We performed a series of five experiments
on Amazon's Mechanical Turk\footnote{\url{mturk.com}} using 1071 workers.  
Every experiment consisted of two treatments, each with 119 workers,
who performed a single assignment\footnote{Workers could not participate 
multiple times in our study}\textsuperscript{,}\footnote{On Mechanical Turk, individual 
assignments are called Human Intelligence Tasks (HITs), but this use of 
``task'' is 
different from our current one: in our experiment each HIT actually 
involves 10 image-labeling tasks.}.  
The setup in each experiment followed the template depicted in 
Figure~\ref{fig:task-schematic}.  
The assignments had two parts.  The first part of the assignment differed
between the two treatments, and consisted of a set of initial tasks 
and/or framing (by describing the purpose of the assignment or source of 
funding).  In the second part of the assignment, workers from both
treatments completed the same five \textit{test tasks}.  Each of the 
initial and test tasks required workers to provide five descriptive labels
for an image.  The tasks were performed one at a time,
and, in treatments involving both initial and test tasks, there was no
interruption or distinction made between the initial and test tasks from
the perspective of the worker.

We analyzed the
workers' responses to the test tasks to determine whether
they were influenced by the initial tasks (or framing).  We looked for 
differences in the distributions of test responses 
using a $\chi^2$ test.  We then measured the 
severity of bias induced by initial tasks and framing, by 
constructing a na\"ive Bayes classifier to infer the workers' 
treatments based on their responses to test tasks, 
and measuring its accuracy.

In our first experiment called \textit{intertask-food-objects},
workers were assigned to either a \textit{food} or \textit{objects} 
treatment.  Workers from these treatments performed initial tasks 
in which they labeled, respectively, 
images depicting either food or (non-food) objects
(see example images in Figure~\ref{fig:task}A and B).  
Workers from both treatments then performed five test tasks, which
contained images
depicting both food and objects (Figure~\ref{fig:task}C).  

The frequencies of words used by
workers in the \textit{food} and \textit{objects} treatments differed
significantly during test tasks (by $\chi^2$ test, $p = 6.0 \times 10^{-14}$),
even though the test tasks were identical in both treatments\footnote{
	see supplementary text for tabulated statistics.
}.  Thus earlier tasks do significantly affect later ones, which 
validates our concerns about the typical microtask methodology.

Having affirmed the \textit{existence} of intertask effects, we sought to
measure the severity of the bias induced, i.e. $\theta$.
To measure $\theta$ we used a na\"ive Bayes classifier based on a 
multinomial distribution over the word frequencies in worker's labels, 
and measured it's accuracy using leave-one-out cross-validation.  
We chose the na\"ive Bayes
classifier for three reasons\footnote{In the supplementary material we 
  compare alternate classifier configurations, and show that they produce
similar results.}.  First, it performs well even when the 
number of features is large compared to the number of training examples.  
Second, there are no hyperparameters to 
optimize, which eliminates the need to partition the response data into
dev and test sets.
Third, the conditional independance assumption, normally 
undertaken for pragmatic reasons, is probably a relatively mild in 
relation to image labels, since they are likely to be less dependent
on one another than in a coherent passage of text.

To produce word frequency features, worker's labels were split on white
space and punctuation, and the resulting tokens were spell-corrected based
on edit-distance to a dictionary of words, followed by stop-word removal
and lemmatization\footnote{A detailed description of preprocessing steps 
is available in the supplementary material.}.  The spelling correction
dictionary was compiled from a combination of WordNet 
\cite{felbaum1998wordnet} and words
collected by crawling the World Food section of \url{allrecipes.com}.

Based on the classifier accuracy, intertask effects in the 
experiment \textit{intertask-food-objects}
lead to (a lower bound) bias of $\theta=30\%$ between workers from the 
\textit{food} and \textit{objects} treatments (Figure~\ref{fig:theta}A).
For reference, the distributions shown in 
Figure~\ref{fig:l1_example}B have $\theta = 30\%$.  This represents a 
substantial potential to distort microtask data.

As already discussed, there has been considerable interest in the 
effects that framing can have on microtask responses
\cite{Kinnaird2012281,chandler2013breaking,thibodeau2013natural}, and 
framing effects are arguably similar to inter-task effects in the sense 
that both arise from the worker's experiences immediately before 
performing a task.
Therefore, as a point of comparison, we performed a similar experiment 
in which we framed the purpose of the tasks.  In the experiment 
\textit{frame-food-objects}, we used the same test tasks as in 
\textit{intertask-food-objects}, but the initial tasks were omitted, 
and in their stead, workers were either told that the study was 
``Funded by the laboratory for the 
visual perception of Food and Ingredients'', 
or ``\ldots of Objects and Tools''.  

Framing induced changes in the frequencies of 
word usage at significance 
(as determined by $\chi^2$ test; $p=0.0012$).  But, 
the \textit{extent} of framing-induced bias was not statistically 
distinguishable from zero ($p =0.37$) (Figure~\ref{fig:theta}A).
Remarkably, the extent of bias due to framing was weaker than that 
due to intertask effects ($p=2.1\times 10^{-5}$).
In other words, the \textit{microtasks themselves}
had a stronger biasing effect than overt framing.

To assess the persistence of intertask effects, we replicated the 
\textit{intertask-food-objects} experiments, rotating the ordering of the
test tasks, so that each test task occupied each of the possible 5 
positions.  By taking the average bias induced for each test task when 
occupying a given position, we determined $\theta$ as a function of the 
task position.
Bias was strongest for the first test task (about 28\%), 
but remained significant through all five test tasks 
($\alpha=0.05$) sustaining over 12\% bias even when the initial and test 
tasks were separated by four intervening tasks (Figure~\ref{fig:theta}B).

We conducted variants of these experiments using different images, to see 
whether this trend was robust.  In the experiment 
\textit{intertask-food-culture},
workers were either assigned to a \textit{food} or \textit{culture} treatment.
The initial tasks contained either images 
depicting food (Figure~\ref{fig:task}D), or depictions of cultural scenes 
(of dance, sport, or music) (Figure~\ref{fig:task}E).  The test tasks, which 
were, again, identical for both treatments, depicted meals of diverse 
cultural origin
%\footnote{
%  	The food depicted in the initial tasks 
%	in \textit{intertask-food-culture} are arguably still of
%	diverse cultural origin, but this would only tend to make our 
%	results more conservative.
%}
(Figure~\ref{fig:task}F).  

Results for this experiment again showed a 
strong bias as a result of intertask effects (about 50\%) 
(Figure~\ref{fig:theta}A).  Again, visualizing the word frequencies 
themselves is difficult, but Figure~\ref{fig:l1_example} provides an 
example of distributions having $\theta = 50\%$.

Once again, we compared the strength of intertask effects to those due to 
framing. In the experiment \textit{frame-food-culture} used
the same test tasks as in \textit{intertask-food-culture}, but before
workers begin the tasks, we indicated
that the purpose of the task was either the the recognition of food or 
culture, depending on the treatment to which the worker was assigned.  
However, unlike in the previous framing experiment, this time we also 
included initial tasks in the framing experiment (although they were 
identical for both treatments).  We did this so that in both experiments,
workers would have completed the same number of tasks before starting the
test tasks.  In \textit{frame-food-culture}, framing did not induce 
significant changes in word frequencies 
(by $\chi^2$ test, $p=0.29$) (Figure~\ref{fig:theta}A).

It was only when framing was combined with an active reiteration step, 
that bias reached a comparable extent to that induced by intertask 
effects.  In the experiment \textit{echo-food-objects},
after framing the purpose of the task (as the recognition of food
or objects), workers were asked to echo the purpose of the task
using a combo-box input.  This  ``echoed framing'' induced a bias of about 
35\% (Figure~\ref{fig:theta}A). However, it is difficult to say whether this 
should be considered as a framing treatment \textit{per se}:
requiring the worker to reiterate the purpose signals our intent, as the 
requester, to ensure that the worker has taken note of it, possibly leading 
the worker to interpret the exchange as an instruction.  
In any case, it is remarkable that intertask effects
were on par with an explicit, actively reinforced statement of the tasks' 
purpose.

\begin{figure}
	\centering
	\includegraphics[scale=0.87]{figs/vocab_specificity.pdf}
	\caption{
		Exposing workers to a priming concept, e.g. food, through
		initial tasks or framing, affects their tendency to focus
		on that concept, and the richness and specialization of their 
		vocaubulary in reference to it.
		In all three plots, positive values indicate a larger quantity for 
		the food-exposed workers.
		(\textbf{A}) Exposing workers to food does not necessarily increase
		the fraction food references they provide during test tasks.
		(\textbf{B}) The number of unique food-related
		words (richness) was greater for food-exposed workers, except in the 
		case of \textit{frame-food-culture} (stars indicate the threshold
		for a significant deviation, $\alpha=0.05$). 
		(\textbf{C}) Food-exposed
		workers used more specialized words to refer to food (see 
		supplementary text for calculation of relative specialization).
		Standard error bars are shown in (\textbf{A} and \textbf{C}).
	}
	\label{fig:specificity}
\end{figure}

\begin{table*}
	\centering
	\setlength{\tabcolsep}{4pt}
	\begin{tabular}{ c c c c c }
	
		\setlength{\tabcolsep}{4pt}
		\begin{tabular}{ r | c }
		\toprule
		\multicolumn{2}{c}{
			\parbox[c]{2.5cm}{
				\centering
					\textit{intertask-food-objects}
			}} \\
		\midrule
		coffee & 38 \\
		meal & 34 \\
		cheese & 34 \\
		apple & 32 \\
		dessert & 21 \\
		cup & -30 \\
		glass & -45 \\
		table & -70 \\
		candle & -74 \\
		food & -80 \\
		\bottomrule
		\end{tabular}

&

		\setlength{\tabcolsep}{4pt}
		\begin{tabular}{ r | c }
		\toprule
		\multicolumn{2}{c}{
			\parbox[c]{2.5cm}{
				\centering
				\textit{frame-food-objects}
			}
		}\\
		\midrule
		bread & 18 \\
		wine & 18 \\
		cheese & 16 \\
		apple & 14 \\
		oil & 12 \\
		table & -9 \\
		meal & -10 \\
		candle & -12 \\
		dinner & -13 \\
		food & -32 \\
		\bottomrule
		\end{tabular}

&

		\setlength{\tabcolsep}{4pt}
		\begin{tabular}{ r | c }
		\toprule
		\multicolumn{2}{c}{
			\parbox[c]{2.5cm}{
				\centering
			\textit{echo-food-objects}} 
		}\\
		\midrule
		apple & 24 \\
		cheese & 23 \\
		wine & 15 \\
		coffee & 14 \\
		oil & 7 \\
		knife & -24 \\
		dinner & -26 \\
		fork & -27 \\
		candle & -35 \\
		food & -55 \\
		\bottomrule
		\end{tabular}

&

		\setlength{\tabcolsep}{4pt}
		\begin{tabular}{ r | c }
		\toprule
		\multicolumn{2}{c}{
			\parbox[c]{2.5cm}{
				\centering
			\textit{intertask-food-culture}} 
		}\\
		\midrule
		spicy & 26 \\
		sauce & 17 \\
		indian & 15 \\
		buffet & 14 \\
		exotic & 12 \\
		festival & -11 \\
		offering & -12 \\
		statue & -15 \\
		india & -20 \\
		food & -56 \\
		\bottomrule
		\end{tabular}

&

		\setlength{\tabcolsep}{4pt}
		\begin{tabular}{ r | c }
		\toprule
		\multicolumn{2}{c}{
			\parbox[c]{2.5cm}{
				\centering
			\textit{frame-food-culture}} 
		}\\
		\noalign{\smallskip}
		\midrule
		indian & 11 \\
		banquet & 8 \\
		spicy & 7 \\
		asian & 6 \\
		variety & 6 \\
		delicious & -6 \\
		meat & -7 \\
		festival & -7 \\
		spice & -7 \\
		food & -9 \\
		\bottomrule
		\end{tabular}

	\end{tabular}
	\caption{
		The five words whose frequencies increased, and those whose 
		frequencies decreased, the most, between treatments of given 
		experiments, within labels attributed to the first test task.
		Values indicate the absolute change in number of occurences
		of the word, and positive values indicate that the food-exposed
		treatment 
		used the word with higher frequency.  
		Note that the word ``food'' is always the most suppressed among
		food-exposed workers.
		Word frequencies for 
		\textit{intertask-food-objects} correspond to the labels attributed
		to image 1 (see Figure \ref{fig:task1:test}) which is not necessarily 
		the first 
		test task due to the tasks being permuted.  There were five times
		as many workers in \textit{intertask-food-objects}, hence
		larger absolute differences are observed.
	}
	\label{table:top-words}
\end{table*}

To better understand the nature of 
intertask effects, we investigated the vocabulary
that workers used to label test tasks. One might expect that, within a given 
experiment, those workers exposed to food (whether through framing or initial
tasks) would label test tasks using 
food-related words more often.  However, this was not generally 
the case. In \textit{intertask-food-culture}, food-exposed
workers actually used significantly \textit{fewer} food-related words 
during test tasks\footnote{
	food-related words were identified using the WordNet knowledge base, 
	augmented with additional words obtained by crawling a recipe website 
	(see supplementary text for details).
} (Figure~\ref{fig:specificity}A).  This finding
rules out a seemingly-simple idea that workers emphasize
content that has been present in earlier tasks: seeing content 
influences, but does not necessarily \textit{increase}, the probability of 
referring to it in subsequent tasks.

To deepen our understanding, we investigated workers' lexical richness in 
reference to food, that is, the number of \textit{unique} food-related words
used.  Even if workers provide an abundance of food-related words, there
can be less diversity, if, for example, workers repeat generic references 
to food.
Both \textit{intertask} experiments showed that food-exposed workers had 
greater lexical richness, in reference to food, than their counterparts 
(as much as 20\% more) (Figure~\ref{fig:specificity}B).  
This is particularly noteworthy in the experiment
\textit{intertask-food-culture}, because there,
food-primed workers made fewer total references to food.  
%We also observed enrichment of the food-related lexicon in the 
%\textit{priming-food-objects}, and \textit{echo-food-objects} experiments, 
%although to a lesser extent.

The observations regarding lexical richness suggest that 
initial tasks might influence workers to use more refined or specialized 
words, when referring to aspects of content that had been present in the 
initial tasks.  
To test this, we used the WordNet knowledge base to operationalize the 
notion of word specialization.  Wordnet establishes relationships between
117,798 English nouns, indicating which words are specializations
(or generalizations) of others \cite{felbaum1998wordnet}. 
%miller1995wordnet,}.  
For example, ``pumpernickel'' is a specialization of ``bread''.
Within each experiment, we determined the percent-excess of food-related
words that were more specialized in the food-exposed treatment relative
to the other treatment (detailed calculation in the supplementary text). 

In all experiments except \textit{frame-food-culture}, food-primed workers 
used significantly more specialized words, in reference to food 
(about 15\% more) (Figure~\ref{fig:specificity}C).
It is interesting that such substantial increases in both the lexical 
richness and specialization of food-related words 
held for \textit{intertask-food-culture}, where, as mentioned, we observed 
that food-exposed workers made \textit{fewer} references to food overall. 
These observations point 
to countervailing factors: one factor tending to activate the more 
specialized and less common food-related words 
(yielding greater lexical richness and specialization), and the other tending 
to suppress certain, presumably more common and generic words 
(yielding fewer food-related words in total).

This hypothesis is corroborated when we look at those words whose 
frequencies changed the most from one treatment to another 
(Table~\ref{table:top-words}).  
The word ``food'', which is the most generic possible food-related word, was 
always \textit{suppressed} among food-primed workers.  In fact, 
for all experiments, ``food'' was the \textit{most suppressed} word.

\section{Discussion}
Our results can be 
explained through a combination of positive and negative priming.
Positive priming (usually simply ``priming'') occurs when a prior stimulus 
predisposes a person to give certain responses in an ensuing task, and
is often observed as an increase in the speed or accuracy of a response, or
the ability to recognize briefer or noisier stimuli 
\cite{BJOP1796,BJOP1826,Huber2008324}.
Negative priming occurs when, after exposure to a stimulus 
considered to be non-salient, subsequent recognition of the stimulus is 
inhibited \cite{mayr2007negative}.

Workers exposed to images containing food are (positively) primed, 
activating memories, concepts, and vocabulary related to food.  
However, if the worker labels several images containing food, the basic 
fact that an image contains food will not seem salient, since it 
does not distinguish one image from another.  Thus, the most 
generic references to that fact, such as the label ``food'', 
will be suppressed, while more specialized references will be elicited.  
Meanwhile, the number of references to food overall might increase or 
decrease, depending on the balance of these factors.  

More generally, we are suggesting that, even though workers are not 
instructed to compare tasks in any way, prior tasks form a 
context relative to which workers judge salience.  Thus, due to a combination 
of negative and positive priming, a worker's focus 
in repeated tasks tends to be directed away from generic, shared features, 
toward specific and distinguishing ones.

Prior to this study, little thought appears to have been given to
earlier tasks as a priming vector for later tasks.
But our findings show this is an important design consideration.  
One recourse might be to randomize task ordering, a practice that is 
commonly employed.  But the sheer extent of bias we observed suggests that 
this will still admit a significant amount of noise.  
Even chains of two or three similar tasks, which
will not be reliably eliminated in random permutations, could lead
to the levels of bias we observed in our experiments.
Preferably, a deeper understanding of intertask effects might allow them
to be properly controlled.

Our findings also suggest that careful task engineering might leverage
intertask effects to achieve greater quality and reproducibility in 
crowdsourcing.
A consistent goal in human computation is the achievement of expert-level
judgments from non-expert workers \cite{kittur2011crowdforge}.  
This has been achieved in some
applications \cite{snow2008cheap,Mortensen20131020,Warby2014385}. 
The distinction between experts and novices is partly attributable
to specialized knowledge and heuristics. 
But experts also simplify tasks by more efficiently directing 
their focus toward salient features \cite{kellman2009perceptual}.  
Using strategic task exposure, it might be possible to guide workers' focus 
and salience attribution, enabling expert-level judgment in a wider variety of
crowdsourced applications.
We anticipate future work will yield techniques to control intertask 
effects, to reduce unwanted bias, and to tune the focus, diversity, and 
specificity of worker responses.

We have shown that a severe bias can be introduced into microtask 
responses, simply from the performance of earlier tasks. 
These effects are much stronger than those of framing.  
While our findings raise serious concerns about the 
current state of microtask design, they reveal potential opportunities 
to refine worker focus and acuity.  Any application that relies
on the repetitive use of human judgment is likely subject to the phenomena
described here.  Exploring the pitfalls and opportunities posed by
intertask effects is an important direction for future work, 
in the effort to develop performant and reliable human computation.

\bibliographystyle{SIGCHI-Reference-Format}
\bibliography{newbib}

\pagebreak
\section*{Supplementary Material}

Microtask platforms are an increasingly popular way to solicit experimental
participants, as well as to annotate large datasets and obtain other 
judgments which are difficult to automate using only a computer.  
Due to the popularity of microtask platforms, there is interest in 
understanding how various task design factors
influence the reliabality of data obtained using microtask platforms.  
Since workers typically perform similar tasks in quick succession, there is
an opportunity for earlier tasks to effectively prime workers, and induce 
bias in the results obtained.
Here we describe in detail experiments presented in the main text, in which
we investigate the effect that prior tasks have on the responses given during
subsequent ones.  

\subsection*{Materials and Methods}

\begin{figure}
	\includegraphics[scale=0.8]{figs/tasks.jpg}
	\caption{Examples of (A) task instructions, (B) a framing slide
		(C) an echoed framing slide, and (D) an image-labeling 
		task.
	}
	\label{fig:hit_preamble}
\end{figure}
\paragraph{Microtask setup.}

The work presented in the main text consisted of six experiments involving
HITs (bundles of tasks) performed on Amazon's Mechanical Turk (MTurk)
platform.  
The flow of events experienced by a worker when participating in this study 
is depicted in Figure~S\ref{fig:task_schematic}.  
Each experiment was divided into two treatments.  Treatments for a given 
experiment differed with respect to a particular priming modality, which 
served as the independent variable
(as specified in Table~S\ref{table:experiments}).  
The HITs for treatments of the same experiment always involved the same
test tasks (the last five tasks in the HIT),
the results of which served as the dependent variable.
\begin{figure}
	\begin{center}
	\includegraphics[scale=0.8]{figs/task_schematic.pdf}
	\caption{Flow of events as experienced by a worker participating in
		our study.  Steps with a dash outlined are only performed by 
		workers assigned to certain treatments 
		(see Table~S\ref{table:experiments}).  Steps outlined in orange are
		part of MTurk's interface, and are common to any 
		microtask performed on that platform.  Steps outlined in black are
		specific to our HIT.
	}
	\label{fig:task_schematic}
	\end{center}
\end{figure}

All HITs were presented as a series of slides.  The first 
slide consisted of a brief set of instructions.  For HITs that included 
framing, a framing statement was shown on a slide immediately after the 
instructions (Figure S\ref{fig:hit_preamble}B and C).  
Initial tasks, when included, followed next.  The initial tasks consisted of 
a series of five slides, each containing an image-labeling task like the one 
shown in 
Figure~S\ref{fig:hit_preamble}D.  Finally, for all HITs, five slides were shown
containing test tasks, in the same format as the initial tasks.

Workers were randomly assigned to the treatments, and 
a given worker could only complete one HIT related to this study.  
In \textit{intertask-food-culture}, the \textit{food} and \textit{culture}
treatments were both divided into five sub-treatments, 
in which the test tasks were permuted.  
The permutations were obtained by rotating 
the order shown in Figure~S\ref{fig:task1:test} by different amounts. 
For example, rotating by one position involves moving the first image to the 
second position, the second image to the third position, and so on, and 
moving the last image to the first position.  
The five different possible rotations were used for the five 
sub-treatments.  This enabled us to study the effects on each image when
it occurred in different positions relative to the initial tasks.

\paragraph{Selection of images.} 

The images used in the initial tasks for \textit{intertask-food-objects}  
are shown in Figs.~S\ref{fig:task1:food} (\textit{food} treatment) and 
S\ref{fig:task1:obj} (\textit{objects} treatment).
We took care to exclude objects from
the food-images (except for the dish supporting the food in some cases), and 
to exclude food from the object-images.  The images used for the test tasks
of \textit{intertask-food-objects} and \textit{frame-food-objects} are shown 
in Figure~S\ref{fig:task1:test}.

The images used in the initial tasks
for \textit{intertask-food-culture} are shown in Figs.~S\ref{fig:task2:food} 
(\textit{food} treatment) and S\ref{fig:task2:cult} 
(\textit{culture} treatment).
We selected the initial images for the treatments of this experiment to 
respectively reflect depict food and culture.
Naturally, since food is a very
important aspect of culture, pictorial depictions of the two concepts 
cannot be cleanly separated,
and images depicting food inevitably also depict culture.
However, the fact that the images for the initial tasks in the 
\textit{food} treatment 
of \textit{intertask-food-culture} also depict culture
will only tend to make the \textit{food} and \textit{culture}
treatments more similar. 
This would tend to reduce the severity of intertask 
effects, but we nevertheless still observed strong intertask effects.
Thus, the presence of culture in the food-related initial images of 
\textit{intertask-food-culture} does not seem to have been problematic.
The images used in the test tasks of \textit{intertask-food-culture} and
\textit{frame-food-culture} are shown in Figure~S\ref{fig:task2:test}.

\begin{figure}
	\begin{center}
	\includegraphics{figs/task1-food.jpg}
	\end{center}
	\caption{
		Images used in the initial tasks for the
		\textit{food} treatment of \textit{intertask-food-objects}.  
		The numbers show the order in which the 
		images were presented to workers.
	}
	\label{fig:task1:food}
\end{figure}

\begin{figure}
	\begin{center}
	\includegraphics{figs/task1-obj.jpg}
	\end{center}
	\caption{
		Images used in the initial tasks for the
		\textit{objects} treatment of \textit{intertask-food-objects}.  
		The numbers show the order in which the images were presented to 
		workers.
	}
	\label{fig:task1:obj}
\end{figure}

\begin{figure}
	\begin{center}
	\includegraphics{figs/task1-test.jpg}
	\end{center}
	\caption{
		Images used in the test tasks for \textit{intertask-food-objects}
		and \textit{frame-food-objects}.  
		The numbers show the order in which the 
		images were presented to workers in \textit{frame-food-objects};
		in \textit{intertask-food-objects}, five different orderings were
		used, which can be obtained by taking the numbers shown, $n$,
		and replacing them by $n + c \bmod 5$ for $c$ running from 0 to 4.
	}
	\label{fig:task1:test}
\end{figure}

\begin{figure}
	\begin{center}
	\includegraphics{figs/task2-food.jpg}
	\end{center}
	\caption{
		Images used in the initial tasks for the
		\textit{food} treatment of \textit{intertask-food-culture}.  
		The numbers show the order in which the 
		images were presented to workers.
	}
	\label{fig:task2:food}
\end{figure}

\begin{figure}
	\begin{center}
	\includegraphics{figs/task2-cult.jpg}
	\end{center}
	\caption{
		Images used in the initial tasks for the
		\textit{culture} treatment of \textit{intertask-food-culture}.  
		The numbers show the order in which the 
		images were presented to workers.
	}
	\label{fig:task2:cult}
\end{figure}

\begin{figure}
	\begin{center}
	\includegraphics{figs/task2-test.jpg}
	\end{center}
	\caption{
		Images used in the test tasks for \textit{intertask-food-culture} 
		and \textit{frame-food-culture}.  
		The numbers show the order in which the 
		images were presented to workers.
	}
	\label{fig:task2:test}
\end{figure}

\begin{figure}
	\begin{center}
		\includegraphics{figs/task2-ingr.jpg}
	\end{center}
	\caption{
		Images used in initial tasks for \textit{frame-food-culture} 
		(both for the \textit{food} and \textit{culture} treatments).
		The numbers show the order in which the images were presented to 
		workers.
	}
	\label{fig:frame2:ingr}
\end{figure}

\paragraph{Framing.}
We framed tasks by including a message, near the beginning of the HIT, that 
suggested a particular purpose for the tasks.
The precise language used is shown in 
Table~S\ref{table:experiments}, and an example of a framing slide is shown in
Figure~S\ref{fig:hit_preamble}B and C.  In \textit{frame-food-objects}
and \textit{frame-food-culture}, the framing language was based on
naming the (fictitious) entity running the study, for example
``The Global Foundation for the Recognition of Cultures'',
or ``Laboratory for the visual perception of Objects and Tools''.
In \textit{echo-food-objects}, we stated the purpose explicitly 
(``The purpose of this task is \dots''), and then required the worker to
echo back the purpose of the task by selecting it from a combo box
Figure~S\ref{fig:hit_preamble}C.

In \textit{frame-food-objects} and \textit{echo-food-objects}, we did not
include any initial tasks, meaning that the test tasks followed the framing
slide immediately.  One could argue that a fairer comparison would be achieved
by including initial tasks even for framing experiments (but keeping the
initial tasks constant while varying the framing language), since the worker
would be ``warmed up'' before performing the test tasks, as is the case for
the experiments investigating the effect of initial tasks. 
We therefore adopted this approach in the experiment 
\textit{frame-food-culture}, in which workers performed the same 
initial tasks containing images that depicted food in both the \textit{food}
and \textit{culture} treatments.  It would seem that the framing effect in 
\textit{frame-food-culture} was reduced by the initial tasks, possibly because
intertask effects tend to override framing. Further experimentation 
would be necessary to be sure, but this was not the purpose of our study.

\subsection*{MTurk's task composition}

Image labeling tasks are one of the most often cited kinds of tasks in the
literature.  To determine whether they do indeed reflect an important fraction
of MTurk tasks, we accessed MTurk on 12 February, 2015, and coded the 200 
most recently posted tasks based on their title and description, 
into one of seven categories.

Tasks relating to image tagging and categorization represented 18\% of the 
total.  Of all tasks seen, those in which images are the primary input
account for 46.5\%.  Considering tasks regardless of input modality (e.g.
text, image, audio, or multimedia), those that involved some kind of tagging
or categorization accounted for 25.5\%.  
The detailed counts for our categorization
of task types are shown in Table~S\ref{table:task_composition}.  
While this does not reflect a full and
a systematic characterization of MTurk task composition, it does support our
choice of image-labeling as a representative microtask.

\subsection*{Testing for intertask and framing effects using $\chi^2$}

\begin{table}
\centering
\begin{tabular}{c c c c}
\toprule
Experiment & Degrees of freedom & $\chi^2$ & $p$-value\\
\toprule
\textit{intertask-food-objects} & 117 & 268.7 & $6.0 \times 10^{-14}$\\
\textit{frame-food-objects} & 116 & 167.8 & $1.2 \times 10^{-3}$\\
\textit{echo-food-objects} & 126 & 381.3 & $1.7 \times 10^{-27}$\\
\textit{intertask-food-culture} & 120 & 447.5 & $2.8 \times 10^{-39}$\\
\textit{frame-food-culture} & 119 & 127.2 & $0.29$\\
\bottomrule
\end{tabular}
\caption{
	Results for Pearson's $\chi^2$ test of homogeneity of the word-frequencies
	between treatments from each experiment.  This shows that the treatments
	significantly affected word usage, 
	except in the case of \textit{frame-food-culture}.
}
\label{table:chi2}
\end{table}

\begin{table}
\centering
\begin{tabular}{c c c c c}
\toprule
Experiment & Treatment & Degrees of freedom & $\chi^2$ & $p$-value\\
\toprule
\noalign{\smallskip}
\multirow{2}{*}{\textit{intertask-food-objects}} & \textit{food} & 65 & 39.0 & $1.0$\\
 & \textit{objects} & 65 & 53.2 & $0.85$\\

\noalign{\smallskip}
\hdashline
\noalign{\smallskip}

\multirow{2}{*}{\textit{frame-food-objects}} & \textit{food} & 67 & 41.3 & $0.99$\\
 & \textit{objects} & 62 & 66.9 & $0.31$\\

\noalign{\smallskip}
\hdashline
\noalign{\smallskip}

\multirow{2}{*}{\textit{echo-food-objects}} & \textit{food} & 66 & 40.9 & $0.99$\\
 & \textit{objects} & 66 & 35.6 & $1.0$\\

\noalign{\smallskip}
\hdashline
\noalign{\smallskip}

\multirow{2}{*}{\textit{intertask-food-culture}} & \textit{food} & 62 & 65.9 & $0.34$\\
 & \textit{culture} & 68 & 57.2 & $0.82$\\

\noalign{\smallskip}
\hdashline
\noalign{\smallskip}

\multirow{2}{*}{\textit{frame-food-culture}} & \textit{food} & 57 & 49.6 & $0.75$\\
 & \textit{culture} & 64 & 55.1 & $0.78$\\

\bottomrule
\end{tabular}
\caption{
	Results for Pearson's $\chi^2$ test of homogeneity of the word-frequencies
	within treatments, which were randomly partitioned for the test.  This
	shows that word-usage within treatments is homogeneous.
}
\label{table:chi2_within}
\end{table}

To test whether intertask and framing effects induced statistically 
significant changes in word frequencies, we assembled a contingency table
for each experiment, containing the frequencies of words in each treatment.  
Any words appearing less than ten times for
an experiment (i.e. one whose expected frequency would be less than five in a 
given cell of the contingency table)
were lumped together into a separate `OTHER' category, to ensure validity
of the test.  The degrees of freedom, test statistic, and $p$-values were
then calculated according to Pearson's $\chi^2$ test, using 
Yates' correction.  Results are tabulated in Table~S\ref{table:chi2}.
With the exception of \textit{frame-food-culture}, all experiments showed 
significant changes in word-frequencies in response
to the task or framing exposures, at $\alpha=0.05$, leading us to 
reject the null hypothesis that the priming exposures had no effect.

The use of Pearson's $\chi^2$ test has been criticized in linguistic 
applications, due to the finding that many corpora are not themselves
homogeneous \cite{kilgarriff1996comparing}.  
This results from the fact that corpora are made of many
distinct texts, together with the fact that words tend to come in bursts.
When a rare word is encountered in a given text, it is much more likely
to be encountered again in the same text. This means that
frequencies within given texts are often far from the average, causing 
corpora to be heterogeneous.
As a result, if a corpus is randomly split, putting half of the texts in one 
set, and half in another, there can be a significant non-homogeneity between 
the halves (when tested using Pearson's $\chi^2$ test).  Thus, finding two
corpora to be different according to a $\chi^2$ test might not 
indicate that they are meaningfully different.

This concern does not apply to our case, because the ``texts'' are 
individual responses from workers, which will not, in general, be subject to 
the ``burstiness'' phenomenon.  Nevertheless, to test this, we randomly 
divided the workers from each experimental treatment into two sets, and 
tested the homogeneity 
between these sets.  Results are tabulated in Table~S\ref{table:chi2_within}, 
and show that treatments 
\textit{are} homogeneous.  Thus, the lack of homogeneity \textit{between} 
the treatments of given experiments can be rightly attributed to the 
framing and intertask effects.

\subsection*{Measuring the extent of intertask and framing effects}

\paragraph{Comparing $\theta$ and $\chi^2$.}
The following is an illustrative example to show why it is important to 
have a measure of effect size, like $\theta$, in addition to a test of the 
null hypothesis like $\chi^2$.
Suppose that we have a very slightly unfairly weighted coin, which turns
up heads ($X=\mathbf{H}$) more often than expected by one part in one hundred:
$\Pr\{X=\mathbf{H}\} = 0.501$. Such a coin is only mildly biased, and would 
be fine for many purposes.  If we did not know that the coin was
biased, we could determine whether it was biased by flipping the coin in 
question, along with a coin we know to be fair, many times 
(say 100 million times), and then use a $\chi^2$ test to decide whether 
the two coins have the same probability of turning up heads.
Simulating this with a random number generator yields the results shown
in Table~S\ref{table:coin}.  Results from a $\chi^2$ test of the
hypothesis that the two coins have the same probability of turning up heads 
is shown in Table~S\ref{table:coin_stats}.

\setlength{\floatsep}{30pt plus 1.0pt minus 2.0pt}

\begin{table}
\centering
\begin{tabular}{c c c c}
\toprule
coin & $N_\mathbf{H}$ & $N_{\mathbf{T}}$ & total \\
\toprule
\textit{fair} & 50002283 & 49997717 & $10^8$\\
\textit{biased} & 50101115 & 49898885 & $10^8$ \\
\bottomrule
total & 100103398 & 99896602 & $2\times 10^8$ \\
\bottomrule
\end{tabular}
\caption{
	Simulated counts of heads ($N_\mathbf{H}$) and tails ($N_\mathbf{T}$)
	occurrences, for a fair coin, and a coin biased to have a probability
	of turning up heads of 0.501.
}
\label{table:coin}
\end{table}


\begin{table}
\centering
	\begin{tabular}{c c c c }
	\toprule
	Degrees of freedom & $\chi^2$ & $p$-value \\
	\toprule
	1 & 195.4 & $2.2 \times 10^{-44}$ \\
	\bottomrule
	\end{tabular}
\caption{
	Statistics for a $\chi^2$ test of the hypothesis that a fair and 
	unfair coin have the same probability of turning up heads.  
	The result overwhelmingly favors rejection of the hypothesis, even
	though the coins differ only slightly.
}
\label{table:coin_stats}
\end{table}

Even though the $\chi^2$ test shows, with tremendous statistical 
significance, that the unfair coin is indeed unfair, we know that actual 
extent of bias is small, 
$\theta = \frac{1}{2}\left( |0.501 - 0.499| + |0.499 - 0.501| \right) = 0.002$.
This demonstrates how a sufficiently large sample size will lead to the 
detection an arbitrarily 
small bias with arbitrarily high statistical significance. 
This is why, to measure the practical significance of intertask effects,
we rely on a measure of effect size, which is in this case a measure
of statistical divergence, $\theta$.

\paragraph{Statistical approaches to measuring bias.}
Recall that Eq.~\ref{eq:theta} in the main text defined the
extent of bias between the microtask responses from two groups
of workers to be $\theta$ (reproduced here):
\begin{equation}
	\theta = \frac{1}{2}\sum_{x \in X} \left| p_0(x) - p_1(x) \right|,
\end{equation}
where $p_0(x)$ and $p_1(x)$ represent the probability that a worker from
treatment 1 (respectively 2) produces response $x$, from a set of possible
responses $X$.  As defined, $\theta$ corresponds to a measure of divergence 
between $p_0$ and $p_1$, called the \textit{total variation distance} or the
\textit{L1-distance}.

Measuring the \textit{L1-distance} between two probability distributions,
given only samples, is difficult.  The na\"ive approach is to take
the maximum likelihood estimate of $p(x)$, which sets it equal to the 
frequency with which the response $x$ has been observed, 
$\hat{p}(x) = \frac{N_x}{N}$, and then estimate $\theta$ 
using these frequencies\cite{batu2013testing}: 
$\hat{\theta} = \sum_{x \in X}|\hat{p}_1(x) - \hat{p}_2(x)|$.  
However, this method tends to drastically overestimate $\theta$
\cite{val-thesis}.
Recently there has been theoretical progress on this problem.
An estimator of L1-distance, and tests of whether the L1-distance is
greater than a specified threshold, have been proposed 
\cite{val-thesis,batu2013testing,chan2014optimal}.  
These methods have excellent theoretical convergence guarantees, but
cannot easily be used to establish estimates with confidence intervals or 
perform hypothesis tests for fixed sample size and significance level.  
We therefore turn to an alternative approach using machine learning. 
While this method yields an estimator that converges more slowly,
it is straightforward to establish confidence intervals and 
use the estimator in hypothesis tests.

\paragraph{The relationship of bias and classifier accuracy.}
We will now show that, any classifier algorithm, $\mathcal{A}$, that
takes the response, $x$, of a worker, and guesses the population to which
the worker belongs ($P_0$ or $P_1$), will do so with an accuracy, $\eta$,
that is bounded according to Eq.~ from the main text 
(reproduced here):
%\begin{equation}
%	\theta \geq 2\eta - 1.
%	\label{eq:sup:l1}
%\end{equation}

To do so, we will first formalize $\eta$ by defining a 
\textit{validation test}.  In a validation test, a uniform random bit 
$z\in\{0,1\}$ is sampled, and then, according to the value of $z$, the
response of a worker, $x$, is sampled uniformly randomly from $P_z$.  
The algorithm is provided
with $x$ and yields a \textit{guess}, $b=\mathcal{A}(x)$, as to whether the 
worker was 
from $P_0$ or $P_1$.  We shall denote such a validation test by 
$V(P_0, P_1, \mathcal{A})$, and define the value of the test to be 1 if 
$b=z$ and 0 otherwise.  

We then define the accuracy of classifier $\mathcal{A}$ to be $\eta$ according
to:
\begin{equation}
\eta = \mathrm{E\{V(P_0, P_1, \mathcal{A})\}} 
	= \mathrm{Pr}\{V(P_0, P_1, \mathcal{A})=1\}.
\end{equation}

To develop the relationship between $\eta$ and $\theta$, let us consider
the accuracy $\eta^*$ of the best possible classifier $\mathcal{A}^*$.
The best possible classifier will not in general have perfect accuracy:
if the responses from workers in $P_0$ and $P_1$ are distributed 
identically, then there is no hope of determining which population the worker 
came from given only her response $x$.  But, assuming that responses are
not distributed identically, then given some $x$, the best possible 
classifier must always guess 1 whenever $p_1(x) > p_0(x)$ and guess 0 whenever
$p_0(x) > p_1(x)$.

Sacrificing, for a moment, some generality, let us assume that, for some give 
$x'$, $p_1(x') > p_0(x')$.  
When this $x'$ is encounterend in a validation test, the optimal classifier
must guess $b = \arg\!\max_{z}(p_{z}(x'))$, which in this case is $b=1$.
The probability that the classifier guesses correctly in this case is:
\begin{align}
	\begin{split}
	\mathrm{Pr}\{V(P_0, P_1, \mathcal{A}^*) = 1 | x = x' \} 
		&= \mathrm{Pr}\{z = \mathcal{A^*}(x) | x = x' \} \\
		&= \mathrm{Pr}\{z = 1 | x = x' \}  \\
		&= \frac{\mathrm{Pr}\{z = 1 , x = x'\}}
			{ \mathrm{Pr}\{z=0 , x=x'\} + \mathrm{Pr}\{z=1 , x=x'\}} \\
		&= \frac{p_1(x')}{p_0(x') + p_1(x')}.
	\end{split}
\end{align}
And now, with full generally, for any $x'$, where $p_1(x')$ is not necessarily
greater than $p_0(x')$:
\begin{align}
	\begin{split}
		\mathrm{Pr}\{V(P_0,P_1,\mathcal{A}^*)=1 | x = x' \} 
		&= \mathrm{Pr}\{z=\mathcal{A^*}(x)  | x' = x' \} \\
		&= \mathrm{Pr}\{z = \arg\!\max_{z'}\big(p_{z'}(x)\big)| x = x' \}  \\
		&= \frac{\max\big( p_0(x'),p_1(x') \big)}
		{ p_0(x') + p_1(x') }.
	\end{split}
\end{align}
Note that we can rewrite $\max\big(p_0(x'),p_1(x')\big)$:
\begin{align}
	\max\big(p_0(x'),p_1(x')\big) = \frac{1}{2}
		\big(
			p_0(x') + p_1(x') + |p_0(x') - p_1(x')|
		\big),
\end{align}
so,
\begin{align}
	\mathrm{Pr}\{V(P_0,P_1,\mathcal{A}^*) = 1 | x = x' \} 
	&= \frac{1}{2} + \frac{|p_0(x') - p_1(x')|}{2 \big(p_0(x') + p_1(x') \big)}.
\end{align}
Then, summing over all possible responses $X$, 
the overall accuracy of the optimal classifier, $\mathcal{A^*}$, is:
\begin{align}
	\begin{split}
	\eta^* &= \mathrm{Pr}\{V(P_0, P_1, \mathcal{A}^*)=1\} \\
	&=\sum_{x'\in X} \mathrm{Pr}\{V(P_0,P_1,\mathcal{A}^*) = 1 | x = x' \} \mathrm{Pr}\{x = x'\}\\
		&= \sum_{x'\in X} \left(
				\frac{1}{2} + 
				\frac{|p_0(x') - p_1(x')|}{2 \big(p_0(x') + p_1(x') \big)}
			\right)
			\left(
				\frac{p_0(x') + p_1(x')}{2}
			\right)\\
		&= \frac{1}{2}\left(
				\frac{1}{2}(
					\sum_{x' \in X}p_0(x') + \sum_{x' \in X}p_1(x')
				) + 
				\frac{1}{2} \sum_{x' \in X} \big(|p_0(x') + p_1(x')|\big)
			\right) \\
		&= \frac{1}{2} \left( \frac{1}{2}(1 + 1) + \theta \right)\\
		&= \frac{1 + \theta}{2}\\
		\implies \theta &= 2\eta^* - 1.
	\end{split}
\end{align}
By definition, no classifier can be more accurate than $\mathcal{A^*}$, so
for every $\mathcal{A}$
%\begin{align}
%		\theta \geq 2\eta - 1.
%		\label{eq:sup:l1_again}
%\end{align}

\paragraph{Using cross-validation accuracy to lower-bound bias.}
	In order for Eq.~ to be applicable, it is necessary to 
	obtain an unbiased estimate of a classifier's accuracy.  To do so, it is
	essential to test the classifier on data that were not used to build 
	the classifier.  Generally, a portion of the data is used to
	optimize preprocessing, select features, and optimize 
	classifier hyper-parameters, while keeping another portion of the data
	set aside for validation.

	In our case, the data are expensive, and must be divided between 18
	experimental treatments, so we sought to keep as many worker responses
	available 
	for validation
	as possible.  For this reason, we made principled decisions 
	between alternative preprocessing options and features, 
	rather than spending sampled responses on empirically testing these 
	options.  Making
	suboptimal choices can only moderate the observed values 
	of $\theta$, making our experiments more conservative. 
	However, conserving more responses for validation 
	tests, by not using them for classifier optimization, 
	enables us to estimate the resulting classifier's accuracy more precisely.
	Using a Na\"ive Bayes classifier, which does not require hyper-parameter 
	optimization, meant that we also did not need to spend data on 
	optimizing hyper-parameters.  
	
	Thus, no worker responses at all were used for optimizations.  We 
	performed 
	leave-one-out validation, which involves training a classifier using 
	all but one response, and then having the classifier guess the class
	(in our case, the priming exposure) of the left-out worker.  This is 
	repeated $n$ times, where $n$ is the pooled number of worker responses 
	in both
	classes (both treatments in a given experiment), 
	each time leaving out the response of a different worker.

\paragraph{Selection of the na\"ive Bayes classifier.}
We chose to use the na\"ive Bayes classifier as our basis for measuring 
bias for three reasons.  First, we had 119 examples (workers) per class 
(treatment), but thousands of distinct features 
(i.e. the number of distinct words occurring in an experiment).  
Having many more features than examples is problematic for many classifiers
because a large number of features leads to a high-variance, over-fit model,
resulting in poor performance.  But one of the strengths of the na\"ive 
Bayes classifier is that it performs well even when the number of features
exceeds the number of examples\cite{bickel2004, hastie2009elements}

Second, the standard implementation of the na\"ive Bayes classifier has no 
hyper-parameter to optimize. This meant that we did not need to use 
samples for hyper-parameters optimization, which would reduce the number
of samples available for validation tests to estimate $\theta$.

Third, the na\"ive Bayes algorithm is based on an assumption that the 
observation of one feature is independent from the observation of any 
other, once we have conditioned on the class under observation 
%\cite{bishop2006} 
(i.e. the priming treatment).  This
conditional independence assumption is made for
pragmatic rather than theoretical reasons, and is rarely satisfied.  However,
if it \textit{is} satisfied, then the na\"ive Bayes classifier is 
optimal \cite{Zhang2004562}.  In most text documents, 
the existence of long-range dependencies in language 
violates the conditional independence of words: when a rare word is mentioned,
it is more likely to be mentioned again, and topics introduced early in a
text are likely to be revisited.
But in our case, there is little opportunity for such effects.  Workers 
generally do not use the same word in multiple labels for the same image. 
Any overarching ``topics'' within the labels that workers use will tend to 
relate to the test images, which are held in common.
Therefore, in our setup, conditional independence is likely to be 
a realistic approximation, and so the na\"ive Bayes classifier is likely to
be closer to optimal than in other text classification applications.

\paragraph{Comparison to other classifiers.} 

\begin{figure}
	\centering
	\includegraphics[scale=0.75]{figs/theta_sup.pdf}
	\caption{
		Empirical bias, measured using classifiers, induced in image 
		labeling tasks, by exposing workers to initial tasks or framing. 
		The bias was determined using (A-F) a na\"ive Bayes classifier, and 
		(G-L) an SVM classifier, according to Eq.~\ref{eq:sup:l1}.
		In panels A through F, each plot adds an additional
		preprocessing step to those used in the previous plot; the same is 
		done for panels G through H: A,G) no 
		preprocessing; B,H) spelling correction; C,I) stop word removal; 
		D,J) lemmatization; E,K) splitting of multiple-word labels; 
		F,L) distinguishing identical labels entered in different form inputs.
		Standard error bars are shown.
	}
	\label{fig:theta_sup}
\end{figure}
The inequality in Eq.~
asserts that a classifier's performance in predicting class membership
sets a lower bound on the L1-distance between the distributions of features 
for the classes.
This bound is tight for the optimal classifier, and in general, the slack
depends on how the classifier is constructed.

Although we believe the na\"ive Bayes classifier is nearly optimal in this
setting, some other classifier may exist that is significantly better than 
the one used to generate our results.  Furthermore, various decisions about
preprocessing and feature representation also
impact the classifier performance.  
For example, we chose to correct spelling, remove stop words,
lemmatize words, and to distinguish which text-input was used to enter a 
given label.  All of these decisions have some effect on classifier 
performance.

We chose not to optimize these decisions, since it would require the
use of samples (worker-responses), which could then not be used in 
cross-validation to measure $\theta$.  Nevertheless, it is
appropriate to look at what affect these decisions had, \textit{post-hoc}.  
We reproduce the plot from Figure~\ref{fig:theta}A in the main text, 
this time using different combinations of preprocessing options,
and using both the na\"ive Bayes and an SVM classifier, in 
Figure~S\ref{fig:theta_sup}.

Unlike the na\"ive Bayes classifier, when using SVM it is necessary to tune 
the cost and gamma hyper-parameters of the SVM classifier, as well as choose 
a kernel.  We used simulated annealing to optimize the cost and gamma settings
data from \textit{intertask-food-culture}.  For this reason, the estimate
of $\theta$ estimate for this experiment in 
Figure~S\ref{fig:theta_sup} (G to J) is inflated. 

These plots suggest that the values of $\theta$ obtained using the 
na\"ive Bayes classifier and our particular choice of preprocessing steps, 
as shown in Figure~\ref{fig:theta} of the main text, are representative.

\paragraph{Statistical significance and confidence intervals for $\theta$.}
The measurements of $\theta$ were based on the number of correct 
classifications during validation tests.  The number of correct 
classifications, $X$, has a binomial distribution.  
We take the null hypothesis that the classifier performs
no better than chance, $X \sim \mathrm{Bin}(N,0.5)$, where $N$ is the number 
of workers in both treatments of an experiment, and consider an alternative
hypothesis, that the classifier does perform better than chance.  Then, the 
critical number 
of correct classifications, $x^*$, for a one-tailed hypothesis test with
significance $\alpha$ is:
\begin{align}
	&x^* = \sup \left\{
			x: \mathrm{Pr}\{ \mathrm{Bin}(N;0.5) \geq x \} < \alpha
		\right\} \label{eq:sup:crit}\\
	\text{where}\quad &\mathrm{Pr}\{ \mathrm{Bin}(N,\eta) \geq x \} = 
		\sum_{x'=x}^{N} \binom{N}{x'}\eta^{x'}(1-\eta)^{(N-x')}
\end{align}
For the treatments in \textit{intertask-food-objects}, five different 
permutations of each treatment were tested, and a different classifier was 
built for each permutation.  For the sake of estimating accuracy, we assume
that the classifiers under different permutations have the same accuracy,
and so we pool their results.  This means that 
\textit{intertask-food-objects} has five times as many validation tests
(larger $N$) compared to the other experiments.  This larger $N$ makes 
evaluation of \ref{eq:sup:crit} difficult, but also makes the normal 
distribution a very good approximation.  We therefore assume 
that $X$ has a normal distribution for the classification of workers from
\textit{intertask-food-objects}, with $\mu=N/2$ and 
$\sigma = \frac{1}{2}\sqrt{N}$.  The statistics for these hypothesis tests
are tabulated in Tables~S\ref{table:theta} and S\ref{table:theta_pos}.

To determine the confidence intervals for measured $\theta$ values we 
use the exact Clopper-Pearson method. % \cite{clopper1934use}.  
Based on the observed number of successes, $X$, we calculate 
$\eta^*_\mathrm{low}$ and $\eta^*_\mathrm{high}$ from:
\begin{align}
	\eta^*_\mathrm{high} &= \sup
		\left\{
			\eta : \mathrm{Pr}\{\mathrm{Bin}(N; \eta) \leq X \} > 
				\frac{\alpha}{2}
		\right\} \\
	\eta^*_\mathrm{low} &= \inf
		\left\{
			\eta : \mathrm{Pr}\{\mathrm{Bin}(N; \eta) \geq X \} > 
				\frac{\alpha}{2}
		\right\}
\end{align}
Again, for the experiment \textit{intertask-food-objects}, which had 
more validation tests than the others, we derive the confidence intervals
assuming $X$ is normally distributed.
Since the confidence intervals are to be shown on a plot of $\theta$, we 
transform them into equivalent values of 
$\theta$: $\theta^*_\mathrm{high} = 2\eta^*_\mathrm{high} - 1$ and 
$\theta^*_\mathrm{low} = 2\eta^*_\mathrm{low} - 1$.
The statistics collected for the measurement of $\theta$, corresponding to
Figs.~\ref{fig:theta}A and B are tabulated in Tables~S\ref{table:theta} and
S\ref{table:theta_pos} respectively.

To determine whether intertask effects were stronger than framing, we 
perform a two-proportion $z$-test, approximating the binomial distributions
as normal distributions, where the test statistic is given by:
\begin{align}
	z = \frac{\hat{\eta}_1 - \hat{\eta}_2}
		{\sqrt{
			\hat{\eta} (1 - \hat{\eta}) 
			\left( \frac{1}{N_1} + \frac{1}{N_2}\right)
		}},
\end{align}
where
\begin{align}
	\hat{\eta} = \frac{X_1 + X_2}{N_1 + N_2}
\end{align}


Since, \textit{a priori}, we do not know which effects
should be stronger (if any), we perform a two-tailed test.  The results for 
these hypothesis tests are tabulated in Table~S\ref{table:intertask_framing}.
The results show that intertask effects are stronger than framing, but are
on-par with echoed framing.


\begin{table}
\begin{center}
\begin{tabular}{c c c c c c c c }
	\toprule
	\multirow{2}{*}{Experiment} & \multirow{2}{*}{$N$} & 
	\multirow{2}{*}{$X$} & \multirow{2}{*}{$x^*$} & \multicolumn{3}{c}{(\%)}
		& \multirow{2}{*}{$p$}\\ \cline{5-7} \noalign{\smallskip}
	& & & & $\hat{\theta}$ & $\theta^*_\mathrm{low}$ 
		& $\theta^*_\mathrm{high}$  \\
	\midrule
	\textit{intertask-food-objects} & 1190 & 777 & 624 & \textbf{30.6} 
		& 25.2 & 36.0 & $2.5 \times 10^{-26}$ \\
	\textit{frame-food-objects} & 238 & 122 & 133 & 2.5 & -10.6 & 14.7
		& 0.37 \\
	\textit{echo-food-objects} & 238 & 162 &  133 & \textbf{36.1} & 23.4 
		& 47.1 & $1.3 \times 10^{-8}$ \\
	\textit{intertask-food-culture} & 238 & 180 & 133 & \textbf{51.3} & 39.3 
		& 61.1 & $5.0 \times 10^{-16}$ \\
	\textit{frame-food-culture} & 238 & 130 & 133 & 9.2 & -3.9 & 21.3 
		& 0.087\\
	\bottomrule

\end{tabular}

\caption{Statistics for the measurement of the extent of bias, $\theta$,
	induced by intertask and framing effects in various experiments.
	Number of validation tests, $N$; number of successful classifications, 
	$X$; critical number of successful classifications to reject the null 
	hypotheses that the classifier does no better than chance 
	(one-tailed test), $x^*$; 
	estimate of bias based on observed number of successful
	classifications, $\hat{\theta}$; lower confidence interval limit 
	for the estimate of bias, $\theta^*_\mathrm{low}$; upper confidence 
	interval limit for same, $\theta^*_\mathrm{high}$.  Hypothesis test and 
	confidence intervals are based on a significance of $\alpha=0.05$.
	Values for $\hat{\theta}$ in boldface are significantly greater than zero.
}
\label{table:theta}
\end{center}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{c c c c c c c c }
	\toprule
	\multirow{2}{*}{Task position} & \multirow{2}{*}{$N$} & 
	\multirow{2}{*}{$X$} & \multirow{2}{*}{$x^*$} & \multicolumn{3}{c}{(\%)}
		& \multirow{2}{*}{$p$}\\ \cline{5-7}\noalign{\smallskip}
	& & & & $\hat{\theta}$ & $\theta^*_\mathrm{low}$ 
		& $\theta^*_\mathrm{high}$  \\
	\midrule
	1 & 238 & 152 & 133 & \textbf{28.1} & 14.8 & 39.1 
		& $1.12 \times 10^{-5}$\\
	2 & 238 & 138 & 133 & \textbf{15.6} & 2.9 & 27.8  
		& $8.14 \times 10^{-3}$\\
	3 & 238 & 140 & 133 & \textbf{17.8} & 4.6 & 29.5  
		& $3.87 \times 10^{-3}$\\
	4 & 238 & 135 & 133 & \textbf{13.6} & 0.3 & 25.4  
		& $0.0221$ \\
	5 & 238 & 134 & 133 & \textbf{12.4} & 0.5 & 24.6 & $0.0300$ \\
	\bottomrule
\end{tabular}
\caption{Statistics for the measurement of the extent of bias, $\theta$,
	induced by intertask for \textit{intertask-food-objects}, measured 
	for test images at specific positions in the sequence of five test images.
	These values are averaged over five different permutations.
	Number of validation tests, $N$; number of successful classifications, 
	$X$; critical number of successful classifications to reject the null 
	hypotheses that the classifier does no better than chance 
	(one-tailed test), $x^*$; 
	estimate of bias based on observed number of successful
	classifications, $\hat{\theta}$; lower confidence interval limit
	for the estimate of bias, $\theta^*_\mathrm{low}$; upper confidence 
	interval for same, $\theta^*_\mathrm{high}$.  Hypothesis test and 
	confidence intervals are based on a significance of $\alpha=0.05$.
	Values for $\hat{\theta}$ in boldface are significantly greater than zero.
}
\label{table:theta_pos}
\end{center}
\end{table}



\begin{table}
\begin{center}
\begin{tabular}{c c c c c c c c c c c }
	\toprule
	Comparison & $N_1$ & $N_2$ & $X_1$ & $X_2$ & $\hat{\eta}_1$ 
		& $\hat{\eta}_2$ & $z$ & $p$-value \\ 
	\midrule
	\parbox[c]{4cm}{\textit{intertask-food-objects} 
	\textit{vs} \textit{frame-food-objects}} & 1190 & 238 & 777 & 122 &
	0.653 & 0.513 & \textbf{4.1} & $2.1 \times 10^{-5}$ \\ 

\noalign{\smallskip}
\hdashline
\noalign{\smallskip}

	\parbox[c]{4cm}{\textit{intertask-food-objects} 
	\textit{vs} \textit{echo-food-objects}} & 1190 & 238 & 777 & 162 
		& 0.653 & 0.681 & -0.82 & 0.79 \\

\noalign{\smallskip}
\hdashline
\noalign{\smallskip}

	\parbox[c]{4cm}{\textit{intertask-food-culture} 
	\textit{vs} \textit{frame-food-culture}} & 238 & 238 & 180 & 130 
	& 0.756 & 0.546 & \textbf{4.8} & $7.6 \times 10^{-7}$ \\
	\bottomrule

\end{tabular}

\caption{
	Results for two-proportion z-tests, testing the null hypothesis that 
	intertask and framing effects are equally strong.
	In the cases of comparing intertask effects to passive framing, the
	null hypothesis should be rejected ($\alpha=0.05$), but 
	echoed framing effects are on par with intertask effects (second row). 
	$z$-statistics in boldface are significantly different from zero.
}
\label{table:intertask_framing}
\end{center}
\end{table}




\subsection*{Data preprocessing}
	\paragraph{Splitting, lemmatization, removal of stop words, and 
		addition of position tags.} 

	Before performing any analysis on the labels that workers provided, we
	performed a series of preprocessing steps.  
	Labels that contained
	multiple words (separated by spaces or punctuation) were split, with
	punctuation removed, and the separated words were treated as distinct 
	features in subsequent analysis.
	Misspelled words were automatically corrected using a spelling 
	correction algorithm described below.  
	Next, words were lemmatized using the
	wordnet lemmatizer \cite{felbaum1998wordnet}.  %miller1995wordnet,
	Common words such as ``the'', ``to'', or ``with'', were found using
	the natural language toolkit (NLTK) English stop word list 
	\cite{loper2002nltk} and removed.

	For the purpose of training and testing a na\"ive Bayes classifier, we 
	performed an additional preprocessing step.  We tagged words with the
	position in which they had been entered (i.e. which of the five text 
	inputs in the task interface, Figure~S\ref{fig:hit_preamble}) 
	as well as the test tasks in which the word had been provided.
	So, if the word ``wine'' was entered into the second text-input for 
	the third test-task, after preprocessing, the feature ``3\_2\_wine'' would
	appear.  Prepending the task number onto words was simply a means to 
	retain correct attribution of words to images, since providing a word 
	during one task 
	is not equivalent to providing the same word during another task.  
	
	\paragraph{Spelling correction.}  
	Spelling correction was performed using an algorithm that first detected
	if a word was likely to be misspelled, then generated a set of candidate 
	corrections, and chose the best candidate based on a scoring mechanism.
	
	A word was considered misspelled if it was not contained in the 
	\textit{legal set}, which was formed by the union of
	the wordnet knowledge base, the stop word list, and a set of words 
	seen while 
	crawling the world food section of the allrecipes.com website.  We
	describe the crawling of the allrecipes.com website in a section below.

	To correct misspellings, we first produced all possible modified forms 
	that could be obtained applying one or two edits.  An edit consisted of 
	adding or removing a letter, changing one letter into another, or 
	swapping the positions of two adjacent letters.  For the purpose of these 
	edits, spaces were treated as any other letter.

	The candidates produced by these edits which were in the legal set were
	then ranked based on a scoring mechanism, and the highest scoring word
	was chosen as the correction.  A candidate $w$'s score, $s_w$, was 
	calculated according to the following formula:
	\begin{equation}
		s_w = (f_w + 1) \times p_1 \times p_2,
	\end{equation}
	where $f_w$ is the frequency with which the word occurred (correctly 
	spelled)
	within the given task, $p_1$ was a penalty for the first edit, and
	$p_2$ was a penalty for the second edit.  If the word was made using only
	one edit, then $p_2 = 1$.  Any edit that did not involve adding a space
	(i.e. separating a word) incurred a penalty of 0.5, while the addition of
	a space incurred a penalty of 0.1.  Word-separation edits were more 
	strongly penalized because it is often possible to split a series of 
	letters into short two- or three-letter words, which leads 
	to many erroneous corrections.  We found these penalties worked well when
	testing on words taken from initial tasks.

	After all of our analyses had been performed, we checked the accuracy of 
	the spelling correction algorithm using three human coders.  The coders
	were shown a set of 
	500 words that were randomly sampled from the labels attributed during 
	test tasks (50 for each test task).  Before sampling, the labels from all
	experimental treatments, for the given test task, were pooled together.
	The coders did not know which treatment any given word came from.  
	In addition to the words, coders were shown the spelling correction 
	produced by the algorithm, as well as the image from the test task.
	The coders were asked to identify any misspelled words which had not 
	been corrected, as well as any corrections that appeared to be erroneous,
	by indicating their own correction.

	According to the human coders, before spelling correction, 17\% of words 
	were misspelled, but only 3.2\% were misspelled after correction.

	\paragraph{Crawling the world food section of allrecipes.com.}
	The website allrecipes.com was accessed on 3-4 November 2014 using 
	automated scripts.  A total of 2642 recipe listings and 15621 recipe
	instruction pages were retrieved.  Recipe listings were pages that 
	provided lists of recipes, and contained a title and short description 
	for each.  
	The recipe instruction pages had lists of ingredients and preparation 
	instructions.  All of the words found in recipe titles, short
	descriptions, ingredients lists, and preparation instructions were
	collected, and saved as an auxiliary set to augment the \textit{legal set}
	used in spelling correction.
	

\subsection*{Analysis of worker vocabulary}
\paragraph{Most suppressed and activated terms.}
The words whose frequencies differed most between the treatments of given 
experiments are shown in Table~S\ref{table:top-words}.  Note that 
the word that was most suppressed in food-exposed treatments was always 
``food''.


\paragraph{Identification of food-related terms.}
The wordnet knowledge base is composed of synsets, 
which are particular senses 
(meanings) and a set of word forms bearing that meaning. A given word form, 
such as ``ring'' has multiple senses (``to ring a bell'', ``a wedding ring''),
and so can be part of many synsets.  Since the wordnet 
knowledge base was designed
to carry semantic information, the synset is the basic organizing element
of wordnet.

We chose the synsets \texttt{food.n.01} and \texttt{food.n.02} to act as roots
in defining which words should be considered food-related.  Any word form
belonging to a synset which was a hyponym of one of the two root synsets
identified above was considered to be a reference to food.  Here, as in the
main text, when we say hyponym, we mean either a direct hyponym, or a hyponym
of a hyponym, and so on.

This is a stringent notion of a word being a food reference.  It roughly
corresponds to whether or not a given thing is reasonably considered 
consumable.  So, ``orange'' is food, but ``salty'' is not.  Although ``salty''
would usually qualify something edible, it is not itself an edible thing.
On the other hand, ``salt'' would be considered a reference to food under 
this operationalization.

The subset of the wordnet knowledge base induced by the hyponyms of \textit{food.n.01}
and \textit{food.n.02} consists of 3590 word-forms.  Manual
testing showed good coverage, except for references to ethnic foods.  
Since, especially in \textit{intertask-food-culture} and 
\textit{frame-food-culture}, we had images containing many ethnic 
foods, we decided to augment the wordnet knowledge base with words learned by 
crawling the world food section of the allrecipes.com website.  

After collecting a set of words using an automated script 
(described above), we filtered down to the set of words that had been used by
workers and which had been seen during crawling of the allrecipes.com website,
but which were not included in wordnet.  We grafted these extra words into
the wordnet hyponym-hypernym graph manually, looking at the image to which 
they had been attributed (to help disambiguate the intended meaning), but 
without revealing the treatment(s) in which
the word was used.  Many (but not all) of the words added this way did refer 
to ethnic foods, and so this helped increase the coverage of wordnet in that 
respect.

\paragraph{Validation of the detection of food references.}
To determine whether our extended version of wordnet provided a good approach
to detecting food-related words, we sampled 500 words 
(50 from each test task)  from the set of labels produced by workers and had 
three human coders manually decide if they were 
food-related or not.

The coders were instructed to consider whether a word was a noun signifying
an edible item.  The guiding principle was, for the coder to ask herself,
``can I eat X'', where X is the word she is coding.  The coders were
shown the image used in the task from which the word came, to help resolve ambiguity
in the sense of the word that had been given.  Coders were also shown the 
spelling correction (if any) that the spell-correction algorithm had made,
to help interpret misspelled words.

We evaluated the food detection algorithm in two ways.  First, we measured
the inter-rater reliability between the three human coders and the algorithm
(treating the algorithm just like any other coder).  Second, adopting the 
majority code given by the human coders as ground-truth, we determined the 
accuracy of the food-detection algorithm.  
Data summarizing this validation process are given
in Table~S\ref{table:inter-rater}.

\begin{table}
\centering
\setlength{\tabcolsep}{12pt}
\begin{tabular}{ r | c }
\toprule    
\# terms coded & 500 \\
\# food references & 130 \\
\# correct machine codes & 440 \\
inter-rater reliability & 82.4\% \\
human-machine code agreement & 88\% \\
%machine recall & 80 \% \\
%machine precision & 75.4 \% \\
%machine code F1 & 0.78 \\
\bottomrule
\end{tabular}
\caption{
	Results for the validation 
	of the automatic detection of food references, by comparison to
	human-generated coding on randomly sampled words.  In calculating the 
	human-machine code agreement, the majority human code was used.
	Inter-rater reliability was calculated as Krippendorff's alpha, treating
	the machine as any other coder.
}
\label{table:inter-rater}
\end{table}

The results show that good inter-rater reliability was achieved during the
validation (82.4\%).  Looking at the agreement between the 
machine codes and the majority human-generated codes, we find they agree
88\% of the time.  This means that the concept used to instruct human coders
provides a good approximation to what the wordnet-based machine codes actually
represent. In other words, the machine codes correspond closely to indicating
which words correspond to nouns that refer to edible things, and which ones
do not, which provides a clear and simple interpretation for the machine
coding of food and non-food words.

\paragraph{Testing differences in proportions of food-related labels.}
We calculate the proportion of food-related labels in a treatment by first
calculating the proportion of food-related labels per worker, and then
taking the average of this value for all workers.  We calculate the
sample standard deviation for workers' proportions of food-related terms,
$s_w$ and then estimate the standard deviation of the average proportion
to be $s = s_w / \sqrt{N}$, where $N$ is the number of workers in the 
treatment.  The proportions of food-related words for all treatments,
and the statistics for the hypothesis test that the proportions between
treatments within given experiments differ, are shown in 
Table~S\ref{table:proportion-food}.

\begin{table}
\centering{
\setlength{\tabcolsep}{4pt}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{c c c c c c c c c c c}
	\toprule
	\multirow{2}{*}{Experiment} 
		& \multicolumn{2}{c}{food treatment}
		& \ 
		& \multicolumn{2}{c}{other treatment} 
		& \
		& \multicolumn{2}{c}{(other - food)} 
	& \multirow{2}{*}{$z$-score}
	& \multirow{2}{*}{$p$-value} \\ \cline{2-3} \cline{5-6} \cline{8-9}
	& \% refs. 
	& $s$ 					
	& \
	& \% refs. 
	& $s$ 					
	& \
	& $\Delta$\% refs. 
	& $s$ 					 \\ 
	\midrule
	\textit{intertask-food-objects} & 54.5 & 0.6 & \ & 57.4 & 0.8 & \ 
		& \textbf{2.9} & 1.0 & 3.0 & $1.3 \times 10^{-3}$\\
	\textit{frame-food-objects} & 56.2 & 1.4 & \ & 60.1 & 1.4 & \ 
		& \textbf{3.9} & 1.9 & 2.0 & $2.2 \times 10^{-2}$\\
	\textit{echo-food-objects} & 54.0 & 1.4 & \ & 62.8 & 1.7 & \ 
		& \textbf{8.8} & 2.2 & 4.0 & $3.5 \times 10^{-5}$\\
	\textit{intertask-food-culture} & 48.8 & 1.6 & \ & 40.8 & 2.0 & \ 
		& \textbf{-8.0} & 2.6 & -3.1 & $9.2 \times 10^{-4}$\\
	\textit{frame-food-culture} & 45.8 & 2.0 & \ & 45.2 & 1.8 & \ 
		& -0.6 & 2.6 & -0.2 & $0.41$\\
	\bottomrule
\end{tabular}
}
\caption{Statistics for the comparison of the proportions of food-references
	among words used in labels, between treatments of all experiments.  
	$s$: sample standard deviation.
}
\label{table:proportion-food}
\end{table}

\paragraph{Variance in food vocabulary size (lexical richness).}
The number of unique words in a text is strongly dependent on the size
of the text: as more text is sampled, previously unseen words continue
to be found, though at a decreasing rate.  
This means that the vocabulary size of a population of documents is 
not well-approximated by a sample.  Nevertheless the vocabulary sizes
obtained by taking equally-sized samples of two different populations
provides a means to compare the populations on an equal footing.

Taking the difference between one sample's vocabulary and the 
other gives us a measure of their difference in vocabulary size, but 
the variance of this measure needs to be characterized to know whether
there is a significant difference.

To test whether one sample has a (statistically) significantly larger
vocabulary size, in reference to food-related words, than another, we adopt 
the null hypothesis that they
have the same vocabulary, and pool the documents sampled from both 
populations together.  We then randomly partition the pooled sample
back into two balanced sets and take the difference in the sets' 
vocabulary sizes.  This models the difference that would be observed
under the null hypothesis that the documents are drawn from the same
population.  We repeat the pooling and partitioning of the samples
1000 times to generate a bootstrap sample of the difference in 
vocabulary sizes under the null hypothesis.  The 
2.5th and 97.5th percentiles of the bootstrap sample then serve as the
critical values of vocabulary size-difference beyond which the null hypothesis
should be rejected (at $\alpha=0.05$).
Statistics for these hypothesis tests are shown in 
Table~S\ref{table:vocab}.

\begin{table}
\centering{
\begin{tabular}{c c c c c c c }
	\toprule
	\multirow{2}{*}{Experiment} & \multicolumn{2}{c}{$|V|$} 
		 & \ & \multicolumn{3}{c}{$(\%)$} \\ \cline{2-3} \cline{5-7}
	& Food trtmt. & Other trtmt. & \ & $\Delta|V|$ & $\Delta|V|^*_{low}$
		& $\Delta|V|^*_{high}$  \\
	\midrule
	\textit{intertask-food-objects} & 297 & 247 & \ 
		& \textbf{20.2} & -10.7 & 11.5 \\
	\textit{frame-food-objects} & 296 & 261 & \ 
		& \textbf{13.4} & -9.7 & 10.1 \\
	\textit{echo-food-objects} & 316 & 280 & \ & 
		\textbf{12.9} & -11.8 & 11.1 \\
	\textit{intertask-food-culture} & 289 & 244 & \ 
		& \textbf{18.4} & -11.4 & 11.6 \\
	\textit{frame-food-culture} & 318 & 332 & \ & -4.2 & -11.9 & 11.2 \\
	\bottomrule
\end{tabular}
}
\caption{
	Statistics for the measurement of vocabulary size difference, for each
	experiment, between the food and non-food treatments. Vocabulary
	size, $|V|$; vocabulary size of non-food less food treatments, 
	$\Delta|V|$; lower and upper critical values, $\Delta|V|^*_{low}$ and 
	$\Delta|V|^*_{high}$, for statistically significant 
	$\Delta|V|$, at $\alpha=0.05$, based on bootstrapping.  
	Boldfaced values for $\Delta|V|$ are statistically significant.
}
\label{table:vocab}
\end{table}





\paragraph{Calculation of relative specificity}
In the main text we present results for the relative specificity of labels
produced by two experimental treatments.  In performing this calculation,
we used the wordnet knowledge base, which has a set of hypernym-hyponym relationships.
The first step in performing this calculation was to map the words occurring 
in (the labels of) a given experimental treatment onto the wordnet synsets.  
In general, a given word
form can have multiple meanings, and therefore maps onto multiple wordnet 
synsets.

Mapping the words from two treatments onto the wordnet 
hypernym-hyponym graph yields two sets of counts, one set per treatment, 
indicating the number of times a word from a given synset occurred among the
labels of the treatment.  For every synset in one treatment, we looked
at the number of synsets in the other treatment that were more generic
(reachable by following hypernym relations) less the number of words that
were more specific (reachable by following hyponym relations).  This quantity,
tallied over all words in the original synset, gives a (non-normalized) 
measure of the overall degree to which words from the first treatment are more
specific than words from the second treatment.  We then normalized this 
quantity by the total number of possible comparisons between the words of
one treatment to those of the other, where two words are considered comparable
only if one word is the hypernym or hyponym of the other.  Hence, ``statue'' 
and ``bread'' are not comparable, but ``pumpernickel'' and ``bread'' are.

This calculation can be summarized by the following equation:
\begin{equation}
	S(P,Q) = \frac{
		\sum_{w\in P}\sum_{v\in Q} \left(
			\mathbf{1}_{[w>v]} - \mathbf{1}_{[v>w]} \right)
	}{
		\sum_{w\in P}\sum_{v\in Q} \left(
			\mathbf{1}_{[w>v]} + \mathbf{1}_{[v>w]} \right)
	},
\end{equation}
where $P$ and $Q$ are sets of synsets associated to different experimental 
treatments, and $\mathbf{1}_{[w>v]}$ evaluates
to 1 if synset $w$ is more specific than (i.e. is a hyponym of) synset $v$.
This measure counts the excess number of cases where synsets from $P$ are more
specific then synsets from $Q$, as a fraction of all comparable synset pairs. 
The relative specificity lies within $[-1,1]$; we report it as a percentage 
in Table~S\ref{table:proportion-food} and in the main text.

In computing this quantity between two treatments, we first computed the 
relative specificity for the treatments separately for each test task, and 
averaged the results obtained accross the five test tasks.

To assess statistical significance, we used the bootstrap method.
For two given treatments $A$ and $B$, 
119 workers were sampled from each with replacement, giving the bootstrap
samples $A'$ and $B'$, and the relative specificity between these 
was measured as described above.
This was repeated 1000 times.  The 2.5 and 97.5 percentiles from the 
bootstrap sample were used to estimate 95\% confidence intervals.  
A confidence interval not containing zero indicates a statistically 
significant difference in specificity.  
The statistics for relative specificity 
calculations are shown in Table~S\ref{table:specificity}.

\begin{table}
\centering{
	\setlength{\tabcolsep}{4pt}
\begin{tabular}{c c c c c c c c c c c}
	\toprule
	Experiment & $S$ & $S^*_\mathrm{lowCI}$ & $S^*_\mathrm{highCI}$ \\ 
	\midrule
	\textit{intertask-food-objects} & \textbf{13.6} & 9.1 & 17.8 \\
	\textit{frame-food-objects} & \textbf{12.4}     & 7.6 & 17.0 \\
	\textit{echo-food-objects} & \textbf{17.1}      & 12.1 & 21.9 \\
	\textit{intertask-food-culture} & \textbf{19.6} & 13.7 & 25.4 \\
	\textit{frame-food-culture} & \textbf{7.0}      & 2.1 & 15.3 \\
	\bottomrule
\end{tabular}
}
\caption{Relative specificity ($S$) of food-related words between the food- 
	and non-food-exposed treatments of all experiments. 
	Positive values indicates the food-exposed treatment was relatively more
	specific (which was always the case); 
	boldfaced values indicate a disparity in specificity that 
	is statistically significant.  
	$S^*_\mathrm{lowCI}, S^*_\mathrm{highCI}$: lower and upper
	confidence intervals for $S$ based on the bootstrapping approach.  
	Specificites are expressed as percentages.}
\label{table:specificity}
\end{table}




\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
