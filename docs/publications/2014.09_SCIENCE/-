% Use only LaTeX2e, calling the article.cls class and 12-point type.

\documentclass[12pt]{article}

% My packages

\usepackage{graphicx}
\usepackage{amsthm}
\newtheorem{mydef}{Definition}
\usepackage{dcolumn}
\usepackage{multirow}
\usepackage{booktabs}
\newcolumntype{d}{D{.}{.}{4.0}}
\newcolumntype{s}{D{.}{.}{1.4}}

% Users of the {thebibliography} environment or BibTeX should use the
% scicite.sty package, downloadable from *Science* at
% www.sciencemag.org/about/authors/prep/TeX_help/ .
% This package should properly format in-text
% reference calls and reference-list numbers.

\usepackage{scicite}

% Use times if you have the font installed; otherwise, comment out the
% following line.

\usepackage{times}

% The preamble here sets up a lot of new/revised commands and
% environments.  It's annoying, but please do *not* try to strip these
% out into a separate .sty file (which could lead to the loss of some
% information when we convert the file to other formats).  Instead, keep
% them in the preamble of your main LaTeX source file.


% The following parameters seem to provide a reasonable page setup.

\topmargin 0.0cm
\oddsidemargin 0.2cm
\textwidth 16cm 
\textheight 21cm
\footskip 1.0cm


%The next command sets up an environment for the abstract to your paper.

\newenvironment{sciabstract}{%
\begin{quote} \bf}
{\end{quote}}


% If your reference list includes text notes as well as references,
% include the following line; otherwise, comment it out.

\renewcommand\refname{References and Notes}

% The following lines set up an environment for the last note in the
% reference list, which commonly includes acknowledgments of funding,
% help, etc.  It's intended for users of BibTeX or the {thebibliography}
% environment.  Users who are hand-coding their references at the end
% using a list environment such as {enumerate} can simply add another
% item at the end, and it will be numbered automatically.

\newcounter{lastnote}
\newenvironment{scilastnote}{%
\setcounter{lastnote}{\value{enumiv}}%
\addtocounter{lastnote}{+1}%
\begin{list}%
{\arabic{lastnote}.}
{\setlength{\leftmargin}{.22in}}
{\setlength{\labelsep}{.5em}}}
{\end{list}}


% Include your paper's title here

\title{Computational hysteresis:\\ inter-task effects in human computation} 


% Place the author information here.  Please hand-code the contact
% information and notecalls; do *not* use \footnote commands.  Let the
% author contact information appear immediately below the author names
% as shown.  We would also prefer that you don't change the type-size
% settings shown here.

\author
{Edward Newell, Derek Ruths,\\
\\
\normalsize{\texttt{edward.newell@mail.mcgill.ca}}\\
\normalsize{\texttt{druths@networkdynamics.org}}\\
\normalsize{School of computer science, McGill University,}\\
\normalsize{3630 rue University, Montreal, Quebec, H3A 0C6, Canada}\\
\\
}

% Include the date command, but leave its argument blank.

\date{}



%%%%%%%%%%%%%%%%% END OF PREAMBLE %%%%%%%%%%%%%%%%



\begin{document} 

% Double-space the manuscript.

\baselineskip24pt

% Make the title.

\maketitle 



% Place your abstract within the special {sciabstract} environment.

\begin{sciabstract}

People still outperform sophisticated artificial 
intelligince systems in many tasks requiring general knowledge, 
qualitative judgment, and visuo-spatial reasoning. Recently, microtask 
platforms enable the programmer to import human judgment directly into 
algorithms: programs can post tasks, to be completed by users of such 
platforms in near real-time.
Some computer scientists consider microtask platforms to be a fundamentally
new computing architecture, but psychological effects can introduce systematic
bias. 
Here we investigate the effects that earlier tasks have on the results of 
subsequent ones, which we call \textit{inter-task effects}.
In a canonical image labeling task, we found that inter-task effects are
very strong---stronger than overtly framing tasks in terms of 
particular research goals.  However, in some cases, inter-task effects can
be leveraged to elicit more nuanced responses.
To study inter-task effects we introduce a framework that characterizes HPU 
bias in an application-independant way, based on its potential to alter an 
algorithm's path of execution.  Our findings suggest that careful consideration
should be given to inter-task effects when designing crowdsourcing studies. 

%microtask worker performs a series of tasks.  We find that early tasks can
%have a strong effect on later tasks, and that these effects can be stronger
%than those brought about by framing.
%
%course of human computation algorithms.
%application-independant quantity that measures the
%severity of priming.  We apply this framework to measure 
%
%in many
%tasks, especially ones requiring general knowledge about the world, qualitative
%judgment, and visuo-spatial reasoning.  Recently, microtask web platforms have 
%emerged. One can upload a batch of tasks to be completed by users who are paid 
%for their efforts.
%Though simple on the surface, these platforms dramatically enlarge the
%programmers toolkit: in effect they allow the programmer to import human 
%judgment into algorithms.  Some computer scientists consider
%such platforms to be a new computing architecture.  But people are very 
%different from electronic processors.  For example, context affects how a 
%person will respond to a prompt, and this complicates the formal treatment of 
%human computation.  To approach this issue, we introduce a framework 
%that quantifies the severity of priming in an application-independant way.
%We apply this technique to reveal an unforseen source of bias: the bias 
%induced by one task on another.  Surprisingly we find that 
%\textit{inter-task effects} introduce more bias than \textit{framing}, which
%has received more attention.
%
%more well-known 
%\textit{framing}, a more well-studied form of priming.
%
%Microtask platforms offer a fast, flexble, and inexpensive source of labour for
%clerical tasks \cite{Finnerty2013, chandler2013breaking, Berinsky2012351} and 
%recently have been adopted by researchers to augment datasets and solicit 
%participants \cite{paolacci2010running, Berinsky2012351, chandler2013breaking}.
%Computer scientists have described such platforms as a new computing 
%architecture, and seek to characterize “human co-processing units” (HPUs) in 
%analogy to CPUs and GPUs \cite{5543192}. However, it is known that people are 
%susceptible to 
%priming effects\cite{No2007,Swaab200299,Gopher2000308,sohn2001task,Ghuman17062008,BJOP1796,BJOP1826,Gass1999549}, which stands to complicate the HPU 
%perspective.  Here, we introduce a framework to recast this psychological 
%phenomenon as a kind of computational hysteresis. 
%We apply this framework to study \textit{inter-task effects} that arise when 
%microtask 
%workers perform a sequence of tasks. Surprisingly, earlier tasks can have a 
%very strong influence on workers during later tasks---stronger than overtly
%framing the tasks as serving different research goals.   
%This result has major implications for the design of crowdsourcing initiatives.
%The framework we introduce provides a general measure of priming severity, 
%and, in combination with machine learning tools, enables measurement without
%knowing ahead of time how priming will affect output.
\end{sciabstract}

\section*{Introduction}
Microtask crowdsourcing platforms like Amazon Mechanical Turk (MTurk) make it 
possible for requesters to submit batches of small tasks to a large pool of 
workers, who do the tasks for fun, a sense of purpose, and remuneration 
\cite{kazai2013analysis,Antin20122925}.  
Originally used to distribute clerical work, these platforms 
increasingly serve as a means to engage experimental 
participants in a research 
setting \cite{paolacci2010running,Berinsky2012351,snow2008cheap,alonso2009can}.
Typical tasks include tagging and categorizing images 
\cite{6116320,Zhai2012357}, transcribing voice recordings 
\cite{chandler2013breaking,paolacci2010running}
or handwritten notes \cite{Berinsky2012351,Finnerty2013}, and judging the 
relevancy of search results \cite{le2010ensuring,grady2010crowdsourcing,alonso2009can,kazai2013analysis}

These platforms make it possible to seemlessly integrate human and 
machine computation.  Researchers coined the term HPU 
(Human co-Processing Unit) in analogy to CPUs and GPUs \cite{5543192}.  
Attempts have been made to establish an HPU instruction set.  For example,
the X library provides function calls such as 
\texttt{X.vote()}, and \texttt{X.map()}
 \cite{little2010turkit,minder2011crowdlang,minder2012crowdlang,kittur2011crowdforge}.

Of course, there are obvious differences between HPUs and their silicone
counterparts.  When facing the same task, people do not generally
respond in the same way.  Common sense would dictate that a certain amount
of variability in responses made by people is inescapable.  But an important 
part of the variation is bound to be systematic, due to psychological or 
demographic factors.

Here, we highlight an important source of bias with
serious implications for any crowdsourcing initiative. 
It is known that people are susceptible to priming effects 
\cite{BJOP1796,No2007,beller1971priming}, and, in particular, task-repetition 
effects \cite{Gass1999549,sohn2001task}.  Thus, a worker's response during
one task may depend in part on previous ones.  Such \textit{inter-task} effects
would amount to a kind of \textit{hysteresis}, meaning that HPU outputs are not
only a function of the current input, but also of the history of inputs.  

There has been considerable investigation into the factors that affect the 
output from microtask work.  These include such factors as the 
level of 
pay \cite{kazai2013analysis}, training \cite{le2010ensuring}, pre-screening of 
workers \cite{paolacci2010running}, and user-interface design 
\cite{Finnerty2013}.  Researchers have also investigated \textit{framing}, 
by testing the effects of describing the workflow context 
\cite{Kinnaird2012281}, the purpose of tasks 
\cite{chandler2013breaking}, or using an alternative problem discription
\cite{thibodeau2013natural}.  To our knowledge, no study has investigated 
inter-task effects on microtask platforms.

We make two contributions towards understanding inter-task effects,
and systematic bias in HPUs in general.  First, we present a framework for 
quantifying bias in HPU outputs in terms of their potential impact 
algorithms.  This yields an approach for measuring the severity of bias 
that removes the need to understand the underlying psychological or 
demographic factors at play.  Second, we apply our framework to measure the
severity of inter-task effects on a canonical microtask: image labeling.

Using the  Amazon Mechanical Turk (Mturk) platform, we solicited workers to 
perform a series of image-labeling tasks.  We chose image labeling because it 
is one of the most common kinds of microtasks
\cite{chandler2013breaking,Berinsky2012351,Finnerty2013,paolacci2010running}, 
and it seems likely that this will remain the case in the long term because
of applications in computer vision research \cite{5543192}.
%In these tasks, workers labeled images featuring iconic cultural scenes,
%food, table-settings, and various utensils and cookware. 
We divided the tasks into an \textit{initial} and \textit{test} set, 
but, crucially, there was no distinction between these sets from the
perspective of the worker.  We varied the initial tasks, while 
keeping the test set the same, to analyze the effect of initial tasks on 
the labels provided during the test tasks.

As a point of comparison, we included treatments in which we induce 
\textit{framing}.
Before the workers labeled images, we indicated that the work 
was funded by a research lab with a particular area of interest, to see the 
influence this has on the workers’ subsequent labels. 
Surprisingly, inter-task effects were stronger than framing, in some cases 
about twice as strong. 
Our results demonstrate that initial tasks can change workers' focus in later 
tasks.  We also found that inter-task effects altered the level of specificity 
of worker's labels.  Our findings suggest that careful consideration should be 
given to the bundling of tasks when designing a study using a microtask 
platform.

\paragraph*{Computational hysteresis.}
Before describing our experiments, we dedicate some space to discussing
how we characterize and measure priming in HPU outputs.  Since HPU outputs are
stochastic, determining the effect of priming on an HPU population
amounts to determining the resulting change to the HPU population's 
distribution of outputs. 

To refine this, we distinguish between methods that determine \textit{whether}
two random sources have the same distribution 
(e.g. Pearson's chi squared test), from those that determine
\textit{the extent} of that difference (e.g. one of the many measures of 
statistical divergence).  We are interested in measuring the extent of
difference because our practical concern is the potential for such
differences to alter the course of an algorithm.

To measure the effect of some priming treatment, we must of course draw a 
comparison to some standard population.  In many cases, there may be an 
obvious comparison between a primed and unprimed population.  However, we 
refrain from proposing a canonical neutrally-primed HPU population; what is 
considered ``unprimed'' in one scenario may be ``primed'' in another. 
Instead, we speak of two HPU populations being \textit{differently primed}.

We present three alternative, equivalent definitions for the 
\textit{difference in priming} of two HPU populations, $\mathcal{P}$, and 
$\mathcal{Q}$, which we denote $\theta(P,Q)$.  To set up the definitions,
we will assume that $\mathcal{P}$ and $\mathcal{Q}$ 
were sampled randomly from the same initial population $\mathcal{R}$, and 
then were exposed to (possibly) different priming treatments. Then, the HPUs 
in $\mathcal{P}$ and $\mathcal{Q}$ perform some task, leading to popualtions 
of work-products, $P$ and $Q$.

The first definition provides the motivation for our choice of divergence
measure, because it directly measures the proportion of the time that a
binary decision will be changed due to priming effects:

\begin{mydef}
	\label{def:bias}
	{\upshape Maximum Bias:}
	Let $\mathcal{D}$ be a binary decision function, which takes a 
	work-product
	$r \in P \cup Q$ as input, and outputs a bit $\mathcal{D}(r)=b$, where 
	$b \in \{0,1\}$. Denote the \emph{bias} of $\mathcal{D}$, due to the
	different priming of $\mathcal{P}$ and $\mathcal{Q}$ by
	$\theta_\mathcal{D}(P,Q)$ and define it to be the difference 
	in the expected output of $\mathcal{D}$ when acting on work-products from 
	$P$ and $Q$:
	$$
	\theta_\mathcal{D}(P,Q) = 
		\left| 
			\mathrm{E}\left\{ \mathcal{D}(p) \right\} 
			- \mathrm{E}\left\{ \mathcal{D}(q) \right\} 
		\right|,
	$$
	where $p \in P$, $q \in Q$.  Then, define the difference in priming 
	between $\mathcal{P}$ and $\mathcal{Q}$ to be the worst-case bias, i.e. 
	the bias of the most susceptible decision:
	$$
	\theta(P,Q) = 
		\sup_\mathcal{D} \theta_\mathcal{D}(P,Q)
	$$
\end{mydef}
If $\mathcal{D}$ is the condition of an \texttt{if}-statement, then the 
definition above is the change in the fraction of the time that a given 
path of execution is followed due to HPU priming.

While this is precisely what we want to measure, it is not at all clear how
to go about doing it.  The second formulation provides a practical connection
by rephrasing priming in terms of its distinguishability:

\begin{mydef}
	{\upshape Distinguishability:}
	Let $\mathcal{D}$ be a distinguisher algorithm, which takes work-product 
	$r \in P \cup Q$ as input, and outputs a \emph{guess} 
	$\mathcal{D}(r) \in \{0, 1\}$. Now define a validation test 
	$V(\mathcal{D}, R_0, R_1)$ as follows. A bit is drawn uniformly at random, 
	$b \in_R \{0, 1\}$, and then a work-product $r$ is chosen uniformly at 
	random from one of the populations, decided by the bit: $r \in_R R_b$. 
	Input $r$ into $\mathcal{D}$, and if $\mathcal{D}(r) = b$, we say 
	$\mathcal{D}$ \emph{guessed correctly}, and the value of $V$ is 1, 
	otherwise $V = 0$.

	A distinguisher that guesses randomly will still be correct half of the 
	time. Therefore, denote the performance of $\mathcal{D}$ by 
	$\eta_\mathcal{D}(P, Q)$, and let
	$$
		\eta_\mathcal{D}(P,Q) = 2\cdot \mathrm{E}\{V(\mathcal{D},P,Q)\} - 1.
	$$
	Then define the difference in priming between $\mathcal{P}$ and 
	$\mathcal{Q}$ to be the performance of the best possible distinguisher:
	$$ 
		\theta(P,Q) = \sup_\mathcal{D} \eta_\mathcal{D}(P,Q)
	$$
	\label{def:dist}
\end{mydef}

\textbf{Definition 2} relates the difference in priming to the 
\textit{optimal Bayes classifier} for determining whether a given HPU output
came from $P$ or $Q$.  This means that we can obtain a lower bound on
$\theta(P,Q)$ based on the performance of an actual classifier, which provides
recourse when purely statistical methods fail.

The statistical divergence measure underlying the previous definitions is the
divergence measure known as the ``total variational distance'' or 
$l_1$-distance:
\begin{mydef}
	\label{def:l1}
	{\upshape Total variational distance $\theta_{l_1}$:}
	Define the difference in priming between $\mathcal{P}$ and $\mathcal{Q}$ 
	to be equal to the total variational distance between the work-products 
	$P$ and $Q$:
	$$
	\theta_{l_1}
	= \sup_{A \subset \mathcal{X}} \left| \sum_{x \in A} f_P(x) - f_Q(x)\right|
	= \frac{1}{2} \sum_{x \in \mathcal{X}} \left| f_P(x) - f_Q(x) \right|,
	$$
	where $f_P(x)$ denotes the probability of observing x when sampling from P.
\end{mydef}
The equivalent formulation for $\theta(P,Q)$ in 
\textbf{definition \ref{def:l1}} provides a grounding in a fundamental
measure of statistical divergence, and a connection to information theory.  
The equivalence of the three definitions
follows from the known fact that no algorithm can have output that differs
on inputs $P$ and $Q$ more than $l_1 \times 100\%$ of the time.  We provide
proofs in the supplementary material.

The difficulty of measuring $\theta(P,Q)$ depends on what the universe of
possible outputs are, as well as how $P$ and $Q$ are actually distributed,
both of which are mainly determined by the task that the HPUs are asked to 
perform.  In the case where workers label an image with a free-text input, 
the number of different possible labels can be in the thousands or more.  If
the support, $\mathcal{X} = P \cup Q$, is large compared to the number of 
HPU outputs sampled, so that many labels appear only once (which in any case
is expected from Zipv's law), then estimating $\theta$ directly from the
observed frequencies of labels is difficult.  

One approach to estimating the $l_1$-distance from samples, which appears
reasonable on the surface, is to use the observed frequencies of each output
$x$ from $P$ and $Q$ to estimate $f_P(x)$ and $f_Q(x)$, and calculate 
$\theta(P,Q)$ directly from \textbf{definition 3}.  This is the so-called naive
approach to estimating $l_1$-distances, and is aptly named because it tends
to produce very inflated values.  More recent techniques have for estimating
$l_1$ have been suggested by Valiant \cite{val-thesis}, and Batu 
\cite{batu2000testing,batu2013testing}.  We reserve the discission of these
techniques to the supplementary material; they do not turn out to be useful
because they do not provide confidence intervals.  Thus our only recourse is
to test the performance of classifiers at distinquishing $p \in P$ from 
$q \in Q$, which allows us to estimate a lower bound on $\theta(P,Q)$.  This 
turns out to be sufficient, and has the advantage that, so long as they are
learnable, it is not necessary to understand exactly how $P$ and $Q$ differ.


\paragraph{Experimental setup.}
We performed two experiments, soliciting respectively 900 and 2300 MTurk 
workers to label images depicting iconic cultural scenes, meals, ingredients, 
table-settings, and various utensils and cookware. In these experiments,
workers were randomly assigned to treatments, and depending on this 
assignment, were exposed to different framing primes, given a different 
set of initial images, and shown a different permutation of the test images. 
For both experiments, workers were first shown brief instructions.  
The framing prime was then shown, unless the given 
treatment involved no framing.  Workers then performed a series of 
image-labeling tasks and had to provide five labels per image. 
For the purpose of analysis, we denote the first five images as the 
\textit{initial set}, and the last five as the \textit{test set}, but no
distingction was made from
the perspective of the worker.
Table 1 sumarizes the various experimental treatments for both 
experiments.

\begin{table}[t]
\centering
	\begin{tabular}{ l  l  l }
		\hline                       
		Treatment & Framing prime & Initial image set	\\ 
		\hline                       
		$\textsc{ambg}$ & None & Ambiguous\\
		$\textsc{img:cult}$ & None & Cultural\\
		$\textsc{img:ingr}$ & None & Ingredients\\
		$\textsc{frm:cult}$ & Cultural & Cultural\\
		$\textsc{frm:ingr}$ & Cultural & Cultural\\
		\hline  
	\end{tabular}


	\caption{ \footnotesize{ 
		Workers were uniformly randomly assigned to one of the 
		treatments listed above. 
		The full funder names used were 
		``The Global Foundation for Cultural Recognition'' and 
		``The National Foundation for Nutritional Awareness''.  
		The ambiguous, cultural, and ingredients initial image sets are shown 
		in \textbf{Figs. 2}, \textbf{3}, and \textbf{4}.
	}}
	\label{table:1}
\end{table}

\begin{figure}
	\centering
	\includegraphics[scale=0.7]{figs/tasks.pdf}
	\caption{Examples of slides used for the image labeling tasks. A) instructions shown to all workers; B) a framing slide; C) a slide from the initial set for treatment IMG:FOOD; D) a slide from the initial set for treatment IMG:OBJ; E) a slide from the test set shown in all treatments. The full set of slides used for all treatments is presented in the supplementary material.}
	\label{fig:task}
\end{figure}




\paragraph{The strength of inter-task effects}
The purpose of the first experiment was to compare the relative strength
of inter-task and framing effects.  In this experiment, the treatment 
\textsc{ambg} served as a base-case.  The other treatments differed from
\textsc{ambg} either in terms of the initial image set, or by the addition of a
framing prime.  

The test images in this experiment 1 consisted of meals having
a strong, specific cultural connection (Fig. \ref{fig:task} shows an example).
The \textsc{ambg} treatment had an initial set which also consisted of meals,
but without more muted cultural overtones.  The initial images for treatment 
\textsc{img:cult} featured iconic cultural scenes (devoid of food) while 
those for treatment \textsc{img:ingr} consisted of isolated ingredients.
Treatments \textsc{frm:cult} and \textsc{frm:ingr} had the same initail images
as \textsc{ambg}, but had respectively a framing prime indicating that the
tasks were funded by a foundation for ``cultural recognition'' and
``nutritional awareness''.

We tested for inter-task effects by evaluating the performance of a Naive 
Bayes classifier when distinguishing the work-products from 
\textsc{img:cult} and \textsc{img:ingr}, and, separately, when distinguishing
those from \textsc{frm:cult} and \textsc{frm:ingr},
using only the labels that workers from these treatments
attributed to the test set of images.

can perform better than the actual difference in priming, so the performance 
of Naive Bayes classifier provides a lower bound on priming difference. We 
used leave-one-out validation to estimate the classifier per- formance. We 
tested for framing effects in a similar way, by training a Naive Bayes 
classifier to classify work-products among \textsc{frm:food} and 
\textsc{frm:obj}.

Overall, the first image in the test set showed the strongest signal for 
priming effects. Fig. 3 shows the empirical lower bound for difference in 
priming due to inter-task effects and framing based on the classifier 
performance when using labels from the first test image. Statistically 
significant levels of priming were detected for both modalities, but the 
inter-task effects were stronger (hypothesis test). This suggests that strong 
bias can result solely from the sequence of tasks performed by a worker, even 
when those tasks are of the same type.

In contrast, the classifier never performed significantly better than chance 
when distinguish- ing treatments that differed only in framing. Thus, framing 
effects were rather weak. In fact, framing effects were generally weak in our 
results, so we omit treatments involving framing from the remainder of our 
presentation to save space. We encourage the interested reader to look at the 
results for these treatments in the supplementary material.

%The first two treatments, IMG:OBJ and IMG:FOOD, tested inter-task effects. 
%Workers in these treatments labelled ten images. In the IMG:OBJ treatment, the 
%\textit{initial set} of five images labeled by workers contained objects such 
%as utensils, kitchenware, and table-settings, but no food items. In the 
%\textsc{img:food}, the initial set contained centered close-ups of meals, 
%without
%any surrounding objects (except sometimes the dish supporting the food). No 
%interruption or distinction made between the initial and test sets from the 
%view of the workers. To study the positional dependance of inter-task effects,
%these treatments were subdivided, and workers were shown one of five 
%permutations. There are of course 5! = 120 permutations of five images, but 
%investigating all permutations was too expensive, and probably redundant, so 
%we chose 5 permutations that ensure each image occupies each position within 
%the test set. We discuss the benefits and drawbacks of this choice in the 
%supplementary material.
%
%Workers in the treatments \textsc{frm:food} and \textsc{frm:obj} were shown a 
%framing slide, which bore a message reading: ``Funded by the laboratory for 
%the visual perception of \{Food and Ingredients $\vert$ Objects and Tools\}''. 
%After the framing slide, workers in these treatments proceeded directly to 
%the test set (and so labeled only 5 images).
\paragraph{Inter-task effects and the orientation of focus.}
Since the initial images were chosen to emphasize either food (ingredients 
set) or culture (cultural set), we looked for effects on the number of 
culture- and food-oriented labels that workers attributed to the test image 
set.

To this end, we constructed an ontology from the labels attributed to the test 
images. In the ontology, edges point from more general labels to more 
specific ones. For example, the ontology contains the path \texttt{food} $\to$ 
\texttt{ingredients} $\to$ \texttt{vegetables} $\to$ \texttt{tomato}.

Since food is a central feature of culture, our ontology contains many labels 
that have both food and culture in their ancestry. Nevertheless, there were 
many food-oriented labels, such as bread, which lacked specific cultural 
connections, as well as non-food, culture- orientedlabels,suchasrussian dolls.

When we tallied labels attributed to the first image of the test set, we found 
that workers from CULTimg produced significantly more culture-oriented labels 
and less food-oriented ones than those from AMBG (see Fig. 2A). The inter-task 
effect was so strong that the proportion of food- and culture-oriented labels 
in CULTimg was essentially the reverse of that in AMBG. However, the 
composition of labels attributed by INGRimg was not significantly different 
from those attributed by AMBG. This shows that inter-task effects can 
profoundly alter workers' focus. Again, we will argue later that the ambiguous 
and ingredients image sets are, in fact, quite similar.

\paragraph{Inter-task effects and specificity.}
Using the ontology described in the previous section, we can compare the 
specificity of two labels $\ell_1$ and $\ell_2$. We say that $\ell_2$ is more specific 
than $\ell_1$ if there is a directed path from $\ell_1$ to $\ell_2$. If there is no 
directed path between labels, we say they are \textit{non-comparable}. For 
example, \texttt{tomato} was more specific than \texttt{food}, while 
\texttt{statue} and \texttt{food} were non-comparable.

We can then define the relative specificity of two workers with respect to 
test-image $i$ as $s_i(u, v)$:
$$
	s_i(u,v) = \sum_{\ell \in u(i)} \sum_{\ell \in v(i)} 
	\left( \mathbf{1}_{[\ell > m]} - \mathbf{1}_{[m>\ell]} \right),
$$
where $u(i)$ denotes the set of labels attributed by worker $u$ to image $i$, 
and $\mathbf{1}_{[l>m]}$ evaluates to 1 if $\ell$ is more specific than $m$, 
and 0 otherwise. We then define the relative specificity of two treatments, 
$U$ and $V$, relative to the $i$th image, denoted $S_i(U,V)$, to be the mean 
relative specificity of two uniformly drawn workers:
$$
	\hat{S}_i(U,V) = \frac{1}{|U \times V|} \sum_{u \in U} \sum_{v in V}
		s_i(u,v).
$$
Looking at labels attributed to the first test image, we found that workers 
from \textsc{ambg} were more specific than workers from either 
$\textsc{CULT}_{img}$ or $\textsc{ingr}_{img}$.  When we compare CULTimg to 
INGRimg, we found that workers from INGRimg were more specific. This shows 
that inter-task effects do substantially influence the specificity of labels 
that workers provide. When we discuss image similarity, below, we will explore 
a possible mechanism for this effect.

\paragraph{The role of task position in inter-task effects.}
It would stand to reason that, as workers proceed through the test images, 
priming from the initial images would be ``washed out'', diminishing the 
observed
inter-task effects. Looking at the classifier performance, 
$\hat{\theta}_\mathrm{NB}$, with respect to the treatments AMBG and CULTimg, 
we find that performance does drop off dramatically after just the first image 
(see Fig. 3A). This suggests that inter-task effects primarily arise between 
consecutive tasks, but that a residual effect can persist for at least five 
tasks (we note, however that although the classifier performance seemed better 
than chance for the fourth test image, this cannot be asserted with 95\% 
confidence).

Next we observed how inter-task effects on label composition (i.e. food- vs 
culture-oriented labels) evolve as workers proceed through test images. We 
define the excess cultural orientation as the number of culture-oriented 
labels minus the number of food oriented ones. To meaning- fully compare the 
excess cultural orientation between test images, we must, however, account 
for the fact that some images inherently carry more cultural content than 
others. In keeping with our notion of priming difference, we calculate the 
excess cultural content for both CULTimg and

AMBG, and take their difference to be the relative excess cultural content, 
$\Delta_{cult}$. Formally,

		*** Formula for delta cult\ldots

where $N^(i)_{w,cult}$ stands for the number of culture-oriented labels 
attributed by worker w to image i, while $N^(i)_{w,food}$ similarly counts
food-oriented labels, and $N$ is the total number of labels in a treatment.

We found $\Delta_{cult}$ was largest for the first test image, but dropped off 
rapidly, 
remaining positive but not to a statistically significant extent (see Fig. 3B).
This is similar to the behavior of $\hat{\theta}_\mathrm{NB}$, however, 
because the statistical significance of $\Delta_{cult}$ dropped more quickly, 
$\hat{\theta}_\mathrm{NB}$ appears to be a more sensitive measure of 
inter-task effects.

\paragraph{Leveraging inter-task effects for detailed responses.}
\begin{table}
\centering
\begin{tabular}{ l  s s s s}

\toprule    
Image set   
& \multicolumn{1}{c}{Ambig.} 
& \multicolumn{1}{c}{Cultural} 
& \multicolumn{1}{c}{Ingr.}
& \multicolumn{1}{c}{Test} \\
  
\midrule

Ambiguous  & 1 & 0.0418 & 0.142 & 0.167 \\

Cultural  & 0.0418  & 1 & 0.0347 & 0.0561 \\

Ingredients  & 0.142  & 0.0347 & 1 & 0.110 \\

Test & 0.167  & 0.0561 & 0.110 & 1
\\
\bottomrule

\end{tabular}
\caption{\footnotesize{
Pairwise similarities of each image set based on the labels attributed to them (see \textbf{Eq. 4}).
}}
\label{table:2}
\end{table}

To continue with the analysis, we seek a measure of image similarity. The 
characterization of image content is a deeply complex issue that has been 
approached by many disciplines \cite{panofsky1939studies,shatford1986analyzing,Tversky1977327,Jaimes20002}. \textit{Perceptual similarity} is a 
particularly recalcitrant concept.

However, in the present study, we are more interested how similar two sets of 
images are, with respect to the labeling task, which is much simpler to 
operationalize than general percep- tual similarity. Here, we measure image 
similarity by looking at the fraction of labels that they share. Formally, to 
measure the similarity between two sets of images, X and Y , we compute the 
Jacquard index between the sets of labels attributed to them:
$$
	\mathrm{Sim}(X,Y) = \frac{L(X) \cap L(Y)}{L(X) \cup L(Y)}
$$
where $L(X)$ denotes the set of labels attributed to $X$.

The pairwise similarities of the image sets are presented in Table 2. First, 
observe that the ambiguous and ingredients sets are much more similar to each 
other than either is to the culture set. Recalling the data presented in 
Fig. 1, this helps explain why two treatments could not be distinguished on 
the basis of having differing initial sets corresponding to ambiguous and 
cultural. This also explains why AMBG and CULTimg had similar label 
compositions in Fig. 2A.

Next we draw attention to the similarity between the initial sets and the test 
set. The am- biguous set is the most similar to the test set, followed by the 
ingredients set, with the cultural set being most different. Observe that these
levels of similarity mirror the label specificity for the corresponding 
treatments (c.f. Fig. 2B).

This suggests that labeling a series of images that are similar to one another 
might encourage workers to use more specific terminology. Such a phenomenon 
would be consistent with the psychological mechanism known as 
\textit{negative priming}.
Negative priming occurs when a person becomes desensitized to non-salient 
stimuli to which she is repeatedly exposed. Consider that workers who 
initially labeled the ambiguous image set had already seen five images 
showing prepared meals once they labeled the first test image. At that point, 
a worker might not regard the generic labels \texttt{food} or \texttt{meal} to 
be salient, and opt instead for \texttt{bread}, or \texttt{pasta}.
 
We are suggesting that, although workers are not instructed to compare images 
in any way, prior tasks nevertheless create a frame of reference relative to 
which later tasks are judged. This in turn influences the perception of 
salience. Thus, in a series of subjective characterization tasks that have 
very similar content, workers' focus will tend to be directed away from 
generic, shared attributes, towards those attributes that are specific and 
distinguishing.

\paragraph{Conclusions.}
Our results show that inter-task effects can have a strong influence on how 
workers label images. In particular, we observed that prior tasks influence 
the specificity and content of labels. Surprisingly, framing a task by 
indicating a semantically-loaded funder had much milder effects on worker 
outputs, generally below the statistical power of our tests.

We therefore caution those designing studies using human computation: even if 
the requester has eliminated surrounding influences to every practical extent, 
\textit{the greatest source of bias might lurk in the tasks themselves}. Due 
consideration should be given to how tasks are bundled together. While our 
measurements of $\hat{\theta}_\mathrm{NB}$ indicated that inter-task effects 
persisted even to the fifth test image, the most severe effects arise between 
consecutive tasks.

Our proposed priming definition yields general purpose measure of priming 
effects. We used three approaches to detect priming effects: 1) using a Naive 
Bayes classifier to measure $\hat{\theta}_\mathrm{NB}$, 2) tallying the 
number of culture- and food-oriented labels, and 3) comparing the relative
specificity of labels based on an ontology. Of the three, $\theta_\mathrm{NB}$ 
was the most sensitive.

In one sense, this is not surprising because the Naive Bayes classifier takes 
into account the frequency of occurrence of all labels encountered during 
training. But it is remarkable that the method which incorporates neither 
prior knowledge about the image contents nor label semantics, outperforms 
those that do. This is a powerful feature because, given the outputs from two 
sets of workers, one can test whether they have been differently primed 
without knowing how that priming might manifest.

Our algorithmic definition of priming is phrased in terms of worker outputs 
rather than psychological phenomena. This provides a connection between the 
priming effects and their potential to impact decisions made on the basis of 
worker outputs. In the case of a 1-bit binary decision, $\theta$ describes the 
worst-case bias introduced by failing to control for priming effects.

Batching similar tasks together appears to yield higher more specific HPU 
outputs. Our proposed connection between similarity and specificity during 
image-labeling might be used to tune the specificity of labels. For example, 
If one seeks very nuanced labeling, our results suggest that the images 
should first be sorted into batches based on their similarity. This could be 
accomplished by beginning with a first, coarse labeling of unsorted images, 
followed by bundling based on the similarity of coarse labels. Then, bundles 
of similar images could be served for a second round of finer labeling. The 
sorting and re-labeling could in principle be repeated.

Such a workflow involves serial processing, which points to an interesting 
possible differ- ence difference between HPUs and CPUs. In general, whenever 
an aspect of a problem can be parallelized when employing CPUs, one gains 
efficiency. But here, because of HPU hysteresis, one might gain precision by 
using HPUs with a more serialized algorithm. Further testing is needed to 
determine the gain in precision from this approach.


\bibliography{newbib}
\bibliographystyle{Science}



\end{document}




















