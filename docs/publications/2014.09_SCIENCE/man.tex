% ULaTeX2e, calling the article.cls class and 12-point type.

\documentclass[12pt]{article}

% My packages

\usepackage{amsmath}
\usepackage{framed, color}
\usepackage{soul}
\definecolor{blu}{rgb}{0,0,1}
\newcommand{\td}[1]{{\color{blu}\hl{TODO: #1}}}
\usepackage{graphicx}
\usepackage{amsthm}
\newtheorem{mydef}{Definition}
\usepackage{dcolumn}
\usepackage{multirow}
\usepackage{booktabs}
\newcolumntype{d}{D{.}{.}{4.0}}
\newcolumntype{s}{D{.}{.}{1.4}}

% Users of the {thebibliography} environment or BibTeX should use the
% scicite.sty package, downloadable from *Science* at
% www.sciencemag.org/about/authors/prep/TeX_help/ .
% This package should properly format in-text
% reference calls and reference-list numbers.

\usepackage{scicite}

% Use times if you have the font installed; otherwise, comment out the
% following line.

\usepackage{times}

% The preamble here sets up a lot of new/revised commands and
% environments.  It's annoying, but please do *not* try to strip these
% out into a separate .sty file (which could lead to the loss of some
% information when we convert the file to other formats).  Instead, keep
% them in the preamble of your main LaTeX source file.


% The following parameters seem to provide a reasonable page setup.

\topmargin 0.0cm
\oddsidemargin 0.2cm
\textwidth 16cm 
\textheight 21cm
\footskip 1.0cm


%The next command sets up an environment for the abstract to your paper.

\newenvironment{sciabstract}{%
\begin{quote} \bf}
{\end{quote}}


% If your reference list includes text notes as well as references,
% include the following line; otherwise, comment it out.

\renewcommand\refname{References and Notes}

% The following lines set up an environment for the last note in the
% reference list, which commonly includes acknowledgments of funding,
% help, etc.  It's intended for users of BibTeX or the {thebibliography}
% environment.  Users who are hand-coding their references at the end
% using a list environment such as {enumerate} can simply add another
% item at the end, and it will be numbered automatically.

\newcounter{lastnote}
\newenvironment{scilastnote}{%
\setcounter{lastnote}{\value{enumiv}}%
\addtocounter{lastnote}{+1}%
\begin{list}%
{\arabic{lastnote}.}
{\setlength{\leftmargin}{.22in}}
{\setlength{\labelsep}{.5em}}}
{\end{list}}


% Include your paper's title here

\title{Hysteresis in human computation:\\ how one task affects another} 


% Place the author information here.  Please hand-code the contact
% information and notecalls; do *not* use \footnote commands.  Let the
% author contact information appear immediately below the author names
% as shown.  We would also prefer that you don't change the type-size
% settings shown here.

\author
{Edward Newell \\ Derek Ruths\\
\\
\normalsize{\texttt{edward.newell@mail.mcgill.ca}}\\
\normalsize{\texttt{druths@networkdynamics.org}}\\
\normalsize{School of computer science, McGill University,}\\
\normalsize{3630 rue University, Montreal, Quebec, H3A 0C6, Canada}\\
\\
}

% Include the date command, but leave its argument blank.

\date{}



%%%%%%%%%%%%%%%%% END OF PREAMBLE %%%%%%%%%%%%%%%%



\begin{document} 

% Double-space the manuscript.

\baselineskip24pt

% Make the title.

\maketitle 



% Place your abstract within the special {sciabstract} environment.

\begin{sciabstract}
Microtask platforms have become commonplace tools for obtaining human 
annotations for large datasets.  Such platforms connect requesters 
(researchers or companies) with large populations (crowds) of workers, who 
will individually complete small information-processing jobs, each taking 
typically between one to five minutes.  
A major challenge in using such platforms, 
and a topic of ongoing research, concerns designing jobs that elicit high 
quality annotations. Here we identify a feature of nearly all crowdsourcing 
jobs that can act as a strong and systemic source of bias in annotations 
produced by workers. Specifically, many microtask jobs consist of a sequence 
of tasks which share a common format (e.g., label pictures, identify people, 
circle galaxies). Using image-labeling, a canonical microtask job format, we 
discover that earlier tasks shift the distribution of future answers by 
30-50\%, irrespective of the images used. 
Moreover, we show that prior tasks influence 
the content that workers chose to focus on, as well as the richness and 
specialization of their word choice. While these intertask effects can be a 
source of systematic bias, our results suggest that, with appropriate job 
design, it might be possible to leverage these biases to hone worker focus 
and specificity, helping to elicit reproducible, expert-level judgments.

%Faced with the ever-growing scale and complexity of data, academic and 
%industrial researchers are increasingly relying on \textit{human computation} 
%to extract useful knowledge.  Microtask
%platforms, like Amazon Mechanical Turk, act like a human compute cluster,
%making it possible to access human cognition quickly and inexpensively. 
%This has lead to their broad adoption by social and computer scientists.  
%Human computation is being used experimentally in applications
%where expert opinion would normally be sought, such as in reviewing 
%medical images.
%Here we show that the repeated performance of similar tasks, an inherent
%feature of microtask and crowdsourcing platforms, acts as a strong
%and largely unexplored source of bias.  The effects that earlier tasks exert
%on later ones are more severe than effects caused by framing, 
%such as by disclosing a task's purpose.  
%Using image-labeling, the canonical microtask, we show that prior 
%tasks influence the content that workers chose to focus on, as well as the 
%richness and specialization of their vocabulary.    
%While intertask effects can be a source of systematic bias, our
%results suggest that with appropriate task-engineering, it might be possible
%to leverage intertask effects to hone worker focus and specificity, helping
%to elicit reproducible, expert-level judgments.
\end{sciabstract}

\section*{Main text}
Microtask platforms are online marketplaces in which \textit{requesters} 
post batches of tasks, and \textit{workers} complete them
for remuneration, a sense of purpose, and for fun
\cite{kazai2013analysis,Antin20122925}.  
Typical tasks include tagging and categorizing images 
\cite{6116320,Zhai2012357}, transcribing voice recordings 
\cite{chandler2013breaking,paolacci2010running}
or handwritten notes \cite{Berinsky2012351,Finnerty2013}, and judging the 
relevancy of search results 
\cite{le2010ensuring,grady2010crowdsourcing,alonso2009can,kazai2013analysis}.

Microtask platforms enable a workflow in which workers are like 
input-output devices.  This provides much of the flexibility and 
cost-savings of fully automating the work in a computer program: 
the workforce is available on demand over the Internet using automated 
scripts, and  no contracts or interviews are required.  Many tasks can be
can be performed at a fraction of the cost of recruiting temporary workers.

For this reason, there has been a surge in demand for microtask work
in both industry and academia.  Industry employs microtask workers
for clerical tasks, such as digitizing records and correcting errors; 
and for marketing activities, such as running focus groups, writing product 
reviews, and moderating website content.  
Academia turns to microtask workers as a way to recruit study participants, 
and to annotate datasets, for example, to provide ground-truth in training 
artificial intelligence systems.

Given their increasing prevalence, there
is a desire to understand the factors affecting the reliability of 
microtask-based methods.  Researchers have investigated task-design 
parameters such as the 
level of pay \cite{kazai2013analysis}, the training and pre-screening of 
workers \cite{le2010ensuring,paolacci2010running}, and user-interface 
design \cite{Finnerty2013}.  
Researchers have also investigated the effects of \textit{framing}, 
by altering the description of the workflow context 
\cite{Kinnaird2012281}, the purpose of tasks 
\cite{chandler2013breaking}, and the problem description
\cite{thibodeau2013natural}.  

Here we focus on a ubiquitous feature of the microtask setup: the 
tendency for microtask workers to perform many tasks in quick succession. 
Even if a worker performs the minimum possible amount of work, 
she will likely have performed several microtasks, since they are typically 
distributed to workers in bundles.  For example, in one bundle, a worker 
might be assigned a set of ten images, and asked to provide descriptive words 
(or \textit{labels}) for each.  On one popular microtask platforms,
Amazon's Mechanical Turk (MTurk), these bundles are called Human 
Intelligence Tasks (HITs), but for the sake of clarity, we will use 
``task'' to mean the smallest repeatable unit of work, which, in our study, 
consists of labeling a single image.

The repetitive completion of tasks is of concern because people's responses
to a prompt can be strongly influenced by their recent exposure to stimuli,
due to the well-known phenomenon of \textit{priming} 
\cite{BJOP1796,No2007,beller1971priming}.  Priming is typically relevant when 
the prior stimuli bear some conceptual or perceptual connection to the 
ensuing task, and so, the repetition of similar tasks within the typical 
microtask setup creates an ideal context for strong priming effects
\cite{Gass1999549,sohn2001task}.

Researchers investigating a somewhat similar class of psychological phenomena,
found that ratings that microtask workers give to items in a list
depend on the order in which the list is presented, an effect known as
\textit{positional bias} \cite{lerman2014leveraging}.  This serves as an 
example of a basic psychological phenomenon impacting crowdsourced data,
and raises questions about what other cognitive biases might
affect microtask work, and how prevelant induced bias might be.  

Here we investigate the potential for task-repetition to induce bias in 
microtask-based work.  If such bias is present, it would impact virtually 
every microtask and crowdsourced workflow.
The results we present here show that a worker's response during a task
does indeed depend strongly on the tasks performed beforehand.   
We call these effects that earlier tasks exert on later ones 
\textit{intertask effects}.

As we have described, microtask platforms have been widely adopted as a 
tool, but computer scientists also hold such platforms as the object of 
research, viewing  microtask platforms as a new kind of computing 
architecture.  In analogy to CPUs (central 
processing units) and GPUs (graphics processing units), 
Davis \textit{et al} invoked the \textit{human processing unit}, or HPU \cite{5543192}.
Researchers sought to define a basic HPU instruction set, and
libraries for HPU programming are under active development
 \cite{little2010turkit,minder2011crowdlang,minder2012crowdlang,kittur2011crowdforge}.  
HPU outputs are inherently stochastic, but our revelation of strong intertask 
effects shows that HPUs are subject to \textit{hysteresis}, 
meaning that their 
outputs depend on the history of their inputs.  Reproducibility and 
path-independance are crucial properties of the components in a 
computing architecture, so it to maintaning the view of the HPU computing 
architecture, necessitates a better understanding of the computational 
ramifications of many psychological phenomena. 

We investigated intertask effects using image-labeling tasks on the Amazon 
Mechanical Turk (MTurk) microtask platform.  
In addition to being one of the most common kinds of tasks
\cite{chandler2013breaking,Berinsky2012351,Finnerty2013,paolacci2010running},
image-labeling is prototypical for any kind of task in which the worker is 
asked to provide a qualitative free-text response to a prompt.  
This includes such examples as describing
video and audio recordings, summarizing texts, or providing personal 
experiential feedback to a product review.
%in the long run, because image labeling is still very difficult to automate
%\cite{5543192}, and is used in many applications, such as website moderation, 
%computer vision research, and social media analytics (\# insert other refs) 
%\cite{5543192}.  

In our first experiment, which we will refer to as 
\textit{intertask-food-objects}, workers were assigned to either a
\textit{food} or \textit{objects} treatment.  Workers in these treatments
performed a set of five \textit{initial tasks}, which each involved 
providing five descriptive labels for an image depicting 
either food or (non-food) objects, depending on the worker's 
treatment (see Fig.~\ref{fig:task}A and B).  Following the initial 
tasks, workers performed five \textit{test tasks}, which were identical for 
both treatments.  The test tasks contained images depicting a 
mixture of food and objects (see Fig.~\ref{fig:task}C), and workers
were again asked to provide five labels.  Five initial 
tasks, together with the test tasks, comprised one HIT\footnote{
	Workers were only allowed to participate in one HIT of a single treatment 
	of a single experiment.
}. During the HIT, workers 
were presented images one at a time, and, crucially, we made no 
distinction between the initial and test tasks.

A $\chi^2$ test shows that the frequency of word usage by
workers in the \textit{food} and \textit{objects} treatments differed
significantly during test tasks ($p = 6.0 \times 10^{-14}$), even though the test tasks were identical 
in both treatments (\# see supplementary text for tabulated statistics).  This shows that earlier tasks have a 
significant effect on later ones, and raises concerns about the 
design of microtasks, and possibly the microtask methodology itself.

While this affirms the \textit{existence} of intertask effects, we wish to
understand the nature and severity of the bias induced by prior task 
exposures.  To this end, we define the extent of bias, $\theta$, in terms of
the strength of influence that a perturbation has on worker's responses.
Since different workers generally provide different responses for the same 
task, we characterize worker's responses by a probability distribution.  
The extent of bias is then defined by the difference in the response 
distribution for different treatments of workers\footnote{
	This is a general divergence metric between probability 
	distributions known as the \textit{total variation distance.}
}:
\begin{equation}
	\theta = \frac{1}{2}\sum_{x \in X} \left| p_0(x) - p_1(x) \right|,
	\label{eq:theta}
\end{equation}
where $p_i(x)$ is the probability that a worker subjected to treatment $i$
provides response $x$, from the set of possible responses $X$.
It is important to emphasize that a significant result from a $\chi^2$ test 
only allows us to conclude that $\theta$ is non-zero 
(see the supplementary text).  
Determining $\theta$ from finite samples of 
responses, using purely statistical approaches, is difficult (\#), which 
leads us to reformulate the
problem in terms of classification: given a worker's 
response, can a machine reliably infer what her prior task-exposure had 
been?  Intuitively, the greater the effect that prior tasks have on subsequent
responses, the more accurate the classifier can be.  Formally (as shown
in the supplementary text), the accuracy of such a classifier, $\eta$,
yields a lower bound on the bias:
\begin{equation}
	\theta \geq 2\eta - 1.
	\label{l1}
\end{equation}
By measuring the accuracy of a classifier, we can bound the 
bias induced by any perturbation, including both intertask effects and 
framing.  As defined, bias ranges from 0 (meaning the exposures 
do not affect the probabilities of responses), to 1 
(or 100\%) (meaning that under different exposures, workers provide completely
different sets of responses).

\begin{figure}
	\centering
	\includegraphics[scale=0.6]{figs/images.pdf}
	\caption{
		Examples of images used in
		initial tasks for the (\textbf{A}) \textit{food} and (\textbf{B}) 
		\textit{objects} treatments of \textit{intertask-food-objects};
		(\textbf{C}) test tasks for \textit{intertask-food-objects} and 
		\textit{frame-food-objects};
		initial tasks for the (\textbf{D}) \textit{food} and (\textbf{E}) 
		\textit{culture} treatments of \textit{intertask-food-culture};
		and (\textbf{F}) test tasks for \textit{intertask-food-culture} and 
		\textit{frame-food-culture}.
		The full set of experimental materials is shown in the 
		supplementary text.
	}

	\label{fig:task}
\end{figure}

%\setlength{\tabcolsep}{2pt}
%\begin{table}[t]
%\centering
%\begin{tabular}{ c c c c c }
%		\hline \noalign{\smallskip}
%		\multicolumn{3}{c}{\textbf{Treatment name}} & \parbox[c]{1.4cm}{\centering \textbf{Frame}} & \parbox[c]{1.3cm}{\centering \textbf{Initial\\ tasks}}	\\ 
%
%		\noalign{\smallskip} \hline \noalign{\smallskip}
%
%		\multirow{6}{*}{ \parbox[c]{0.8cm}{ \phantom{XX} exp1}} 
%			& \multirow{2}{*}{task1} & food & none & food\\
%			& & obj & none & objects\\
%
%			\noalign{\smallskip} \cline{2-5} \noalign{\smallskip}
%			& \multirow{2}{*}{frame1} & food & food & none\\
%			& & obj & objects & none\\
%
%			\noalign{\smallskip} \cline{2-5} \noalign{\smallskip}
%			& \multirow{2}{*}{echo1} & food & food & none\\
%			& & obj & objects & none\\
%
%		\noalign{\smallskip} \hline \noalign{\smallskip}
%
%		\multirow{4}{*}{\parbox[c]{0.8cm}{exp2}} 
%			& \multirow{2}{*}{task2}  &  food & none & food\\
%			& 	&  cult & none & culture\\
%			\noalign{\smallskip} \cline{2-5} \noalign{\smallskip}
%			& \multirow{2}{*}{frame2} & food & food & food\\
%			& 	& cult & culture & food\\
%
%		\noalign{\smallskip} \hline  
%	\end{tabular}
%
%	\caption{ 
%		We use the treatment names shown to refer to specific sections of 
%		the experimental data.
%	}
%	\label{table:treatments}
%\end{table}

Using a na\"ive Bayes classifier\footnote{
	We discuss our choice of classifier and preprocessing methods, 
	and show similar results obtained by using a support vector 
	machine, in the supplementary material.
}, we found that intertask effects 
lead to a 30\% bias between workers from the \textit{food} and 
\textit{objects} treatments.  This represents a substantial potential to 
distort microtask data.

There has been considerable interest in the effects that framing can have
on microtask responses
\cite{Kinnaird2012281,chandler2013breaking,thibodeau2013natural}, so,
as a point of comparison, we performed a similar experiment 
in which we framed the purpose of the tasks.  In the experiment 
\textit{frame-food-objects}, workers were either told that 
the tasks were ``Funded by the laboratory for the visual perception of Food 
and Ingredients'', or ``\ldots of Objects and Tools''.  
Framing did induce changes in the frequencies of 
word usage at significance (as determined by $\chi^2$ test; $p=0.0012$), but 
the \textit{extent} of framing-induced bias we found was statistically 
insignificant ($p =0.37$) (see Fig.~\ref{fig:theta}).  This suggests 
that prior task exposure is a stronger priming modality than framing.

We conducted variants of these experiments using different images, to see 
whether this trend was robust.  In the experiment 
\textit{intertask-food-culture},
workers were either assigned to a \textit{food} or \textit{culture} treatment.
As before, the initial tasks for the \textit{food} treatment contained images 
depicting food (see Fig.~\ref{fig:task}D). 
In the \textit{culture} treatment, 
the initial tasks contained images depicting cultural scenes 
(of dance, sport, or music) (see Fig.~\ref{fig:task}E).  The test tasks, which were identical for both
treatments, included images depicting meals of identifiable cultural 
origin\footnote{
	Though we attempted to choose more globalized foods, the initial tasks 
	in \textit{intertask-food-culture} are arguably still of
	identifiable cultural origin. This would tend to reduce, rather than 
	inflate, the bias observed in our results 
	(we discuss this further in the supplementary text).
}
(see Fig.~\ref{fig:task}F).  

Results for this experiment again showed a 
strong bias as a result of intertask effects ($\theta$~about 50\%) 
(see Fig.~\ref{fig:theta}A).  Again, we compared intertask effects to 
framing, by performing an experiment (\textit{frame-food-culture}) using 
the same test tasks as in \textit{intertask-food-culture}, in which we exposed
workers to a statement framing the purpose of the tasks as the recognition 
of either food or culture.  
For this framing experiment, we did not observe a statistically significant 
difference between the labels attributed by the \textit{food} and 
\textit{culture} treatments during test tasks ($p=0.29$) 
(see Fig.~\ref{fig:theta}).

It was only when we combined the use of framing, with an active 
reiteration step, that bias reached a level comparable to that achieved 
by intertask effects.  In the experiment \textit{echo-food-objects},
after framing the purpose of the task around either the recognition of food
or objects, we required the worker to echo back the purpose of the task
using a combo-box input.  In this case, workers exposed to different 
``echoed framing'' showed a bias of about 35\% in the labels provided during 
test tasks
(see Fig.~\ref{fig:theta}A). However, it is possible that by requiring 
workers to echo the language used in framing, we provided an implicit 
but nonetheless strong signal of our intent that workers act on the 
framing statement.  In this sense, \textit{echo-food-culture} may reflect the 
influence of an \textit{instruction} rather than framing.

\begin{figure}
	\centering
	\includegraphics[scale=1]{figs/theta.pdf}
	\caption{
		Empirical bias, $\theta_\mathrm{NB}$, measured using a na\"ive Bayes 
		classifier, induced in image labeling tasks,
		(\textbf{A}) by exposing workers to initial tasks or framing. 
		As workers proceed through test tasks, the effects of initial tasks 
		wane, as shown (\textbf{B}) for \textit{intertask-food-objects}, but 
		remain significant even after five tasks.  Standard error bars are 
		shown.
	}
	\label{fig:theta}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[scale=0.94]{figs/vocab_specificity.pdf}
	\caption{
		(\textbf{A}) Exposing workers to initial tasks and framing 
		involving food changed how often they referred to food 
		during test tasks, but did not always increase it (in all three plots,
		positive values indicate a larger quantity for food-exposed workers).
		(\textbf{B}) The number of unique food-related
		words (richness) was greater for food-exposed workers, except in the 
		case of \textit{frame-food-culture} (stars indicate the threshold
		for a significant deviation from the null hypothesis, $\alpha=0.05$). 
		(\textbf{C}) Food-exposed
		workers also used more specialized words to refer to food.
		Standard error bars are shown in (\textbf{A} and \textbf{C}).
	}
	\label{fig:specificity}
\end{figure}

To better understand the nature of 
intertask effects, we investigated the vocabulary
that workers used to label test tasks. A natural expectation is that, during
the test tasks within
a given experiment, the food-primed workers would use food-related words 
more often than their non-food-primed counterparts.  However, this was not 
generally 
the case. In the \textit{intertask-food-culture} experiment, food-primed
workers actually used significantly \textit{fewer} food-related words 
during test tasks\footnote{
	food-related words were identified using the wordnet corpus, 
	augmented with additional words obtained by crawling a recipe website 
	(see supplementary information for details)
} (see Fig.~\ref{fig:specificity}).  This finding
rules out a seemingly-simple idea that workers emphasize
content that has been present in earlier tasks: seeing content in prior tasks
affects the probability that a worker refers to it in subsequent tasks, but 
does not necessarily \textit{increase} the probability.

To deepen our understanding, we investigated workers' lexical richness in 
reference to food, that is, the number of \textit{unique} food-related words
used.  Even if workers provide an abundance of food-related words, there
can be less diversity if the same words are often repeated.  
This could happen, for example, if workers use generic references 
to food.
Both \textit{intertask} experiments showed that food-exposed workers had 
greater lexical richness, in reference to food, than their non-food-primed 
counterparts (as much as 20\% more) (see Fig.~\ref{fig:specificity}).  
The increase in lexical richness in the case of 
\textit{intertask-food-culture} is particularly noteworthy, because in that 
experiment, food-primed workers made fewer total references to food.  
We also observed enrichment of the food-related lexicon in the 
\textit{priming-food-objects}, and \textit{echo-food-objects} experiments, 
although to a lesser extent.

The observations regarding lexical richness lead us to suspect that 
initial tasks might influence workers to use more refined or specialized 
words, when referring to aspects of content that had been present in the 
initial tasks (i.e. food).  This would help explain why, in the case of 
\textit{intertask-food-culture}, we observed a significantly greater 
\textit{diversity} of food-related words despite their significantly lesser 
\textit{total amount}.
To test whether exposure to food in initial tasks increased the specialization
of food-references, we used the wordnet corpus to operationalize the 
notion of word specialization.  Wordnet provides a set of hypernym-hyponym 
relations between 117,798 English nouns (\#).  Hypernyms are generalizations 
(for example, ``bread'' is a hypernym for 
``pumpernickel''), while hyponyms are specializations.
For each experiment, we compared food-related words provided by workers from 
one treatment to those from the other, and tallied the excess number of cases
where one treatment's words were more specialized than the other, as a 
percentage of the total 
(see Fig.~\ref{fig:specificity}B) 
(details for this calculation are in the supplementary text). 

In all experiments except \textit{frame-food-culture}, food-primed workers 
used significantly more specialized words than their non-food-primed 
counterparts (about 15\% more) (see Fig.~\ref{fig:specificity}C).
It is interesting that such substantial increases in both the lexical 
richness and specialization of food-related words 
held for \textit{intertask-food-culture}, where, as mentioned, we observed 
that food-exposed workers made \textit{fewer} references to food overall. 
These observations point 
to countervailing factors: one factor tending to activate the more 
specialized and less common food-related words 
(yielding greater lexical richness and specialization), and the other tending 
to suppress certain, presumably more common and generic words 
(yielding fewer food-related words in total).

This hypothesis is corroborated when we look at those words whose 
frequencies changed the most from one treatment to another (see Table S1).  
The word ``food'', which is the most generic possible food-related word, was 
always \textit{suppressed} among food-primed workers.  In fact, 
for all experiments, ``food'' was the \textit{most suppressed} word.

Taken together, our results can be 
explained through a combination of positive and negative priming effects.
Positive priming (usually simply ``priming'') occurs when a prior stimulus 
predisposes a person to give certain responses in an ensuing task, and
is often observed as an increase in the speed or accuracy of a response, or
the ability to recognize features of a briefer, quieter, or noisier stimulus 
(\#).
Negative priming occurs when, after being repeatedly exposed to a stimulus 
considered to be non-salient, a person begins to ignore that 
stimulus (\#).  

Workers exposed to images containing food will be (positively) primed, 
activating memories, concepts, and vocabulary related to food.  
However, as the worker labels successive images containing food, the basic 
fact that an image contains food will not appear to be salient, since it 
does not serve to distinguish one image from another.  Thus, the most 
generic references to that fact, such as the label ``food'', 
will be suppressed, while more specialized references will be elicited.  
Meanwhile, the number of references to food overall might increase or 
decrease, depending on the balance of these factors.  

More generally, we are suggesting that, even though workers are not 
instructed to compare tasks in any way, prior tasks form a 
context relative to which workers judge salience.  Thus, due to a combination 
of negative and positive priming, a worker's focus 
in repeated tasks tends to be directed away from generic, shared features, 
toward specific and distinguishing ones.

Prior to this study, little thought appears to have been given to the 
ordering and bundling of microtasks.  But our findings show that these are 
powerful design factors.  A natural response might be simply to randomize 
task ordering, a practice that is commonly employed.  But the sheer extent of 
bias suggests that this will still introduce a significant amount of noise
into the results.  Even chains of two or three similar tasks, which
will not be reliably eliminated in random permutations, could lead
to the levels of bias we observed in our experiments.
It would be preferable to understand intertask
effects in greater depth so that they might be properly \textit{controlled}.

Our findings suggest that, through careful task engineering,
it might be possible to achieve greater quality and reproducibility
of responses.  
A consistent goal in human computation is the achievement of expert-level
judgments from non-expert workers (\#).  This has been achieved in some
applications (\#). The distinction between experts and novices can be 
attributed in part to the possession of specialized knowledge and heuristics. 
But to a large degree, this distinction lies in the ability of 
experts to direct their focus toward salient features, while filtering out 
non-salient ones \cite{kellman2009perceptual}.  Using strategic task 
exposures, it might be possible to guide workers' focus and salience 
attribution, enabling expert-level judgment in a wider variety of 
crowdsourced applications.
We anticipate that future work will yield techniques to control intertask 
effects, enabling one to both reduce unwanted bias, as well as tune the 
focus, diversity, and specificity of worker responses.

Here we have shown that a large bias can be introduced into microtask 
responses, simply from the performance of earlier tasks. 
These effects are much stronger than framing.  
While our findings raise serious concerns about the 
current state of microtask design, they also reveal potential opportunities 
for more 
refined control over worker focus and acuity.  Any application that relies
on the repetitive use of human judgment is likely subject to the phenomena
we described here.  Exploring the issues and opportunities posed by 
intertask effects are important directions for future work, 
in the effort to develop performant and reliable methods for human 
computation.
\bibliography{newbib}
\bibliographystyle{Science}


\section*{Supplementary Material}
\renewcommand{\figurename}{Figure S\!\!}
\renewcommand{\tablename}{Table S\!\!}
\setcounter{figure}{0}
\setcounter{table}{0}

\subsection*{Materials and Methods}

\paragraph{Microtask setup.}
Altogether the experiments presented here consisted of data from 3200 HITs 
(bundles of 10-image labeling tasks).  In all experiments, each treatment
consisted of results from at least 119 workers.  
For the purpose of statistical calculations,
treatments having more than 119 workers were subsampled, producing a uniform
sample size accross all treatments.  The \textit{food} and \textit{objects}
treatments in the \textit{intertask-food-objects} experiment were further
broken down into sub-treatments (each containing at least 119 workers),
corresponding to different orderings of the test tasks.  These alternative
orderings were obtained by rotation of the test tasks 
(i.e. moving all tasks forward by one position, and making the last task 
become the first test task).  This ensured that each test task appeared in
each of the five test-task positions.

The HITs (as defined in the main text) for all treatments were presented as 
a series of slides.  The first 
slide consisted of a brief set of instructions.  In framing-treatments, a 
framing slide was shown immediately after the instructions, see 
Fig. S\ref{fig:hit_preamble}.  In intertask-treatments, a set of initial
tasks were presented (and no framing slide).  Following the
framing slide or initial tasks, the test-tasks were presented.

All initial and test tasks had the format shown in 
Fig.~S\ref{fig:hit_preamble}: an image taking up most of
the task pane, with five text inputs and a ``next'' button.  Images were
presented in this way, one at a time. 
For treatments having initial tasks, the test
tasks followed seemlessly, and no distinction was made between the test 
tasks and the initial tasks.

In an attempt to increase the strength of framing, we included the
\textit{echo-food-objects} treatment, in which, during the framing slide,
the worker was required to echo the frame using a combo-box input.  The
instructions, both kinds of framing slide (simple and echoed), 
and an example task slide, are shown 
in Fig.~S\ref{fig:hit_preamble}.

Given the setup described above, the only tasks that workers in framing 
treatments perform are the test tasks, whereas in the 
intertask-treatments, the test tasks were performed as tasks
5 through 10.  
It could be argued that a better comparison would be achieved
by having all workers perform some initial tasks, and simply ensuring that
the initial tasks are \textit{not} varied between framing treatments. 
We took this approach in the experiment 
\textit{frame-food-culture}, in which workers performed the same 
initial tasks containing images that depicted food in both the \textit{food}
and \textit{culture} treatments.  If anything, this appears
to have reduced the strength of framing for that experiment.

\begin{figure}
	\includegraphics[scale=0.8]{figs/tasks.pdf}
	\caption{Examples of A) instructions; B) a frame, as shown in 
		\textit{frame1:obj}, and similar to those shown in 
		\textit{frame1:food} and \textit{frame2:food}, and 
		\textit{frame2:cult};  
		C) an echoed frame, as used in \textit{echo1:obj}, and similar to that
		used in \textit{echo1:food}; and D) an 
		example of an image-labeling task.
	}
	\label{fig:hit_preamble}
\end{figure}

\paragraph{Selection of images.} 


The images used in the initial tasks for \textit{intertask-food-objects}  
are shown in Figs.~S\ref{fig:task1:food} (\textit{food} treatment) and 
S\ref{fig:task1:obj} (\textit{objects} treatment).
We selected images for the initial tasks that show either food or objects, 
according to the treatment in which they were used.  
We took care to exclude objects from
the food-images (except in some cases a dish supporting the food), and 
to exclude food from the object-images.  The images used for the test tasks
of \textit{intertask-food-objects} and \textit{frame-food-objects} are shown 
in Fig.~S\ref{fig:task1:test}.

The images used in the initial tasks
for \textit{intertask-food-culture} are shown in Figs.~S\ref{fig:task2:food} 
(\textit{food} treatment) and S\ref{fig:task2:cult} 
(\textit{culture} treatment).
We selected the initial images for the treatments of this experiment to 
respectively reflect depict food and culture.

Naturally, since food is a very
important aspect of culture, pictorial depictions of the two concepts 
cannot be cleanly separated,
and images depicting food inevitably also depict culture.
The fact that the inital-task images for the food treatment of 
\textit{intertask-food-culture} do also depict culture
will only tend to make them more similar to those from the \textit{culture}
treatment, which would tend to \textit{reduce} the severity of intertask 
effects.  In any case, since we still found strong intertask effects, 
the presence of culture in the food-related initial images of 
\textit{intertask-food-culture} does not seem to have been problematic.
The images used in the test tasks of \textit{intertask-food-culture} and
\textit{frame-food-culture} are shown in Fig.~S\ref{fig:task2:test}.

\begin{figure}
	\includegraphics{figs/task1-food.pdf}
	\caption{
		Images used in the initial tasks for the
		\textit{food} treatment of \textit{intertask-food-objects}.  
		The numbers show the order in which the 
		images were presented to workers.
	}
	\label{fig:task1:food}
\end{figure}

\begin{figure}
	\includegraphics{figs/task1-obj.pdf}
	\caption{
		Images used in the initial tasks for the
		\textit{objects} treatment of \textit{intertask-food-objects}.  
		The numbers show the order in which the images were presented to 
		workers.
	}
	\label{fig:task1:obj}
\end{figure}

\begin{figure}
	\includegraphics{figs/task1-test.pdf}
	\caption{
		Images used in the test tasks for \textit{intertask-food-objects}
		and \textit{frame-food-objects}.  
		The numbers show the order in which the 
		images were presented to workers in \textit{frame-food-objects};
		in \textit{intertask-food-objects}, five different orderings were
		used, which can be obtained by taking the numbers shown, $n$,
		and replacing them by $n + c \bmod 5$ for $c$ running from 0 to 4.
	}
	\label{fig:task1:test}
\end{figure}

\begin{figure}
	\includegraphics{figs/task2-food.pdf}
	\caption{
		Images used in the initial tasks for the
		\textit{food} treatment of \textit{intertask-food-culture}.  
		The numbers show the order in which the 
		images were presented to workers.
	}
	\label{fig:task2:food}
\end{figure}

\begin{figure}
	\includegraphics{figs/task2-cult.pdf}
	\caption{
		Images used in the initial tasks for the
		\textit{culture} treatment of \textit{intertask-food-culture}.  
		The numbers show the order in which the 
		images were presented to workers.
	}
	\label{fig:task2:cult}
\end{figure}

\begin{figure}
	\includegraphics{figs/task2-test.pdf}
	\caption{
		Images used in the test tasks for \textit{intertask-food-culture} 
		and \textit{frame-food-culture}.  
		The numbers show the order in which the 
		images were presented to workers.
	}
	\label{fig:task2:test}
\end{figure}

\subsection*{Testing for intertask and framing effects using $\chi^2$}
To test whether intertask and framing effects induced statistically 
significant changes in word frequencies, we assembled a contingency table
for each experiment, containing the frequencies of words in each treatment.  
Any words that appeared less than ten times for
an experiment (i.e. appearing less than five times averaged over each 
treatment)
were lumped together into a separate `OTHER' category, to ensure validity
of the test.  The degrees of freedom, test statistic, and $p$-values were
then calculated according to Pearson's $\chi^2$ test, using 
Yate's correction.  Results are tabulated in Table~S\ref{table:chi2}.
With the exception of \textit{frame-food-culture}, all experiments showed s
ignificant changes in word-frequencies in response
to the task or framing exposures, at $\alpha=0.05$, leading us to 
reject the null hypothesis that the priming exposures had no effect.

The use of Pearson's $\chi^2$ test has been criticized in linguistic 
applications, due to the finding that many corpuses are not themselves
homogeneous \cite{kilgarriff1996comparing}.  
This results from the fact that corpuses are made of many
distinct texts, together with the fact that words tend to come in bursts.
When a rare word is encountered in a given text, it is much more likely
to be encountered again in the same text, which making the word's frequency
within the text far from the average.  As a result,
if a corpus is randomly split, putting half of the texts in one set, 
and half in another, there can be a significant non-homogeneity between the 
two halves, when tested using Pearson's $\chi^2$ test.  Thus, it has 
been argued that finding two
corpuses to be different according to a $\chi^2$ test might simply be an 
artifact of the corpus' own inhomogeneity.

This concern does not apply to our case, because the ``texts'' are 
individual responses from workers, which will not, in general, be subject to 
the burstiness phenomenon.  Nevertheless, to test this, 
we randomly divided the workers 
from each experimental treatment into two sets, and tested the homogeneity 
between these sets.  Results are tabulated in Table~S\ref{table:chi2_within}, 
and show that treatments 
\textit{are} homogeneous.  Thus, the lack of homogeneity \textit{between} 
the treatments of given experiments can be attributed to the framing / 
intertask effects.

\begin{table}
\centering
\begin{tabular}{c c c c}
\toprule
Experiment & Degrees of freedom & $\chi^2$ & $p$-value\\
\toprule
\textit{intertask-food-objects} & 117 & 268.7 & $6.0 \times 10^{-14}$\\
\textit{frame-food-objects} & 116 & 167.8 & $1.2 \times 10^{-3}$\\
\textit{echo-food-objects} & 126 & 381.3 & $1.7 \times 10^{-27}$\\
\textit{intertask-food-culture} & 120 & 447.5 & $2.8 \times 10^{-39}$\\
\textit{frame-food-culture} & 119 & 127.2 & $0.29$\\
\bottomrule
\end{tabular}
\caption{
	Results for Pearson's $\chi^2$ test of homogeneity of the word-frequencies
	between treatments from each experiment.  This shows that the treatments
	significantly affected word usage, 
	except in the case of \textit{frame-food-culture}.
}
\label{table:chi2}
\end{table}

\begin{table}
\centering
\begin{tabular}{c c c c c}
\toprule
Experiment & Treatment & Degrees of freedom & $\chi^2$ & $p$-value\\
\toprule
\noalign{\smallskip}
\multirow{2}{*}{\textit{intertask-food-objects}} & \textit{food} & 65 & 39.0 & $1.0$\\
 & \textit{objects} & 65 & 53.2 & $0.85$\\
\noalign{\smallskip}
\noalign{\smallskip}
\multirow{2}{*}{\textit{frame-food-objects}} & \textit{food} & 67 & 41.3 & $0.99$\\
 & \textit{objects} & 62 & 66.9 & $0.31$\\
\noalign{\smallskip}
\noalign{\smallskip}
\multirow{2}{*}{\textit{echo-food-objects}} & \textit{food} & 66 & 40.9 & $0.99$\\
 & \textit{objects} & 66 & 35.6 & $1.0$\\
\noalign{\smallskip}
\noalign{\smallskip}
\multirow{2}{*}{\textit{intertask-food-culture}} & \textit{food} & 62 & 65.9 & $0.34$\\
 & \textit{culture} & 68 & 57.2 & $0.82$\\
\noalign{\smallskip}
\noalign{\smallskip}
\multirow{2}{*}{\textit{frame-food-culture}} & \textit{food} & 57 & 49.6 & $0.75$\\
 & \textit{culture} & 64 & 55.1 & $0.78$\\
\noalign{\smallskip}
\noalign{\smallskip}
\bottomrule
\end{tabular}
\caption{
	Results for Pearson's $\chi^2$ test of homogeneity of the word-frequencies
	within treatments, which were randomly partitioned for the test.  This
	shows that word-usage within treatments is homogeneous.
}
\label{table:chi2_within}
\end{table}

\subsection*{Measuring intertask and framing effects.}

\paragraph{Comparing $\theta$ and $\chi^2$.}
The following is an illustrative example to show why it is important to 
have a measure of effect size, like $\theta$, in addition to effect 
significance, like $\chi^2$.
Suppose that we have a very slightly unfairly weighted coin, which turns
up heads ($x=\mathbf{H}$) more often than expected by one part in one hundred:
$\Pr\{x=\mathbf{H}\} = 0.501$. Such a coin would be fine for many 
practical purposes, like deciding who buys lunch.  We could
perform a $\chi^2$ test to determine if the coin is biased, by flipping 
it 100 million times, while doing the same with a coin we know is fair.
Simulating this with a random number generator yields the results shown
in Table~S\ref{table:coin}, and results from a $\chi^2$ test of the 
hypothesis that the two coins have the same probability of turning up heads 
is shown in Table~S\ref{table:coin_stats}.

\setlength{\floatsep}{30pt plus 1.0pt minus 2.0pt}

\begin{table}
\centering
\begin{tabular}{c c c c}
\toprule
coin & $N_\mathbf{H}$ & $N_{\mathbf{T}}$ & total \\
\toprule
\textit{fair} & 50002283 & 49997717 & $10^8$\\
\textit{biased} & 50101115 & 49898885 & $10^8$ \\
\bottomrule
total & 100103398 & 99896602 & $2\times 10^8$ \\
\bottomrule
\end{tabular}
\caption{
	Simulated counts of heads ($N_\mathbf{H}$) and tails ($N_\mathbf{T}$)
	occurences, for a fair coin, and a coin biased to have a probability
	of turning up heads of 0.501.
}
\label{table:coin}
\end{table}


\begin{table}
\centering
	\begin{tabular}{c c c c }
	\toprule
	Degrees of freedom & $\chi^2$ & $p$-value \\
	\toprule
	1 & 195.4 & $2.2 \times 10^{-44}$ \\
	\bottomrule
	\end{tabular}
\caption{
	Statistics for a $\chi^2$ test of the hypothesis that a fair and 
	unfair coin have the same probability of turning up heads.  
	The result overwhelmingly favors rejection of the hypothesis, even
	though the coins differ only slightly.
}
\label{table:coin_stats}
\end{table}

Even though the $\chi^2$ test shows, with tremendous statistical 
significance, that the unfair coin is indeed unfair, the actual extent
of bias is small, 
$\theta = \frac{1}{2}\left( |0.501 - 0.499| + |0.499 - 0.501| \right) = 0.002$.
This demonstrates how a sufficiently large sample size will lead to the 
detection an arbitrarily 
small bias with arbitrarily high statistical significance. 
This is why, to measure the practical significance of intertask effects,
we rely on a measure of effect size, which is in this case a measure
of statistical divergence, $\theta$.

\paragraph{Statistical approaches to measuring bias.}
Recall that in Eq.~\ref{eq:theta} in the main text, we defined the
extent of bias that exists between two groups of workers to be $\theta$:
$$
	\theta = \frac{1}{2}\sum_{x \in X} \left| p_0(x) - p_1(x) \right|,
$$
where $p_0(x)$ and $p_1(x)$ represent the probability mass functions
for responses from two differently treated populations of workers.  
Let us denote these two populations by $P_0$ and $P_1$.
As mentioned, this is a measure of the divergence between $p_0$ and $p_1$ 
which is called either the \textit{total variation} or the 
\textit{L1-distance}.

Measuring the \textit{L1-distance} between two probability distributions,
given only samples, is difficult.  The na\"ive approach is to take
the maximum likelihood estimate of $p(x)$, which sets it equal to the 
frequency with which the response $x$ has been observed, 
$\hat{p}(x) = \frac{N_x}{N}$, and then estimate $\theta$ 
using these frequencies: 
$\hat{\theta} = \sum_{x \in X}|\hat{p}_1(x) - \hat{p}_2(x)|$.  
However, this method tends to drastically overestimate $\theta$.
Recently progress has been made on this problem, and two algorithms have 
been proposed to estimate the L1-distance between distributions
given only samples.  \td{cite these} But these methods provide only asymptotic guarantees
which cannot be directly applied to our case.  However, machine learning
approaches provide a general alternative that we describe in the next 
section.

\paragraph{The relationship of bias and classifier accuracy.}
We will now show that, any classifier algorithm, $\mathcal{A}$, that
takes the response, $x$, of a worker, and guesses the population to which
the worker belongs ($P_0$ or $P_1$), will do so with an accuracy, $\eta$,
that is bounded according to Eq.~\ref{l1}, which is reproduced here:
$$
	\theta \geq 2\eta - 1.
$$

To do so, we will first formalize $\eta$ by defining a 
\textit{validation test}.  In a validation test, a uniform random bit 
$z\in\{0,1\}$ is sampled, and then, according to the value of $z$, the
response of a worker, $x$, is sampled uniformly randomly from $P_z$.  
The algorithm is provided
with $x$ and yields a guess, $b=\mathcal{A}(x)$, as to whether the worker was 
from $P_0$ or $P_1$.  We shall denote such a validation test by 
$V(P_0, P_1, \mathcal{A})$, and define the value of the test to be 1 if 
$b=z$ and zero otherwise.  

We then define the accuracy of classifier $\mathcal{A}$ to be $\eta$ according
to:
$$
\eta = \mathrm{E\{V(P_0, P_1, \mathcal{A})\}} 
	= \mathrm{Pr}\{V(P_0, P_1, \mathcal{A})=1\}.
$$

To develop the relationship between $\eta$ and $\theta$, let us consider
the accuracy $\eta^*$ of the best possible classifier $\mathcal{A}^*$.
The best possible classifier will not in general have perfect accuracy:
if the responses from workers in $P_0$ and $P_1$ are distributed 
identically, then there is no hope of determining which population the worker 
came from given only her response $x$.  But, assuming that responses are
not distributed identically, then given some $x$, the best possible 
classifier must always guess 1 whenever $p_1(x) > p_0(x)$ and otherwise 
guess 0.

Sacrificing, for a moment, some generality, let us assume that, for a given 
$x$, $p_1(x) > p_0(x)$.  
Then, given some $x'$ has been chosen during a 
validation test, the optimal classifier must guess that 
$z = \arg\!\max_{z'}(p_{z'}(x)) = 1$.
The probability that the classifier guesses correctly, given a validation
test in which the resonse $x$ was chosen, is therefore:
\begin{align}
	\mathrm{Pr}\{V = 1 | x' = x \} 
		&= \mathrm{Pr}\{\mathcal{A^*}(x) = z | x' = x \} \\
		&= \mathrm{Pr}\{z = 1 | x' = x \}  \\
		&= \frac{\mathrm{Pr}\{z = 1 , x' = x\}}
			{ \mathrm{Pr}\{z=0 , x'=x\} + \mathrm{Pr}\{z=1 , x'=x\}} \\
		&= \frac{p_1(x)}{p_0(x) + p_1(x)}.
\end{align}
And now, with full generally,
\begin{align}
	\mathrm{Pr}\{V=1 | x' = x \} 
		&= \mathrm{Pr}\{\mathcal{A^*}(x) = z | x' = x \} \\
		&= \mathrm{Pr}\{z = \arg\!\max_{z'}\big(p_{z'}(x)\big)| x' = x \}  \\
		&= \frac{\max\big( p_0(x),p_1(x) \big)}
		{ p_0(x) + p_1(x) }.
\end{align}
We can rewrite:
\begin{align}
	\max\big(p_0(x),p_1(x)\big) = \frac{1}{2}
		\big(
			p_0(x) + p_1(x) + |p_0(x) - p_1(x)|
		\big),
\end{align}
so,
\begin{align}
	\mathrm{Pr}\{V = 1 | x' = x \} 
	&= \frac{1}{2} + \frac{|p_0(x) - p_1(x)|}{2 \big(p_0(x) + p_1(x) \big)}.
\end{align}
Then the overall accuracy that probability that $\mathcal{A^*}$ guesses correctly in 
any validation test is:
\begin{align}
	\eta^* &= \mathrm{Pr}\{V=1\} \\
		&=\sum_{x\in X} \mathrm{Pr}\{V = 1 | x' = x \} \mathrm{Pr}\{x' = x\}\\
		&= \sum_{x\in X} \left(
				\frac{1}{2} + 
				\frac{|p_0(x) - p_1(x)|}{2 \big(p_0(x) + p_1(x) \big)}
			\right)
			\left(
				\frac{p_0(x) + p_1(x)}{2}
			\right)\\
		&= \frac{1}{2}\left(
				\frac{1}{2}(
					\sum_{x \in X}p_0(x) + \sum_{x \in X}p_1(x)
				) + 
				\frac{1}{2} \sum_{x \in X} \big(|p_0(x) + p_1(x)|\big)
			\right) \\
		&= \frac{1}{2} \left( \frac{1}{2}(1 + 1) + \theta \right)\\
		&= \frac{1 + \theta}{2}\\
		\implies \theta &= 2\eta^* - 1.
\end{align}
By definition, no classifier can be more accurate than $\mathcal{A^*}$, so
for every $\mathcal{A}$
\begin{align}
		\theta \geq 2\eta - 1.
\end{align}

\paragraph{Using cross-validation accuracy to lower-bound bias.}
	In order for Eq.~\ref{l1} to be applicable, it is necessary to obtain
	an unbiased estimate of a classifier's accuracy.  One well-known
	pitfall in measuring classifier accuracy is to underestimate it due to 
	over-fitting to the data used.  As an example, if one were to try 
	a large number of classifiers, eventually one would find one that
	classifies the data perfectly, even though it might non generalize to
	new data.

	It is customary to use part of a data set to optimize hyperparameters of
	a model, and then test the model on as-yet unseen data.  In our case,
	don't have enough samples in each treatment to do this.  Instead, for 
	this reason, we make principled decisions for all of the options used
	in our classifier, and then test it's accuracy using cross validation,
	with no optimization performed on the data.  It was crucial for 
	the statistical validity of our approach that we test our classifier on 
	each datum only once, and report these results without any optimizations
	of the classifier.

	In performing the classification we had various options, and since we
	could not optimize these choices, we made principled decisions based on
	prior knowledge from text classification applications.  We describe the
	options we faced and the choices we made below.

	\textit{Spelling correction.}  Because we used free-text
	entry for the labels, there was bound to be many spelling mistakes.
	This would tend to reduce the accuracy of a classifier, because the 
	entries ``bread'' and ``braed'' would be regarded as completely different
	features, even though they clearly correspond to the same 
	\textit{intended word}.  While no algorithm can determine a worker's 
	intentions, it is possible to check words against a corpus, and, in the
	case of a misspelled word, look for plausible corrections based on 
	edit distance.

	\textit{Lemmatization.}  In some cases it is helpful to simplify words 
	before providing them to the classifier.  For example, the words 
	``berries'' and ``berry'' only differ in number, but would normally be
	treated as completely different features.  Lemmatization ensures that
	nouns are in their singular form, which helps to eliminate variations in
	labels that increase sparsity without conveying discriminative 
	information.

	\textit{Removal of stop-words.}  Certain very common words such as `the',
	``to'', and ``it'' are extremely common but don't help discriminate a 
	worker's exposure.  Removing these words is a common practice because
	they otherwise introduce noise into the classifier learning process.

	\textit{Splitting multiple words.}  Although workers were 
	instructed to put one word per text input, many put multiple words.
	Theoretically the original multi-word entry might bear additional 
	information that could be useful in discerning a worker's exposure. 
	In practice, it would be necessary for that word combination to recur,
	which is unlikely.  Therefore we chose to split multi-word entries,
	using the individual words as features.  This meant that the outputs of
	some workers generated more features than those of others.

	\textit{Preserving the location of labels.}  When workers completed image
	labeling tasks, they had to fill five text inputs, which were stacked
	vertically.  Presumably, they began with the top input and proceeded 
	downwards.  It seems likely that exposure effects might influence the
	\textit{order} in which workers apply labels, so when processing the 
	labels we prepended an integer to indicate into which text input the 
	label been input.  This meant that the classifier could distinguish
	``bread'' entered into the first text input from ``bread'' entered into
	the second.  A drawback is that this increases sparsity, preventing the
	classifier from accounting for the fact that different though they are,
	these entries should be treated similarly.  For this reason, we also
	provided features to the classifier which did not encode the particular
	input that had been used.  Thus, two workers that entered ``bread''
	into different inputs would generate both a feature in common indicating
	the use of the word ``bread'', as well as distinct features, indicating
	entry of the word ``bread'' \textit{in a particular input}.

	\textit{Preserving the connection between tasks and labels.}  In many
	cases a particular label would appear in many tasks.  For example, the
	word ``food'' appears in all test tasks.  It stands to reason that
	exposure effects play out differently for different tasks, and that
	the attribution of a particular label during one task provides different
	information from the attribution of the same label during a different 
	task.  So, as we did in the case of input location, we prepended labels
	with the identity of the task in which they were added, so that the 
	two otherwise identical labels added for different tasks were treated
	by the classifier as unique features.  Unlike in the case of input 
	position attribution, we didn't include raw labels unadorned with 
	the task number.  Our rationale for this is that the task defines the 
	context in which a label is applied, and so the elicitation of a label 
	during one task represents a distinct output from the elicitation of the 
	same label during a different task.  

\paragraph{Selection of the na\"ive Bayes classifier.}
A great deal of work has been done in the application of classifiers to
text documents, which provides the background for our choice of classifier.
The Support Vector Machine (SVM) approach is known to perform well, better in 
general than na\"ive Bayes.  However, three considerations drove us to use
a na\"ive Bayes classifier in favor of SVM.

First, we used 119 examples (workers) per treatment, but the number of 
features
for each worker (i.e. the number of distinct labels, or modifications 
thereof produced in preprocessing), was generally in the thousands.  
This tends to
be problematic for many classifier algorithms, including SVM, because it
leads to very high variance models, which manifests as overfitting and poor
performance.  To deal with this, some approach to culling features is 
generally used, but this raises the issue of how to decide the culling in 
a principled manner, generally adding further hyperparameters which could
be optimized.  However, one of the strengths of the na\"ive Bayes classifier
is that it's performance is not generally degraded from the inclusion of 
large numbers of features, even when the number of examples is comparatively
small.  We expected this feature of the na\"ive Bayes classifier to be 
advantageous in our case.

Second, the standard implementation of the na\"ive Bayes classifier has no 
hyperparameters to optimize.  This allows us to forgo the common practice 
of holding out a test set, and instead use cross-validation as a method to
estimate the true classification error, \textit{since we did not perform
any kind of model selection or optimization whatsoever using our data}.
This point is crucial, because if we did optimize our classifier during 
cross-validation, then the validation error would no longer provide an 
unbiased estimate of the true classification error, but would instead lead
to us over-reporting exposure effects.

Third, the na\"ive Bayes algorithm is based on an assumption that the 
probability of observing one feature is independent from the observation
of any other, \textit{when conditioned on the class}.  This
\textit{conditional independence} assumption is generally made for
pragmatic rather than theoretical reasons, and is rarely satisfied.  However,
if it \textit{is} satisfied, then the na\"ive Bayes classifier is optimal.
In our case, conditional independence means that, provided we know the 
exposure of a worker, knowing one label they provided to an image does
not in any way help us guess any other label for that image.  In most
text documents, the existence of topics and language structure certainly
violates the conditional independence of words.  But in our case, such 
effects are less likely to carry from one text input to the next, and in the
small space provided by five inputs, any overarching topics are likely to
be in relation to the image itself, upon which we are conditioning.  In other
words, we believe that, in our setup, conditional independence is better 
satisfied than in most text classification contexts.  That being the case,
one would not expect much improvement in using a more sophisticated algorithm,
such as SVM.

\paragraph{Comparison to other classifiers.} 

\begin{figure}
	\centering
	\includegraphics[scale=0.75]{figs/theta_sup.pdf}
	\label{fig:theta_sup}
	\caption{
		Figure S2: Bias induced by exposure to initial tasks and frames.
		Bias is defined to be the difference in the probability 
		distribution over workers' labels (L1-distance).  The plotted values
		give a lower bound to the L1-distance, $D_{L1}^-$, which was 
		determined from a classifier's performance in 
		distinguishing workers that had undergone different exposures.
		The bias was determined using (A-F) a na\"ive Bayes classifier, and 
		(G-L) an SVM classifier.
		In panels A through F, each plot adds an additional
		preprocessing step to those used in the previous plot; the same is 
		done for panels G through H: A,G) No 
		preprocessing; B,H) spelling correction; C,I) stopword removal; 
		D,J) lemmatization; E,K) splitting of multiple-word labels; 
		F,L) distinguishing identical labels entered in different form inputs.
	}
\end{figure}
The inequality in Eq.~\ref{l1}
asserts that a classifiers performance in predicting class membership
bounds the L1-distance between the distributions of features for the classes.
This bound is tight for the optimal classifier, and in general, the slack
depends on how the classifier is constructed.

Therefore there may exist a classifier that is 
significantly better than the one used to generate our results.  Even before
committing to a particular classifier algorithm, various decisions about
preprocessing need to be made.  For example, we chose to remove stopwords,
lemmatize, split multi-word labels, distinguish the input used to enter 
labels, and correct spelling.  All of these decisions affect the classifier
performance.  The particular classifier algorithm chosen also has a strong 
effect.

Since we did not have enough data to create a separate test set, we could not
optimize these decisions.  Doing so would lead to the inflation of the 
performance, which could then only be estimated using an independent test set.
Instead, we made principled decisions as described above.

Although we cannot optimize these decisions, it is appropriate to look at
what affect these decisions had, post-hoc.  We reproduce the plot shown in 
Fig.~\ref{fig:theta}A using different combinations of preprocessing options,
and using both the na\"ive Bayes and an SVM classifier.

Unlike the na\"ive Bayes classifier, it is necessary to tune the cost and 
gamma hyperparameters of the SVM classifier, as well as choose a kernel.
We used simulated annealing to optimize the cost and gamma settings in the
classification of $task2:food$ vs $task2:cult$.  For this reason, we expect
the bar for $task2$ in Fig.~S8G-J is likely to be an overestimate due to
overfitting on those data.

These plots show that the result shown in Fig.~\ref{fig:theta}A is 
representative, and supports the validity of the lower bounds on the 
exposures exposure effects presented in the main text, in terms of induced 
L1-distance.

\paragraph{Statistical significance and confidence intervals for $\theta$.}
The measurements of $\theta$ were based on the number of correct 
classifications during validation tests.  We used leave-one-out validation,
and count the number of correct classifications, $X$, which has a binomial
distribution.  We take the null hypothesis that the classifier performs
no better than chance, $X \sim \mathrm{Bin}(N,0.5)$, where $N$ is the number 
of workers in both treatments of an experiment.  Then, the critical number 
of correct classifications, $x^*$, in for a one-tailed hypothesis test with
significance $\alpha$ is:
\begin{align}
	&x^* = \sup \left\{
			x: \mathrm{Pr}\{ \mathrm{Bin}(N;0.5) \geq x \} < \alpha
		\right\}\\
	\text{where}\quad &\mathrm{Pr}\{ \mathrm{Bin}(N,p) \geq x \} = 
		\sum_{x'=x}^{N} \binom{N}{x'}p^{x'}(1-p)^{(N-x')}
\end{align}
For the treatments in \textit{intertask-food-objects}, 5 different 
permutations of each treatment was tested, and a different classifier was 
built for each permutation.  For the sake of estimating the accuracy of 
these classifiers, we assume that they have the same accuracy, which means
that \textit{intertask-food-objects} has a larger number of validation tests
(larger $N$), and for this hypothesis test we assume that $X$ has a normal 
distribution, with $\mu=N/2$ and $\sigma = \frac{1}{2}\sqrt{N}$.

To determining the confidence intervals for measured $\theta$ values we 
use the exact Clopper-Pearson method.  
Based on the observed number of successes, $X$, we calculate 
$\eta^*_\mathrm{low}$ and $\eta^*_\mathrm{high}$ from:
\begin{align}
	\eta^*_\mathrm{high} &= \sup
		\left\{
			\eta : \mathrm{Pr}\{\mathrm{Bin}(N; \eta) \leq X \} > 
				\frac{\alpha}{2}
		\right\} \\
	\eta^*_\mathrm{low} &= \inf
		\left\{
			\eta : \mathrm{Pr}\{\mathrm{Bin}(N; \eta) \geq X \} > 
				\frac{\alpha}{2}
		\right\}
\end{align}
Again, for the experiment \textit{intertask-food-objects}, which had 
more validation tests than the others, we derive the confidence intervals
assuming $X$ is normally distributed.
Since the significance values are to be shown on a plot of $\theta$, we 
transform them into equivalent values of 
$\theta$: $\theta^*_\mathrm{high} = 2\eta^*_\mathrm{high} - 1$ and 
$\theta^*_\mathrm{low} = 2\eta^*_\mathrm{low} - 1$.
The statistics collected for the measurement of $\theta$, corresponding to
Fig.~\ref{fig:theta}A are tabulated in 
table X.

\begin{table}
\begin{center}
\begin{tabular}{c c c c c c c c }
	\toprule
	\multirow{2}{*}{Experiment} & \multirow{2}{*}{$N$} & 
	\multirow{2}{*}{$X$} & \multirow{2}{*}{$x^*$} & \multicolumn{3}{c}{(\%)}
		& \multirow{2}{*}{$p$}\\ \cline{5-7} \noalign{\smallskip}
	& & & & $\hat{\theta}$ & $\theta^*_\mathrm{low}$ 
		& $\theta^*_\mathrm{high}$  \\
	\midrule
	\textit{intertask-food-objects} & 1190 & 777 & 624 & \textbf{30.6} 
		& 25.2 & 36.0 & $2.5 \times 10^{-26}$ \\
	\textit{frame-food-objects} & 238 & 122 & 133 & 2.5 & -10.6 & 14.7
		& 0.37 \\
	\textit{echo-food-objects} & 238 & 162 &  133 & \textbf{36.1} & 23.4 
		& 47.1 & $1.3 \times 10^{-8}$ \\
	\textit{intertask-food-culture} & 238 & 180 & 133 & \textbf{51.3} & 39.3 
		& 61.1 & $5.0 \times 10^{-16}$ \\
	\textit{frame-food-culture} & 238 & 130 & 133 & 9.2 & -3.9 & 21.3 
		& 0.087\\
	\bottomrule

\end{tabular}

\caption{Statistics for the measurement of the extent of bias, $\theta$,
	induced by intertask and framing effects in various experiments.
	Number of validation tests, $N$; number of successful classifications, 
	$X$; critical number of succesful classifications to reject the null 
	hypotheses that the classifier does no better than chance 
	(one-tailed test), $x^*$; 
	lower bound estimate of bias based on observed number of successful
	classifications, $\hat{\theta}$; lower confidence interval for lower
	bound estimate of bias, $\theta^*_\mathrm{low}$; upper confidence 
	interval for same, $\theta^*_\mathrm{high}$.  Hypothesis test and 
	confidence intervals are based on a significance of $\alpha=0.05$.
	Values for $\hat{\theta}$ in boldface are significantly greater than 0.
}
\end{center}
\end{table}

The statistics for Fig.~\ref{fig:theta}B are shown in table X.

\begin{table}
\begin{center}
\begin{tabular}{c c c c c c c c }
	\toprule
	\multirow{2}{*}{Task position} & \multirow{2}{*}{$N$} & 
	\multirow{2}{*}{$X$} & \multirow{2}{*}{$x^*$} & \multicolumn{3}{c}{(\%)}
		& \multirow{2}{*}{$p$}\\ \cline{5-7}\noalign{\smallskip}
	& & & & $\hat{\theta}$ & $\theta^*_\mathrm{low}$ 
		& $\theta^*_\mathrm{high}$  \\
	\midrule
	1 & 238 & 152 & 133 & \textbf{28.1} & 14.8 & 39.1 
		& $1.12 \times 10^{-5}$\\
	2 & 238 & 138 & 133 & \textbf{15.6} & 2.9 & 27.8  
		& $8.14 \times 10^{-3}$\\
	3 & 238 & 140 & 133 & \textbf{17.8} & 4.6 & 29.5  
		& $3.87 \times 10^{-3}$\\
	4 & 238 & 135 & 133 & \textbf{13.6} & 0.3 & 25.4  
		& $0.0221$ \\
	5 & 238 & 134 & 133 & \textbf{12.4} & 0.5 & 24.6 & $0.0300$ \\
	\bottomrule
\end{tabular}
\caption{Statistics for the measurement of the extent of bias, $\theta$,
	induced by intertask for \textit{intertask-food-objects}, measured 
	for test images at specific positions in the sequence of 5 test images.
	These values are averaged over 5 different permutations, such that 
	the strength of intertask effects are measured for each of the test images
	in each of the positions.
	Number of validation tests, $N$; number of successful classifications, 
	$X$; critical number of succesful classifications to reject the null 
	hypotheses that the classifier does no better than chance 
	(one-tailed test), $x^*$; 
	lower bound estimate of bias based on observed number of successful
	classifications, $\hat{\theta}$; lower confidence interval for lower
	bound estimate of bias, $\theta^*_\mathrm{low}$; upper confidence 
	interval for same, $\theta^*_\mathrm{high}$.  Hypothesis test and 
	confidence intervals are based on a significance of $\alpha=0.05$.
	Values for $\hat{\theta}$ in boldface are significantly greater than 0.
}






\end{center}
\end{table}





\subsection*{Data preprocessing}
	\paragraph{Splitting, Lemmatization, removal of stop-words, and 
		addition of position tags.} 

	Before performing any analysis on the labels that workers provided, we
	performed a series of preprocessing steps.  
	Labels that contained
	multiple words (separated by spaces or punctuation) were split, with
	punctuation removed, and the separated words were treated as distinct 
	features in subsequent analysis.
	Misspelled words were automatically corrected using a spelling 
	correction algorithm described below.  
	Next, words were lemmatized using the
	wordnet lemmatizer \cite{miller1995wordnet,felbaum1998wordnet}.  
	Common words such as ``the'', ``to'', or ``with'', were found using the
	the NLTK's English stop-word list and removed \cite{loper2002nltk}.  

	For the purpose of training and testing a na\"ive Bayes classifier, we 
	performed an additional preprocessing step.  We tagged words with the
	position in which they had been entered (i.e. which of the five text 
	inputs in the task interface) as well as the test tasks in which the word 
	had been provided.
	So, if the word ``wine'' was entered into the second text-input for 
	the third test-task, after preprocessing, the feature ``3\_2\_wine'' would
	appear.  Prepending the task number onto words was simply a means to 
	retain correct attribution, which itself was essential since providing a
	word during one task is not equivalent to providing the same word during 
	another task.  
	
	\paragraph{Spelling correction.}  
	Spelling correction was performed using an algorithm that first detected
	if a word was likely to be misspelled, then generated a set of candidate 
	corrections, and chose the best candidate based on a scoring mechanism.
	
	A word was considered misspelled if it was not contained in the 
	\textit{legal set}, which was formed by the union of
	the wordnet corpus, the stop-word list, and a set of words seen while 
	crawling the world food section of the allrecipes.com website.  We
	describe the crawling of the allrecipes.com website in a section below.

	To correct misspellings, we first produced all possible modified forms 
	that could be obtained applying one or two edits.  An edit consisted of 
	adding or removing a letter, changing one letter into another, or 
	swapping the positions of two adjacent letters.  For the purpose of these 
	edits, spaces were treated as any other letter.

	The candidates produced by these edits which were in the legal set were
	then ranked based on a scoring mechanism, and the highest scoring word
	was chosen as the correction.  A candidate $w$'s score, $s_w$, was 
	calculated according to the following formula:
	\begin{equation}
		s_w = (f_w + 1) \times p_1 \times p_2,
	\end{equation}
	where $f_w$ is the frequency with which the word occurred (correctly 
	spelled)
	within the given task, $p_1$ was a penalty for the first edit, and
	$p_2$ was a penalty for the second edit.  If the word was made using only
	one edit, then $p_2 = 1$.  Any edit that did not involve adding a space
	(i.e. separating a word) incurred a penalty of 0.5, while the addition of
	a space incurred a penalty of 0.1.  Word-separation edits were more 
	strongly penalized because it is often possible to split a series of 
	letters into many short two- or three-letter words, which otherwise leads 
	to many erroneous corrections.  We found that setting the penalty for
	word splitting at 0.1 worked well by testing on words taken from
	initial tasks.

	After all of our analyses had been performed, we checked the accuracy of 
	the spelling correction algorithm using three human coders.  The coders
	were shown a set of 
	500 words that were randomly sampled from the labels attributed during 
	test tasks (50 for each test task).  Before sampling, the labels from all
	experimental treatments, for the given test task, were pooled together.
	The coders did not know which treatment any given word came from.  
	In addition to the words, coders were shown the spelling correction 
	produced by the algorithm, as well as the image from the test task.
	The coders were asked to identify any misspelled words which had not 
	been corrected, as well as any corrections that appeared to be erroneous,
	by indicating their own correction.

	Based on this exercise, we found that, according to the human coders,
	17\% of the words were misspelled, but that after applying the 
	automated spelling correction, only 3.2\% of words were misspelled.

	\paragraph{Crawling the world food section of allrecipes.com.}
	The website allrecipes.com was accessed on 3-4 November 2014 using 
	automated scripts.  A total of 2642 recipe listings and 15621 recipe
	instruction pages were retrieved.  Recipe listings were pages that 
	provided lists of recipes, and contained a title and short description 
	for each.  
	The recipe instruction pages had lists of ingredients and preparation 
	instructions.  All of the words found in recipe titles, short
	descriptions, ingredients lists, and preparation instructions were
	collected, and saved as an auxiliary set to augment the \textit{legal set}
	used in spelling correction.
	

\subsection*{Most suppressed / activated terms}
The words whose frequencies differed most between the treatments of given 
experiments are shown in Table~S\ref{table:top-words}.  Note that 
the word that was most suppressed in food-exposed treatments was always 
``food''.

\begin{table}
	\centering
	\setlength{\tabcolsep}{10pt}
	\begin{tabular}{ c c c c c }
	
		\setlength{\tabcolsep}{4pt}
		\begin{tabular}{ r | c }
		\toprule
		\multicolumn{2}{c}{\textit{task1}} \\
		\toprule
		coffee & 38 \\
		meal & 34 \\
		cheese & 34 \\
		apple & 32 \\
		dessert & 21 \\
		cup & -30 \\
		glass & -45 \\
		table & -70 \\
		candle & -74 \\
		food & -80 \\
		\bottomrule
		\end{tabular}

&

		\setlength{\tabcolsep}{4pt}
		\begin{tabular}{ r | c }
		\toprule
		\multicolumn{2}{c}{\textit{frame1}} \\
		\toprule
		bread & 18 \\
		wine & 18 \\
		cheese & 16 \\
		apple & 14 \\
		oil & 12 \\
		table & -9 \\
		meal & -10 \\
		candle & -12 \\
		dinner & -13 \\
		food & -32 \\
		\bottomrule
		\end{tabular}

&

		\setlength{\tabcolsep}{4pt}
		\begin{tabular}{ r | c }
		\toprule
		\multicolumn{2}{c}{\textit{echo1}} \\
		\toprule
		apple & 24 \\
		cheese & 23 \\
		wine & 15 \\
		coffee & 14 \\
		oil & 7 \\
		knife & -24 \\
		dinner & -26 \\
		fork & -27 \\
		candle & -35 \\
		food & -55 \\
		\bottomrule
		\end{tabular}

&

		\setlength{\tabcolsep}{4pt}
		\begin{tabular}{ r | c }
		\toprule
		\multicolumn{2}{c}{\textit{task2}} \\
		\toprule
		spicy & 26 \\
		sauce & 17 \\
		indian & 15 \\
		buffet & 14 \\
		exotic & 12 \\
		festival & -11 \\
		offering & -12 \\
		statue & -15 \\
		india & -20 \\
		food & -56 \\
		\bottomrule
		\end{tabular}

&

		\setlength{\tabcolsep}{4pt}
		\begin{tabular}{ r | c }
		\toprule
		\multicolumn{2}{c}{\textit{frame2}} \\
		\toprule
		indian & 11 \\
		banquet & 8 \\
		spicy & 7 \\
		asian & 6 \\
		variety & 6 \\
		delicious & -6 \\
		meat & -7 \\
		festival & -7 \\
		spice & -7 \\
		food & -9 \\
		\bottomrule
		\end{tabular}

	\end{tabular}
	\caption{
		Top 5 words, used to label the first test image, whose usage frequency
		increased or decreased the most between treatments of given 
		experiments.  Positive values indicate that the food-exposed treatment
		used the word with higher frequency.  Word frequencies for 
		\textit{intertask-food-objects} are aggregated across 5 
		permutations, and hence tend to show larger absolute differences.
	}
	\label{table:top-words}
\end{table}

\subsection*{Identification of food-related terms}
The wordnet corpus is composed of synsets, which are particular senses 
(meanings) and a set of word forms bearing that meaning. A given word form, 
such as ``ring'' has multiple senses (``to ring a bell'', ``a wedding ring''), 
and so can be part of many synsets.  Since the wordnet corpus was designed
to carry semantic information, the synset is the basic organizing element
of wordnet.

We chose the synsets \texttt{food.n.01} and \texttt{food.n.02} to act as roots
in defining which words should be considered food-related.  Any word form
belonging to a synset which was a hyponym of one of the two root synsets
identified above was considered to be a reference to food.  Here, as in the
main text, when we say hyponym, we mean either a direct hyponym, or a hyponym
of a hyponym, and so on.

This is a stringent notion of a word being a food reference.  It roughly
corresponds to whether or not a given thing is reasonably considered 
consumable.  So, ``orange'' is food, but ``salty'' is not.  Although ``salty''
would usually qualify something edible, it is not itself an edible thing.
On the other hand, ``salt'' would be considered a reference to food under 
this operationalization.

The subset of the wordnet corpus induced by the hyponyms of \textit{food.n.01}
and \textit{food.n.02} provide a set of 3590 words.  Manual
testing showed good coverage, except for references to ethnic foods.  
Since, especially in \textit{exp2}, we had images containing many ethnic 
foods, we decided to augment the wordnet corpus with words learned by 
crawling the world food section of the allrecipes.com website.  

After collecting a set of words using an automated script 
(described above), we collected the set of words that had been used by
workers and which had been seen during crawling of the allrecipes.com website,
but which were not included in wordnet.  We grafted these extra words into
the wordnet hyponym-hypernym graph manually, which ensured that any references
to ethnic words learned from crawling would be properly classified as 
food references.

\paragraph{Validation of the detection of food references.}
To determine whether our extended version of wordnet provided a good approach
to detecting food-related words, we sampled 500 words 
(50 from each test task)  from the set of labels produced by workers and had 
three human coders manually decide if they were 
food-related or not.

The coders were instructed to consider whether a word was a noun signifying
an edible item.  The guiding principle was, for the coder to ask herself,
``can I eat X'', where X is the word she is coding.  The coders were
shown the used in the task from which the word came, to help resolve ambiguity
in the sense of the word that had been given.  Coders were also shown the 
spelling correction (if any) that the spell-correction algorithm had made,
to help interpret misspelled words.

We evaluated the food detection algorithm in two ways.  First, we measured
the inter-rater reliability between the three human coders and the algorithm
(treating the algorithm just like any other coder).  Second, adopting the 
majority code given by the human coders, we determined the accuracy of the
food-detection algorithm.  Data summarizing this validation process are given
in Table~S\ref{table:inter-rater}.

\begin{table}
\centering
\setlength{\tabcolsep}{12pt}
\begin{tabular}{ r | c }
\toprule    
\# terms coded & 500 \\
\# food references & 130 \\
\# correct machine codes & 440 \\
inter-rater reliability & 82.4\% \\
human-machine code agreement & 88\% \\
%machine recall & 80 \% \\
%machine precision & 75.4 \% \\
%machine code F1 & 0.78 \\
\bottomrule
\end{tabular}
\caption{
	Results for the validation 
	of the automatic detection of food references, by comparison to
	human-generated coding on randomly sampled words.  In calculating the 
	human-machine code agreement, the majority human code was used.
	Inter-rater reliability was calculated as Krippendorff's alpha.
}
\label{table:inter-rater}
\end{table}

The results show that good inter-rater reliability was achieved during the
validation (82.4\%).  Looking at the agreement between the 
machine codes and the majority human-generated codes, we find they agree
88\% of the time.  This means that the concept used to instruct human coders
provides a good approximation to what the wordnet-based machine codes actually
represent. In other words, the machine codes correspond closely to indicating
which words correspond to nouns that refer to edible things, and which ones
do not, which provides a clear and simple interpretation for the machine
coding of food and non-food words.

\paragraph{Testing differences in proportions of food-related labels.}
We calculate the proportion of food-related labels in a treatment by first
calculating the proportion of food-related labels per worker, and then
taking the average of this value for all workers.  We calculate the
sample standard deviation for workers' proportions of food-related terms,
$s_w$ and then estimate the standard deviation of the average proportion
to be $s = s_w / \sqrt{N}$, where $N$ is the number of workers in the 
treatment.  The proportions of food-related words for all treatments,
and the statistics for the hypothesis test that the proportions between
treatments within given experiments differ, are shown in 
Table~S\ref{table:proportion-food}.

\begin{table}
\centering{
\setlength{\tabcolsep}{4pt}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{c c c c c c c c c c c}
	\toprule
	\multirow{2}{*}{Experiment} 
		& \multicolumn{2}{c}{food treatment}
		& \ 
		& \multicolumn{2}{c}{other treatment} 
		& \
		& \multicolumn{2}{c}{(other - food)} 
	& \multirow{2}{*}{$z$-score}
	& \multirow{2}{*}{$p$-value} \\ \cline{2-3} \cline{5-6} \cline{8-9}
	& \% refs. 
	& $s$ 					
	& \
	& \% refs. 
	& $s$ 					
	& \
	& $\Delta$\% refs. 
	& $s$ 					 \\ 
	\midrule
	\textit{intertask-food-objects} & 54.5 & 0.6 & \ & 57.4 & 0.8 & \ 
		& \textbf{2.9} & 1.0 & 3.0 & $1.3 \times 10^{-3}$\\
	\textit{frame-food-objects} & 56.2 & 1.4 & \ & 60.1 & 1.4 & \ 
		& \textbf{3.9} & 1.9 & 2.0 & $2.2 \times 10^{-2}$\\
	\textit{echo-food-objects} & 54.0 & 1.4 & \ & 62.8 & 1.7 & \ 
		& \textbf{8.8} & 2.2 & 4.0 & $3.5 \times 10^{-5}$\\
	\textit{intertask-food-culture} & 48.8 & 1.6 & \ & 40.8 & 2.0 & \ 
		& \textbf{-8.0} & 2.6 & -3.1 & $9.2 \times 10^{-4}$\\
	\textit{frame-food-culture} & 45.8 & 2.0 & \ & 45.2 & 1.8 & \ 
		& -0.6 & 2.6 & -0.2 & $0.41$\\
	\bottomrule
\end{tabular}
}
\caption{Statistics for the comparison of the proportions of food-related
	labels between treatments of all experiments.  $s$: sample standard
	deviation.
}
\label{table:proportion-food}
\end{table}

\subsection*{Calculation of relative specificity}
In the main text we present results for the relative specificity of labels
produced by two experimental treatments.  In performing this calculation,
we used the wordnet corpus, which has a set of hypernym-hyponym relationships
as described in the main text.  The first step in performing this calculation
was therefore to map the words occurring from (the labels of) a given 
experimental treatment onto the wordnet synsets.  In general, a given word
form can have multiple meanings, and therefore maps onto multiple wordnet 
synsets.

The result of mapping the words from two treatments onto the wordnet 
hypernym-hyponym graph two sets of counts, one set per treatment, 
indicating the number of times a word from a given synset occurred among the
labels of the treatment.  Then, for every synset in one treatment, we looked
at the number of synsets in the other treatment that were more generic
(reachable by following hypernym relations) less the number of words that
were more specific (reachable by following hyponym relations).  This quantity
tallied for all words in the original synset gives a (non-normalized) measure
of the degree to which words from the first treatment are more specific
than words from the second treatment, overall.  We then normalized this 
quantity by the total number of possible comparisons between the words of
one treatment to those of the other, where two words are considered comparable
only if one word is the hypernym or hyponym of the other.  Hence, ``statue'' 
and ``bread'' are not comparable, but ``pumpernickel'' and ``bread'' are.

This calculation can be summarized by the following equation:
\begin{equation}
	S(P,Q) = \frac{
		\sum_{w\in P}\sum_{v\in Q} \left(
			\mathbf{1}_{[w>v]} - \mathbf{1}_{[v>w]} \right)
	}{
		\sum_{w\in P}\sum_{v\in Q} \left(
			\mathbf{1}_{[w>v]} + \mathbf{1}_{[v>w]} \right)
	},
\end{equation}
where $P$ and $Q$ are sets of synsets, and $\mathbf{1}_{[w>v]}$ evaluates
to 1 if synset $w$ is more specific than (i.e. is a hyponym of) synset $v$.
This measure counts the excess of cases where synsets from $P$ are more
specific then synsets from $Q$, as a fraction of all comparable synset pairs. 

In computing this quantity between two treatments, we first computed it 
using the labels attributed to a particular test task, and then
averaged the result obtained in this way for each of the five test tasks.

To assess statistical significance, we used the bootstrap method.
For two given treatments $A$ and $B$, 
119 workers were sampled from each with replacement, giving the bootstrap
samples $A'$ and $B'$, and the relative specificity between these 
was measured as described in \textsection S6.
This was repeated 1000 times.  The 2.5 and 97.5 percentiles from the 
bootstrap sample to create the 95\% confidence intervals.  If the confidence
interval did not contain zero it indicates a statistically significant 
difference in specificity.  The statistics for relative specificity 
calculations are shown in Table~S\ref{table:specificity}.

\begin{table}
\centering{
	\setlength{\tabcolsep}{4pt}
\begin{tabular}{c c c c c c c c c c c}
	\toprule
	Experiment & $S$ & $S^*_\mathrm{lowCI}$ & $S^*_\mathrm{highCI}$ \\ 
	\midrule
	\textit{intertask-food-objects} & \textbf{13.6} & 9.1 & 17.8 \\
	\textit{frame-food-objects} & \textbf{12.4}     & 7.6 & 17.0 \\
	\textit{echo-food-objects} & \textbf{17.1}      & 12.1 & 21.9 \\
	\textit{intertask-food-culture} & \textbf{19.6} & 13.7 & 25.4 \\
	\textit{frame-food-culture} & \textbf{7.0}      & 2.1 & 15.3 \\
	\bottomrule
\end{tabular}
}
\caption{Relative specificity ($S$) of food-related words between the food- 
	and non-food-exposed treatments of all experiments. 
	Positive values indicates the food-exposed treatment was relatively more
	specific; boldfaced values indicate a disparity in specificity that 
	is statistically significant.  
	$S^*_\mathrm{lowCI}, S^*_\mathrm{upCI}$: lower and upper
	confidence intervals for $S$ based on the bootstrapping approach.  All
values are expressed as percentages.}
\label{table:specificity}
\end{table}




	\paragraph{Variance in food vocabulary size (lexical richness).}
	The number of unique words in a text is strongly dependent on the size
	of the text: as more text is sampled, previously unseen words continue
	to be found, though at a decreasing rate.  
	This means that the vocabulary size of a population of documents is 
	not well-approximated by a sample.  Nevertheless the vocabulary sizes
	obtained by taking equally-sized samples of two different populations
	provides a means to compare the populations on an equal footing.

	Taking the difference between one sample's vocabulary and the 
	other gives us a measure of their difference in vocabulary size, but 
	the variance of this measure needs to be characterized to know whether
	there is a significant difference.
	
	To test whether one sample has a (statistically) significantly larger
	vocabulary size than another, we adopt the null hypothesis that they
	have the same vocabulary, and pool the documents sampled from both 
	populations together.  We then randomly partition the pooled sample
	back into two balanced sets and take the difference in the sets' 
	vocabulary sizes.  This models the difference that would be observed
	under the null hypothesis that the documents are drawn from the same
	population.  We repeat the pooling and partitioning of the samples
	1000 times to generate a bootstrap sample of the difference in 
	vocabulary sizes under the null hypothesis.  The 
	2.5th and 97.5th percentiles of the bootstrap sample then serve as the
	critical values of vocabulary size-difference that would justify 
	rejection of the null hypothesis that the original samples have the same 
	vocabulary size.

	Statistics for these hypothesis tests are shown in 
	Table~S\ref{table:vocab}.

\begin{table}
\centering{
\begin{tabular}{c c c c c c c }
	\toprule
	\multirow{2}{*}{Experiment} & \multicolumn{2}{c}{$|V|$} 
		 & \ & \multicolumn{3}{c}{$(\%)$} \\ \cline{2-3} \cline{5-7}
	& food trtmt. & other trtmt. & \ & $\Delta|V|$ & $\Delta|V|^*_{low}$
		& $\Delta|V|^*_{high}$  \\
	\midrule
	\textit{intertask-food-objects} & 297 & 247 & \ 
		& \textbf{20.2} & -10.7 & 11.5 \\
	\textit{frame-food-objects} & 296 & 261 & \ 
		& \textbf{13.4} & -9.7 & 10.1 \\
	\textit{echo-food-objects} & 316 & 280 & \ & 
		\textbf{12.9} & -11.8 & 11.1 \\
	\textit{intertask-food-culture} & 289 & 244 & \ 
		& \textbf{18.4} & -11.4 & 11.6 \\
	\textit{frame-food-culture} & 318 & 332 & \ & -4.2 & -11.9 & 11.2 \\
	\bottomrule
\end{tabular}
}
\caption{
	Statistics for the measurement of vocabulary size difference, for each
	experiment, between the food and non-food treatments. $|V|$: vocabulary
	size; $\Delta|V|$: vocabulary size of non-food less food treatments;
	$\Delta|V|^*_{low}, \Delta|V|^*_{low}$: lower and upper critical values
	for statistically significant $\Delta|V|$, at $\alpha=0.05$, 
	based on bootstrapping.  
	Boldfaced values for $\Delta|V|$ are statistically significant.
}
\label{table:vocab}
\end{table}




\end{document}

