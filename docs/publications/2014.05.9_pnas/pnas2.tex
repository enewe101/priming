%        File: pnas1.tex
%     Created: Wed Apr 30 04:00 pm 2014 E
% Last Change: Wed Apr 30 04:00 pm 2014 E
%
\documentclass[a4paper]{report}
\usepackage[hmargin=1in,vmargin=1in]{geometry}

\usepackage{graphicx}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{gensymb}
\usepackage{algorithm2e}
\usepackage{amsthm}

\newtheorem*{mydef}{Definition}

\usepackage{cite}

\usepackage{framed, color}
\usepackage{soul}
\usepackage[colorlinks=false, urlcolor=blue]{hyperref}

\newcommand{\td}[1]{{\color{blu}\hl{TODO: #1}}}

\definecolor{shadecolor}{rgb}{.93,.93,.93}
\definecolor{blu}{rgb}{0,0,1}
\setlength{\parindent}{0cm}
\setlength{\parskip}{4mm plus1mm minus1mm}


\title{The effects of priming in human computation}
\author{Edward Newel, Derek Ruths}
 
\parindent0pt \parskip8pt

\begin{document}
\maketitle
\section*{Abstract}
\section*{Significance Statement}
\section*{Introduction}

Microtask crowdsourcing platforms like Amazon Mechanical Turk (AMT) make it 
possible to submit batches of small tasks to a large pool of human workers, 
who do the tasks for fun and remuneration.  Tasks involving qualitative
judgment (e.g. flagging innapropriate images), natural language annotation,
and object recognition are commonplace, since people still outperform  
computers in these areas.

Such platforms render human cognition as fluid a resource as compute cycles: 
it is available on demand in small units, with low transaction cost, from 
anywhere via the Internet \footnote{\href{http://mturk.com}{http://mturk.com}}.
The task requestor can interact with the platform like a compute server, 
seemlessly 
integrating human and machine computation.  Researchers have put forward the 
term HPU (Human co-Processing Unit)\footnote{We acknowledge the irony of this 
term: the original meaning of `computer' was a person who performs 
computations, its reference to machines being a personification brought into 
usage in the late 1800s\cite{Dictionary:hl}.}, viewing the introduction of 
microtask platforms as a fundamentally new computing architecture
\cite{5543192}.  

Originally used to distribute clerical work, these platforms 
increasingly serve as a cheap and fast means to engage experimental 
participants and provide qualitative annotations of datasets in a research 
setting.  Given the limited ability to screen workers, and their transient 
low-commitment engagement with the employer, researchers have criticised the
reliability of crowdsourced output \cite{marsden2009crowdsourcing}.  

For 
example, cheating
is a problem \cite{lease2011quality}, but straightforward methods have been 
developped to screen out cheaters \cite{snow2008cheap, kazai2013analysis}.
Still, there is a great deal of variance in worker attention and 
accuracy \cite{kazai2013analysis}.  Researchers have found many factors that 
modulate worker performance, such as level of pay\cite{kazai2013analysis}, 
training\cite{le2010ensuring}, pre-screening 
workers\cite{paolacci2010running},  user-interface design
\cite{Finnerty2013}, providing context for the task in the wider workflow
\cite{Kinnaird2012281}, and disclosing the purpose of the task 
\cite{chandler2013breaking}.

Here we are concerned with effects of a more subtle origin: the influence that
prior tasks have on subsequent ones.  Psychology experiments document the 
strong effects that a prior exposure to stimulus can have on participants' 
performance in related tasks, a phenomenon known as \textit{priming}.  Strong 
priming effects have been demonstrated for
perceptual tasks such as the visual recognition of 
objects \cite{BJOP:BJOP1796},
auditory recognition of words \cite{BJOP:BJOP1826}, and the recognition
of gender* and affect*: precisely the kinds of tasks well-served by microtask 
platforms \cite{yuen2011survey, snow2008cheap}.  

Reserearchers 
have investigated various effects due to \textit{framing}, at the outset of 
the task.  But psychological experiments show that priming is strongest when 
delivered through the same modality as the task.  This suggests that it might
be more important to investigate the effects of priming that occurs 
\textit{during} tasks.  We call such effects \textit{in-task priming}.  
Whereas requestors can take measures to frame 
their task as they see fit, in-task priming would be unavoidable in many 
contexts, with consequences for the validity of microtask platforms as a 
research tool.

We investigate the effects of in-task priming on using an image-labeling task
on the AMT platform.  Each worker is required to label a \textit{prior set} of
images followed by a \textit{posterior set}, but no distinction is made between
these sets from the perspective of the worker.  The posterior set is
always the same, but workers are randomly assigned one of three prior sets.
We analyze effects on the labels given to the the posterior set in terms of 
what aspects the workers focus on (``orientation of labels''), and the degree 
of specificity of the labels, both of direct consequence to the quality of 
labeling and classification.

As a point of comparison, we subject some groups of workers to a kind of
framing, by disclosing a semantically-loaded (fictitious) name of the 
funder of the research.  Remarkably, we find that in-task priming has a much 
stronger effect
on label orientation and specificity.  This has major implications for the 
design of qualitative labeling and classification tasks.  Our results show that
significant bias in worker labelling can be introduced by the progression of 
tasks.  On the other hand, our results suggest the progression of tasks can 
alse be used to tune workers to produce more nuanced output.

Traditionally, priming effects are
measured using methods chosen to elucidate the underlying psychological 
mechanisms.  For the human computation context, we seek general 
methods that detect any hysteresis effects in HPUs, whatever they may be.  
Here we present a novel general-purpose method for detecting priming, based on 
a rigorous algorithmic definition. 
This method provides a powerful new tool for detecting priming in populations
without having to know anything about how the priming was carried out, nor the 
effects that it might be expected to have.
We demonstrate the utility of this method by showing that
it can detect priming effects of all treatments in our experiment, even when 
they fail to be detected through their effects on label orientation and 
specificity. This method has the potential for a wide range of applications 
outside computer science.  

\section*{Text}
\subsection*{An algorithmic definition of priming}
Motivate the need for this kind of instrumentalization.

One difficulty with the notion of priming is that there appears to be no 
defensible notion of an ``unprimed state''.  Consider the ambiguous image set 
in our experiment.  Would we be better to include a treatment with no
priming images at all?  We argue that, in that case, one would mainly observe  
effects of \textit{primacy}.  
Further, a participant always enters the experiment in \textit{some} state of 
mind, depending on her exposures prior to the experiment, which cannot be 
known by the experimenter.  By including a priming exposure that is in some 
sense `neutral' for the purposes of an experiment, one at least makes the 
priming of participants more uniform.  

Because there
can be no unprimed state, we define priming relatively, and speak of one
treatment being \textit{differently primed} from another.

Another factor that we must consider is that the effects of priming will
depend on the task presented to the participant.  Two priming treatments that
produce markedly different performance for one task might produce 
indistinguishable performance for a diffirent task.  Thus, we define priming
in relation to the task.

A final factor that we must consider is the fact that, even when priming 
treatments are taken into account, the performance of participants in a task 
will differ due to the natural variance between people.  There would appear to
be no hope of accounting for the myriad factors that drive this variance.  Thus
to be practical, priming must be defined in relation to populations, not
individuals.

With these considerations, we propose the following algorithmic definition of
priming:

\begin{mydef}
	Two populations of people $J$ and $K$ that produce respectively two 
	populations of outputs $\mathcal{J}$ and $\mathcal{K}$ in the completion
	of a task $\mathcal{T}$, are said to \emph{differ in priming by 
	a degree at least $\theta$}, if there exists an algorithm $\mathcal{A}$ 
	that can distinguish (classify)
	a person drawn uniformly from $J$ or $K$ based on the outputs $x$ of the 
	person during $\mathcal{T}$ with accuracy $\frac{1+\theta}{2}$.
	$\mathcal{A}$ receives as input $\mathcal{J}$, $\mathcal{K}$, and $x$,
	and must have a running time that is polynomial in the size of its input.
\end{mydef}

In the language of computer science, $\mathcal{A}$ is a classifier.  Thus,
intuitively, if there exists a classifier that can distinguish two populations,
then they are differently primed.

Other than providing the usual running time constraint on $\mathcal{A}$, we
leave it completely undefined.  This is intentional since we want the 
definition to depend on the inherent distinguishability of populations, and
restricting the basis of distinction would arbitrarily mask certain kinds of
distinctness.

One consequence of this choice is that empirical measures of $\theta$ always
represent lower bounds on the theoretical value appearing in the definition.
So, empirically demonstrating non-negligible $\theta$ establishes a priming 
difference of \textit{at least} $\theta$, but on measuring a negligible 
$\theta$, one might argue that $\theta$ is not really negligible, and rather 
that a bad choice of classifier was made.

In practice we do not expect any serious difficulty on choosing a classifier
and establishing a meaningful value of $\theta$.  One of the simplest 
classifier algoritms, the \textit{Naive Bayes classifier}, performs very 
nearly as well as the most sophisticated classifiers in nearly all 
applications.  Therefore, we submit $\theta_\text{NB}$, as determined by a 
Naive Bayes classifier, to be a general-purpose measure of priming difference.

\subsection*{Experimental Setup}
AMT workers are randomly assigned to one of seven treatments 
(see \textbf{Table 1}).  After brief instructions, workers are either
shown a credit for the research funder, which can be either 
``The Global foundation for Cultural Recognition'', or 
``The National Foundation for Nutritional Awareness'', or else they are 
immediately presented with a prior image set.

The prior and posterior images sets both consist of five images. Each image 
must be assigned labelled five labels.  For the posterior image set we chose 
images that contain prepared meals and feature a prominent, iconic culture 
(see \textbf{Fig. 1}).  These images provide a wide array of possible labels, 
may focus on food, culture or both.

One of the prior image sets, which we call \textit{ambiguous}, was chosen to
be most similar to the posterior image set, and contains images of prepared
meals, but whose cultural features are less prominent (see \textbf{Fig. 2}).  
The \textit{cultural} set features iconic cultural scenes, and no food at all
(see \textbf{Fig. 3}).  Finally, images from
the \textit{ingredients} image set depict separated ingredients, having no
prominent cultural aspects (see \textbf{Fig. 4}).  \textbf{Table 1} shows the 
combinations of framing and in task priming that we used for each 
experimental treatment.  See the \textbf{Materials and Methods} section for
further details.



\subsection*{Results}
\paragraph{All treatments were differently primed.}
We test for priming effects using our definition proposed in the introduction,
implemented with a Naive Bayes classifier.  \textbf{Figure 5} shows
the classifier performance in distinguishing between different pairs of 
treatments, when provided workers' labels for the test images.

The classifier distinguishes the various treatments from one another (pairwise)
with high accuracy, confirming that both priming modalities 
(in-task and framing) were effective.  It is perhaps not surprising that
a Naive Bayes classifier can distinguish treatments tending toward a different 
``orientation'', e.g. $\textsc{cult}_{img}$ and $\textsc{ingr}_{img}$
(\textbf{Fig. 5A}).
But it is remarkable that, based only on the labels provided for the five test 
images, the classifier can distinguish workers from treatments of the 
\textit{same} orientation, such as $\textsc{cult}_{img}$ and 
$\textsc{cult}_{fund, img}$, with high accuracy (\textbf{Fig. 5B}).

Next we look at the degree to which labels from individual test images reveal 
priming.  It would stand to reason that, as workers proceed through 
the test images, the effects of priming might be ``washed out''.  
\textbf{Figure 6} shows the
performance of the Naive Bayes classifier when using the labels workers provide
for particular images.  This does show that, as we look to later test images, 
the treatments' difference in priming, $\theta_\text{NB}$, declines.  

\paragraph{Priming orients focus.} Having demonstrated differential 
priming, we investigate the \textit{nature} of the priming.  To this
end, we construct an ontology of all the labels produced by the workers
for the five test images.  The ontology is a directed acyclic graph of terms,
with directed edges drawn from abstractions toward instances.  For example,
our ontology contains the path \texttt{food} $\to$ \texttt{ingredients} $\to$ 
\texttt{vegetables} 
$\to$ \texttt{tomato}.  In general, each of these terms is a label that
occured in the corpus of labels provided, but in some cases we added 
an abstraction when it was missing, to connect the ontology.

At the root of the ontology are the abstract classes \texttt{activity}, 
\texttt{adjective}, \texttt{culture}, \texttt{food}, and \texttt{thing}.
By simply counting the number of labels descended from \texttt{culture}
and \texttt{food}, we can guage effects on the orientation of workers's focus
with respect to these concepts.

\textbf{Figure 7} plots the composition of labels (fraction of culture- and 
food-oriented labels) for different treatments and images.  In 
panel \textbf{A}, we can see that, before considering the effects of 
treatments, there is significant variance in the composition of the labels 
for different images due to inherent differences in their content.  For our 
purpose, we are interested in the differential of composition between 
different treatments.

In \textbf{Fig. 7B} we exhibit the composition of labels for the various 
treatments (aggregating together the labels for all images for a given 
treatment).  We can see that the treatments which expose workers to the
cultural image sets do elicit significantly more culture-oriented and fewer
food-oriented labels compared to the other treatments ($\alpha=0.05$).  It 
also apperas that all the $\textsc{ingr}_x$ treatments produce more 
food-oriented labels, but we cannot assert this at significance 
$\alpha = 0.05$. 

The effect size, however, is somewhat small.  Since we observed that priming 
difference ``wears off'', it may be the case 
that aggregating label compositions for all images actually masks the effects. 
In \textbf{Fig. 7C}, we plot label compositions for only the first
image.  This reveals that, in labeling the first test image, workers in the
$\textsc{cult}_{img,x}$ were strongly influenced to favor culture- over 
food-oriented labels.  The effect was strong enough to essentially reverse 
the proportions of these labels compared to the other treatments.

We next inspect this effect longitudinally, that is, as a function of 
the test image.  To do so, we compute the ``excess cultural orientation'' 
($\Delta_{cult}$) of
the $\textsc{cult}_{img}$ relative to $\textsc{ambg}$ as follows: 
\begin{align}
	\Delta_{cult}(i) = \frac{1}{N}\left[ \sum_{w\in\textsc{cult}_{img}} \left(N_{w,cult}^{(i)} - N_{w,food}^{(i)}\right)
	- \sum_{w\in\textsc{ambg}} \left(N_{w,cult}^{(i)} - N_{w,food}^{(i)}\right)\right],
\end{align}
where $N_{w,cult}^{(i)}$ stands for the number of culture-oriented labels 
attributed by worker $w$ to image $i$, while $N_{w,food}^{(i)}$ similarly 
counts food-oriented labels, and $N$ is the total number of labels in a 
treatment (2500).  
Intuitively, this is intended to capture how much 
more culture-oriented $\textsc{cult}_{img}$ is than $\textsc{ambg}$.  
\textbf{Figure 8} plots $\Delta_{cult}$ as a function of test image, $i$. It
clearly shows the strong influence on the labels attributed to the first image,
but this effect immediately drops below statistically measureable levels, while
remaining always positive.

\paragraph{Priming affects attention to detail.} Using the ontology described
in the previous section, we compare treatments in their degree of specificity.
When comparing two labels $\ell_1$ and $\ell_2$ produced by different workers,
we say that $\ell_2$ is more specific than $\ell_1$ if there is a 
\textit{directed path} from $\ell_1$ to $\ell_2$.  If there is no directed path
between $\ell_1$ and $\ell_2$, then they are uncomparable.  As an example,
\texttt{tomato} is more specific than \texttt{food}, while \texttt{statue}
and \texttt{food} are uncomparable.

We can compare the bulk specificity of two treatments by taking a subsample
of workers from each, and the labels of the workers in one
subsample to each of those in the other.  In drawing this comparison, we only 
compare labels that were attributed to the same image.  Thus the relative 
specificity of two treatments can then be computed as:

We can compare the specificity of two workers by comparing, for each image,
labels given by one worker with those of the other.  Thus, we define the the
relative specificity of worker $u$ to worker $v$ to be $s(u,v)$:

\begin{align}
	s(u,v) = \sum_{i=1}^5 \;
	\sum_{\ell \in u(i) } \;
	\sum_{m \in v(i)} 
	\left(\mathbf{1}[\ell > m] - \mathbf{1}[m>\ell]\right),
	\label{eq:worker-specificity}
\end{align}

We can then calculate the relative specificity of two treatments to be the
average relative specificity of workers drawn uniformly from the treatments.

\begin{align}
	S(U,V) = 
		\frac{1}{|U \times V|}
		\sum_{(u,v) \; \in \; U \times V} \;
		s(u,v)
		\label{eq:specificity}
\end{align}

Where \textsc{x} and \textsc{y} are treatment subsamples,
$\mathcal{L}(\textsc{x},i)$ is the set of all labels produced by workers in 
$\textsc{x}$ for test image $i$, and $\mathbf{1}[\ell > m]$ evaluates to 1 if 
$\ell$ is more specific than $m$, and 0 otherwise.  \textbf{Figure 9} 
exhibits the results.

In panel \textbf{A}, we see that when we compare each of the treatments to
$\textsc{ambg}$, all treatments in which the non-ambiguous image set was 
displayed have \textit{lower} specificity (though not all at $\alpha=0.05$).  
In panels \textbf{D} and \textbf{G} we compare the same pairs of treatments, 
but restrict to words of culture- and food-orientation respectively (words
of both orientations are excluded).  These plots show that the relative lack of specificity is mainly attributable to food-oriented words.  

Likewise, in panels \textbf{B}, and \textbf{C}, 
wherever we see significant differences in specificity, it turns out that
these differences appear to mainly be due to differences in food specificity,
as can be seen by comparing to panels \textbf{E}, \textbf{F}, \textbf{H}, and 
\textbf{I}.  In fact, looking to panels \textbf{D}, \textbf{E}, and \textbf{F},
there are no significant differences in cultural specificity to speak of. 

\subsection*{Discussion}

\paragraph{In-task priming is strong.}  Perhaps the most unexpected result of 
this study is the finding that the in-task priming was more influential than
framing.  We expected that the funder names would provide an overt signal of 
intent behind the task, and so would be stronger than in-task priming which 
we expected would not signal intent.  Perhaps in applications where priming 
is a concern, a certain portion of the task should be devoted to  
``burn-in''.  During burn-in, the participant would acclimatize to the task, 
while the effects of stimuli preceeding the task are washed out.

\paragraph{The rate of priming wash-out depends on how it is measured}
Seeing that $\theta_\text{NB}$ declines as one proceeds through the test 
images (\textbf{Fig. 6}), one should ask whether this actually represents 
priming wash-out.  One might also attribute this to 
differences in the \textit{content} of the images.  Nevertheless, we can
certainly assert that the effects of priming detected by the classifier 
persisted through the labelling of all five test images.  

It is interesting to compare this to the result in \textbf{Fig. 8}, which
shows that the effect of priming on \textit{orientation} (culture vs food) 
decays much more quickly than the overall effects detected by
the classifier.  We take this to be the result of the complex network of 
mechanisms likely to be at play in priming, wherein the orientation of focus
is one mechanism that is particularly fast-responding.


\paragraph{Priming effects can be subtle even when strong.}
Just as the classifier continues to detect priming difference between 
$\textsc{cult}_{img}$ and \textsc{ambg} after the first test image,
it is able to differentiate both $\textsc{cult}_{fund}$ and 
$\textsc{ingr}_{fund}$ from \textsc{ambg}, even though we find no significant
differences in label orientation or specificity.  

Clearly, the effects of
priming, even when definitive, can be subtle. Adopting the classifier-based
definition of priming allows one to detect effecs even if they are subtle or
not of an expected nature.


\paragraph{Priming did not affect cultural specificity.}
It was quite surpising that the effects on workers' specificity was essentially
confined to food-oriented labels, and that we observed no significant movements
in cultural specificity.   This might simply reflect the different roles
played by food and culture in the human lexicon.  This seems plausible when we 
consider the vital role of food, and the known result that people are less
discerning of characteristics of ``outsiders''.

Another explanation is that food is more so-called \textit{imageable} than 
culture, because although aspects of culture can be depicted in an image,
culture itself cannot.  Studies show that the brain processes high- and 
low-imageable words differently \cite{Swaab200299}, and it would not be 
surprising if the effects of priming on image labeling were modulated by how 
imageable the labels are.

Even with the above explanations, is surprising that 
$\textsc{cult}_{img,x}$ simultaneously could be strongly influenced to 
produce a higher quantity of culture-oriented labels, and yet not a higher
specificity of culture-oriented labels.  Since effects on 
label composition were only appreciable for the first test image 
(see \textbf{Fig. 8}), we also
performed the specificity analysis restricting to labels from the first
test image (see \textbf{Fig. S1}).  But even focusing on the first test image
shows no significant increase in cultural specificity for 
$\textsc{cult}_{img,x}$.  Thus, we observe that orientation of focus and 
label specificity do not necessarily.


\paragraph{Similarity drives nuance.}
We observed already that treatments primed with the 
culture or ingredients image sets have less specificity compared to 
\textsc{ambg}.  Actually, nearly all the difference in specificity
can be explained by how different a treatment's priming images are from the 
test images.  

Recall that the ambigouous set featured prepared meals, and 
was very similar to the test set.  The ingredients set at least featured 
food, although as separate ingredients instead of prepared meals, while the
culture image set was very different from the test images, containing no food
at all.
Indeed, not only is it true for comparisons to \textsc{ambg}, but in all the 
comparisons between treatments, specificity is higher for the 
treatments shown the ambiguous set than those shown ingredients set, 
and higher for those shown the ingredients set than those shown the culture 
set.

An appealing explanation for 
this is the notion of \textit{negative priming}, wherin the participant
becomes desensitized to stimuli that do not seem salient.  According to this 
argument, workers who have been shown successive images of prepared meals 
would be much less inclined to provide generic terms such as \texttt{food} or 
\texttt{meal} since these terms would no longer seem significant in the 
context of their priming treatment.  

This suggests that, in the serial image labelling task, the worker
does not seek labels that are appropriate descriptors for the image so much 
as labels that distinguish the images from others she has recently seen.
It would be interesting to investigate whether this manifests other 
categorization and labeling tasks.

\paragraph{Priming for better HPU performance.}
Thus, if one seeks very nuanced sorting or labeling from human workers, 
we recommend assembling the HPUs in a hierarchical fashion, with the 
HPUs at the top of the hierarchy performing coarse sorting (or labeling), 
splitting objects into streams to be handled by HPUs performing more nuanced 
work.  To implement this for an
image-labeling task, one could first serve the images for coarse labelling,
then cluster the images based on the labels and a language model, and then 
reserve the images, ensuring that each HPU operates on images from a single 
cluster.  The ``denser'' the image set in terms of coverage of certain
regions of ``content space'', the more one might gain through repeated 
clustering and labelling.

This exhibits an interesting difference between HPUs and CPUs.  In general,
whenever an aspect of a problem can be parallelized when employing CPUs, one
gains efficiency.  But in the case of HPUs, which are hysteretic, we see an 
example where one might lose efficiency through parallelization.

\subsection*{Conclusion}
We have demonstrated that in-task priming has a strong effect.  It is stronger
in its direct effects on orientation of focus and specificity than is 
disclosing a funder name that signals the intent of the study.  
Rather than being a nuisance, in-task priming might be exploited to improve
task output, for example, to drive more (or less) nuanced responses.

Although the nature of the effects of framing were difficult to trace
in this experiment, we nevertheless demonstrated that those effects exist
and are strong, using our algorithmic definition of priming.  This exhibits
one of the main advantages of this definition: the analyst need 
not understand the details of \textit{how} priming influences a task to 
quantify its effect.

We advocate the view of priming as a form of hysteresis.  The notion of 
hysteresis is already used in varied disciplines like physics, engineering,
and economics.  This view invites the use of insights already gained from 
other fields, and application-independant models, to help understand human 
cognition.

\subsection*{Materials and Methods}

\textbf{AMT Tasks.} The task desicribed in the \textbf{Experimental Setup}
section was served to 900 workers, who were uniformly randomly assigned to
treatments.  All treatments had at least 134 task completions.  Workers were
only allowed to participate once and could not see the prior or posterior 
images when previewing the task.

\textbf{Building the ontology of terms.}  The ontology was built by hand, 
starting from the corpus of labels. Labels occuring only once were discarded.  
We created the 
root classes \texttt{activity}, \texttt{adjective}, \texttt{culture}, 
\texttt{food}, and \texttt{thing}. \texttt{culture} and \texttt{food} were 
given top-level status because they were prominently represented in the corpus.
Some labels descend from both \texttt{food} and \texttt{cultural}  
(e.g. \texttt{naan}), but there were also \texttt{food} labels that had no
obvious cultural association (e.g. \texttt{bread}). Whenever one label 
could be regarded as a more specific case of another, they were given a 
child-parent relationship. In some cases missing abstract labels were added to 
the ontology.  Labels which were roughly synonymous were treated as identical,
and misspellings were handled the same way.

\textbf{Assessing relative specificity.}
We used \textbf{Eq.~\ref{eq:specificity}} to compute the relative specificity
of two treatments.  To analyze this similarity measure statistically
we define the specificity of a worker $u$ relative to a 
\textit{basis treatment} $V$ as $S_V(u)$:
\begin{align}
S_V(u) = \frac{1}{|V|} \sum_{v \in V} s(u,v),
\end{align}
And rewrite the relative specificity of $U$ and $V$ as the expected value of
$S_V(u)$ for workers in $U$:
\begin{align}
	S(U,V) &= E\{S_V(u)\}, \\
	\text{with} \quad\quad \text{Var}\{S(U,V)\} 
		&= \frac{\text{Var}\{S_V(u)\}}{|U|}.
\end{align}
To decide if $U$ is significantly more (less) specific than $V$, we do
a null comparison of $V$ to itself: $S(V,V)$.  This value is in principle zero,
but the comparison yields $\text{Var}\{S(V,V)\}$ which we use to state and 
test the null hypothesis that $U$ and $V$ are equally specific:
\begin{align}
	\mathbf{H_0}: \quad S(U,V)-S(V,V)=0
\end{align}
Note that since $S(V,V)=0$, $\mathbf{H_0}$ essentially states that $S(U,V)=0$,
however, it is more stringent since we consider variance due to sampling both
$U$ and $V$:
\begin{align}
\text{Var}\{S(U,V) - S(V,V)\} &= \text{Var}\{S(U,V)\} + \text{Var}\{S(V,V)\}\\
						&> \text{Var}\{S(U,V)\}
\end{align}
Therefore, we find $U$ to be significantly more (less) specific than $V$ when:
\begin{align}
z = \frac{S(U,V)}{ \sqrt{ \text{Var}\{S(U,V)\} + \text{Var}\{S(V,V)\} }} 
	> 1.96.
\end{align}

Due to 
computational intensity, we randomly subsampled 65 workers from each treatment.
To compute the relative food- or culture-specificity, we alter the calculation
in \textbf{Eq.~\ref{eq:worker-specificity}} by only including labels 
respectively of food- or culture-orientation, while excluding all labels that
aro both food- and culture-oriented.



\section*{Acknowledgements}
\section*{References}
\begingroup
\renewcommand{\chapter}[2]{}
\bibliography{newbib.bib}
\endgroup
\bibliographystyle{plain} 

\section*{Figure Legends}

\paragraph{Table 1.}
	Experimental treatments.  Workers were uniformly randomly
	assigned to one of the treatments listed above.  The full funder 
	names used were ``The Glodal Foundation
	for Cultural Recognition'' and ``The National Foundation for 
	Nutritional Awareness''.  The ambiguous, cultural, and ingredients 
	priming images used are shown in \textbf{Figs. B}, \textbf{C}, and 
	\textbf{D}.

\paragraph{Figure 1.}
	Testing image set. These images were presented to all workers in the order 
	shown after the priming set of images. 

\paragraph{Figure 2.}
	Ambiguous image set. These images were presented to workers from certain
	treatements (see \textbf{Table 1}) in the order shown.  

\paragraph{Figure 3.}
	Cultural image set. These images were presented to workers from certain
	treatements (see \textbf{Table 1}) in the order shown.  

\paragraph{Figure 4.}
	Ingredients image set. These images were presented to workers from certain
	treatements (see \textbf{Table 1}) in the order shown.  

\paragraph{Figure 5.}
$F_1$ score and $\theta_\text{NB}$ for the various 
priming treatments, as measured using a Naive Bayes classifier. Each panel 
presents classifier performance results that indicate the difference in
priming between a basis 
treatment, indicated in the inset, and the other treatments, indicated on the
abscissa.

\paragraph{Figure 6.}
$F_1$ score and $\theta_\text{NB}$ for the classification of 
$\textsc{cult}_{img}$ and $\textsc{ingr}_{img}$ using a Naive Bayes 
classifier.  The classifier is provided only the labels attributed to
particular images as indicated on the abscissa.

\paragraph{Figure 7.}
Percent label composition (culture- vs food-oriented labels) for 
various images and treatments.  Panel A shows the composition of 
labels, aggregated over all treatments, as a function of test image.
Panel B shows the composition of labels, aggregated over all images, as
a function of treatment.  Panel C shows the composition of labels 
attributed to the first test image by various treatments.

\paragraph{Figure 8.}
Excess culture-oriented label composition in the labels provided
by $\textsc{cult}_{img}$ as compared to $\textsc{ambg}$, for various
test images (see \textbf{Eq. 1} for calculation).

\paragraph{Figure 9.}
Relative specificity of various treatments.
Each panel shows the comparison between a basis treatment (inset) and 
target treatments (abscissa).
Bar heights indicate relative specificity of the target 
treatment compared to the basis (a positive quantity means the target 
is more specific), and are normalized to units of standard 
deviations of the null comparison.  Dotted horizontal lines indicate
the 95\% confidence interval for the null comparison, which necessarily
has zero relative specificity (see \textbf{Methods} 
for an explanation of the null comparison).  Panels D, E, and F 
represent relative specificity calculated using only culture-oriented
labels, while panels G, H, and I using only food-oriented labels.
Labels that are both culture- and food-oriented are excluded from the
calculations for panels D through I.

\paragraph{Figure S1}
Relative specificity of various treatments, for labels attributed
to the first test image.
Each panel shows the comparison between a basis treatment (inset) and 
target treatments (abscissa).
Bar heights indicate relative specificity of the target 
treatment compared to the basis (a positive quantity means the target 
is more specific), and are normalized to units of standard 
deviations of the null comparison.  Dotted horizontal lines indicate
the 95\% confidence interval for the null comparison, which necessarily
has zero relative specificity (see \textbf{Methods} 
for an explanation of the null comparison).  Panels D, E, and F 
represent relative specificity calculated using only culture-oriented
labels, while panels G, H, and I using only food-oriented labels.
Labels that are both culture- and food-oriented are excluded from the
calculations for panels D through I.

\end{document}


